<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Pierre Karashchuk, Katie L. Rupp, Evyn S. Dickinson, Sarah Walling-Bell, Elischa Sanders, Eiman Azim, Bingni W. Brunton, John C. Tuthill">

<title>Anipose: a toolkit for robust markerless 3D pose estimation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="anipose_files/libs/clipboard/clipboard.min.js"></script>
<script src="anipose_files/libs/quarto-html/quarto.js"></script>
<script src="anipose_files/libs/quarto-html/popper.min.js"></script>
<script src="anipose_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="anipose_files/libs/quarto-html/anchor.min.js"></script>
<link href="anipose_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="anipose_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="anipose_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="anipose_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="anipose_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec:intro" id="toc-sec:intro" class="nav-link active" data-scroll-target="#sec\:intro"><span class="toc-section-number">1</span>  Introduction</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="toc-section-number">2</span>  Results</a>
  <ul class="collapse">
  <li><a href="#robust-calibration-of-multiple-camera-views" id="toc-robust-calibration-of-multiple-camera-views" class="nav-link" data-scroll-target="#robust-calibration-of-multiple-camera-views"><span class="toc-section-number">2.1</span>  Robust calibration of multiple camera views</a></li>
  <li><a href="#accurate-reconstruction-of-physical-lengths-and-angles-in-3d" id="toc-accurate-reconstruction-of-physical-lengths-and-angles-in-3d" class="nav-link" data-scroll-target="#accurate-reconstruction-of-physical-lengths-and-angles-in-3d"><span class="toc-section-number">2.2</span>  Accurate reconstruction of physical lengths and angles in 3D</a></li>
  <li><a href="#animal-tracking-in-3d" id="toc-animal-tracking-in-3d" class="nav-link" data-scroll-target="#animal-tracking-in-3d"><span class="toc-section-number">2.3</span>  Animal tracking in 3D</a></li>
  <li><a href="#addition-of-filters-to-improve-tracking-accuracy" id="toc-addition-of-filters-to-improve-tracking-accuracy" class="nav-link" data-scroll-target="#addition-of-filters-to-improve-tracking-accuracy"><span class="toc-section-number">2.4</span>  Addition of filters to improve tracking accuracy</a>
  <ul class="collapse">
  <li><a href="#refining-keypoints-in-2d" id="toc-refining-keypoints-in-2d" class="nav-link" data-scroll-target="#refining-keypoints-in-2d"><span class="toc-section-number">2.4.1</span>  Refining keypoints in 2D</a></li>
  <li><a href="#refining-poses-and-trajectories-in-3d" id="toc-refining-poses-and-trajectories-in-3d" class="nav-link" data-scroll-target="#refining-poses-and-trajectories-in-3d"><span class="toc-section-number">2.4.2</span>  Refining poses and trajectories in 3D</a></li>
  <li><a href="#improving-estimation-of-derivatives" id="toc-improving-estimation-of-derivatives" class="nav-link" data-scroll-target="#improving-estimation-of-derivatives"><span class="toc-section-number">2.4.3</span>  Improving estimation of derivatives</a></li>
  </ul></li>
  <li><a href="#structured-processing-of-videos" id="toc-structured-processing-of-videos" class="nav-link" data-scroll-target="#structured-processing-of-videos"><span class="toc-section-number">2.5</span>  Structured processing of videos</a></li>
  <li><a href="#visualization-of-tracking" id="toc-visualization-of-tracking" class="nav-link" data-scroll-target="#visualization-of-tracking"><span class="toc-section-number">2.6</span>  Visualization of tracking</a></li>
  <li><a href="#d-tracking-with-anipose-provides-insights-into-motor-control-of-drosophila-walking" id="toc-d-tracking-with-anipose-provides-insights-into-motor-control-of-drosophila-walking" class="nav-link" data-scroll-target="#d-tracking-with-anipose-provides-insights-into-motor-control-of-drosophila-walking"><span class="toc-section-number">2.7</span>  3D tracking with Anipose provides insights into motor control of <em>Drosophila</em> walking</a></li>
  <li><a href="#analysis-of-3d-mouse-reaching-and-human-walking-kinematics" id="toc-analysis-of-3d-mouse-reaching-and-human-walking-kinematics" class="nav-link" data-scroll-target="#analysis-of-3d-mouse-reaching-and-human-walking-kinematics"><span class="toc-section-number">2.8</span>  Analysis of 3D mouse reaching and human walking kinematics</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="toc-section-number">3</span>  Discussion</a>
  <ul class="collapse">
  <li><a href="#impact-of-robust-markerless-3d-tracking" id="toc-impact-of-robust-markerless-3d-tracking" class="nav-link" data-scroll-target="#impact-of-robust-markerless-3d-tracking"><span class="toc-section-number">3.1</span>  Impact of robust markerless 3D tracking</a></li>
  <li><a href="#insights-into-the-motor-control-of-drosophila-walking" id="toc-insights-into-the-motor-control-of-drosophila-walking" class="nav-link" data-scroll-target="#insights-into-the-motor-control-of-drosophila-walking"><span class="toc-section-number">3.2</span>  Insights into the motor control of <em>Drosophila</em> walking</a></li>
  <li><a href="#potential-for-future-improvement-based-on-related-work" id="toc-potential-for-future-improvement-based-on-related-work" class="nav-link" data-scroll-target="#potential-for-future-improvement-based-on-related-work"><span class="toc-section-number">3.3</span>  Potential for future improvement based on related work</a></li>
  <li><a href="#limitations-and-practical-recommendations" id="toc-limitations-and-practical-recommendations" class="nav-link" data-scroll-target="#limitations-and-practical-recommendations"><span class="toc-section-number">3.4</span>  Limitations and practical recommendations</a></li>
  <li><a href="#outlook" id="toc-outlook" class="nav-link" data-scroll-target="#outlook"><span class="toc-section-number">3.5</span>  Outlook</a></li>
  </ul></li>
  <li><a href="#people-involved" id="toc-people-involved" class="nav-link" data-scroll-target="#people-involved"><span class="toc-section-number">4</span>  People involved</a>
  <ul class="collapse">
  <li><a href="#acknowledgments" id="toc-acknowledgments" class="nav-link" data-scroll-target="#acknowledgments">Acknowledgments</a></li>
  <li><a href="#contributions" id="toc-contributions" class="nav-link" data-scroll-target="#contributions">Contributions</a></li>
  <li><a href="#declaration-of-interests" id="toc-declaration-of-interests" class="nav-link" data-scroll-target="#declaration-of-interests">Declaration of Interests</a></li>
  <li><a href="#inclusion-and-diversity" id="toc-inclusion-and-diversity" class="nav-link" data-scroll-target="#inclusion-and-diversity">Inclusion and Diversity</a></li>
  </ul></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods"><span class="toc-section-number">5</span>  Methods</a>
  <ul class="collapse">
  <li><a href="#resource-availability" id="toc-resource-availability" class="nav-link" data-scroll-target="#resource-availability"><span class="toc-section-number">5.1</span>  Resource availability</a></li>
  <li><a href="#experimental-model-and-subject-details" id="toc-experimental-model-and-subject-details" class="nav-link" data-scroll-target="#experimental-model-and-subject-details"><span class="toc-section-number">5.2</span>  Experimental model and subject details</a></li>
  <li><a href="#method-details" id="toc-method-details" class="nav-link" data-scroll-target="#method-details"><span class="toc-section-number">5.3</span>  Method details</a></li>
  <li><a href="#quantification-and-statistical-analysis" id="toc-quantification-and-statistical-analysis" class="nav-link" data-scroll-target="#quantification-and-statistical-analysis"><span class="toc-section-number">5.4</span>  Quantification and statistical analysis</a>
  <ul class="collapse">
  <li><a href="#neural-network-keypoint-detections" id="toc-neural-network-keypoint-detections" class="nav-link" data-scroll-target="#neural-network-keypoint-detections"><span class="toc-section-number">5.4.1</span>  Neural network keypoint detections</a></li>
  <li><a href="#filtering-of-2d-keypoint-detections" id="toc-filtering-of-2d-keypoint-detections" class="nav-link" data-scroll-target="#filtering-of-2d-keypoint-detections"><span class="toc-section-number">5.4.2</span>  Filtering of 2D keypoint detections</a></li>
  <li><a href="#calibration-of-multiple-cameras." id="toc-calibration-of-multiple-cameras." class="nav-link" data-scroll-target="#calibration-of-multiple-cameras."><span class="toc-section-number">5.4.3</span>  Calibration of multiple cameras.</a></li>
  <li><a href="#triangulation-and-3d-filtering" id="toc-triangulation-and-3d-filtering" class="nav-link" data-scroll-target="#triangulation-and-3d-filtering"><span class="toc-section-number">5.4.4</span>  Triangulation and 3D filtering</a></li>
  <li><a href="#evaluation" id="toc-evaluation" class="nav-link" data-scroll-target="#evaluation"><span class="toc-section-number">5.4.5</span>  Evaluation</a></li>
  <li><a href="#analysis-of-kinematics" id="toc-analysis-of-kinematics" class="nav-link" data-scroll-target="#analysis-of-kinematics"><span class="toc-section-number">5.4.6</span>  Analysis of kinematics</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Anipose: a toolkit for robust markerless 3D pose estimation</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Pierre Karashchuk, Katie L. Rupp, Evyn S. Dickinson, Sarah Walling-Bell, Elischa Sanders, Eiman Azim, Bingni W. Brunton, John C. Tuthill </p>
          </div>
  </div>
    
    
  </div>
  

</header>

<section id="sec:intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Tracking body kinematics is key to answering questions in many scientific disciplines. For example, neuroscientists quantify animal movement to relate it to brain dynamics&nbsp;<span class="citation" data-cites="mathis_deep_2020 seethapathi_movement_2019">(<a href="#ref-mathis_deep_2020" role="doc-biblioref">M. W. Mathis and Mathis 2020</a>; <a href="#ref-seethapathi_movement_2019" role="doc-biblioref">Seethapathi et al. 2019</a>)</span>, biomechanists quantify the movement of specific body structures to understand their mechanical properties&nbsp;<span class="citation" data-cites="alexander_natures_2017 bender_computer-assisted_2010">(<a href="#ref-alexander_natures_2017" role="doc-biblioref">Alexander 2017</a>; <a href="#ref-bender_computer-assisted_2010" role="doc-biblioref">Bender, Simpson, and Ritzmann 2010</a>)</span>, social scientists quantify the motion of multiple individuals to understand their interactions&nbsp;<span class="citation" data-cites="schwager_data-driven_2008 halberstadt_incipient_2016">(<a href="#ref-schwager_data-driven_2008" role="doc-biblioref">Schwager et al. 2008</a>; <a href="#ref-halberstadt_incipient_2016" role="doc-biblioref">Halberstadt et al. 2016</a>)</span>, and rehabilitation scientists quantify body movement to diagnose and treat disorders&nbsp;<span class="citation" data-cites="souza_evidence-based_2016 chiba_differential_2005 rinehart_examination_2006">(<a href="#ref-souza_evidence-based_2016" role="doc-biblioref">Souza 2016</a>; <a href="#ref-chiba_differential_2005" role="doc-biblioref">Chiba et al. 2005</a>; <a href="#ref-rinehart_examination_2006" role="doc-biblioref">Rinehart et al. 2006</a>)</span>. In all of these disciplines, achieving rapid and accurate quantification of animal pose is a major bottleneck to scientific progress.</p>
<p>While it is possible for human observers to recognize body movements, scoring behaviors by eye is laborious and often fails to detect differences in the rapid, fine-scale movements that characterize many behaviors. Methods for automated tracking of body kinematics from video have existed for many years, but they typically rely on the addition of markers to identify and disambiguate body parts. Although such methods can achieve very precise pose estimation&nbsp;<span class="citation" data-cites="marshall_2021">(<a href="#ref-marshall_2021" role="doc-biblioref">Marshall et al. 2021</a>)</span>, the use of markers is often impractical, particularly when studying natural behaviors in complex environments, tracking multiple body parts, or studying small animals. Thus, there is a pressing need for methods that perform automated, markerless tracking of body kinematics.</p>
<p>Recent advances in computer vision and machine learning have dramatically improved the speed and accuracy of markerless body pose estimation&nbsp;<span class="citation" data-cites="mathis_deep_2020">(<a href="#ref-mathis_deep_2020" role="doc-biblioref">M. W. Mathis and Mathis 2020</a>)</span>. There are now a number of tools that apply these methods to track animal movement from 2D videos, such as DeepLabCut&nbsp;<span class="citation" data-cites="mathis_deeplabcut_2018">(<a href="#ref-mathis_deeplabcut_2018" role="doc-biblioref">A. Mathis et al. 2018</a>)</span>, SLEAP&nbsp;<span class="citation" data-cites="pereira_sleap_2020">(<a href="#ref-pereira_sleap_2020" role="doc-biblioref">Pereira et al. 2020</a>)</span>, DeepPoseKit&nbsp;<span class="citation" data-cites="graving_deepposekit_2019">(<a href="#ref-graving_deepposekit_2019" role="doc-biblioref">Graving et al. 2019</a>)</span>, among others&nbsp;<span class="citation" data-cites="cao_openpose_2018 machado_quantitative_2015 branson_apt_2020">(<a href="#ref-cao_openpose_2018" role="doc-biblioref">Cao et al. 2019</a>; <a href="#ref-machado_quantitative_2015" role="doc-biblioref">Machado et al. 2015</a>; <a href="#ref-branson_apt_2020" role="doc-biblioref">Branson [2015] 2020</a>)</span>. These software packages allow users to label keypoints, train convolutional neural networks, and apply them to identify keypoints from videos; several toolkits also include auxiliary tools, such as visualizing and filtering the tracked keypoints. Among them, DeepLabCut is the most widely used&nbsp;<span class="citation" data-cites="mathis_primer_2020">(<a href="#ref-mathis_primer_2020" role="doc-biblioref">A. Mathis et al. 2020</a>)</span>.</p>
<p>While tracking of animal movement from 2D video is useful for monitoring specific body parts, full body pose estimation and measurement of complex or subtle behaviors require tracking in three dimensions. Multiple tools have emerged for 3D tracking and body pose estimation, including DANNCE <span class="citation" data-cites="dunn_geometric_2021">(<a href="#ref-dunn_geometric_2021" role="doc-biblioref">Dunn et al. 2021</a>)</span>, FreiPose <span class="citation" data-cites="zimmermann_freipose_2020">(<a href="#ref-zimmermann_freipose_2020" role="doc-biblioref">Zimmermann et al. 2020</a>)</span>, DeepFly3D <span class="citation" data-cites="gunel_deepfly3d_2019">(<a href="#ref-gunel_deepfly3d_2019" role="doc-biblioref">Günel et al. 2019</a>)</span>, and OpenMonkeyStudio <span class="citation" data-cites="bala_openmonkeystudio_2020">(<a href="#ref-bala_openmonkeystudio_2020" role="doc-biblioref">Bala et al. 2020</a>)</span>. However, these tools use fundamentally distinct network architectures, workflows, and user interfaces from popular 2D tracking methods. Out of the existing 2D tracking tools, only DeepLabCut&nbsp;<span class="citation" data-cites="nath_deeplabcut_2019">(<a href="#ref-nath_deeplabcut_2019" role="doc-biblioref">Nath et al. 2019</a>)</span> supports triangulation with up to 2 cameras. However, three or more cameras are often required to resolve pose ambiguities, such as when one body part occludes another. Thus, there is a need for additional tools that allow users to extend their existing 2D tracking setups to achieve robust 3D pose estimation while preserving their established workflows.</p>
<p>Here, we introduce Anipose (a portmanteau of “animal” and “pose”), a toolkit to quantify 3D body kinematics by integrating DeepLabCut tracking from multiple camera views. Anipose consists of a robust calibration module, filters to further refine 2D and 3D tracking, and an interface to visualize and annotate tracked videos (example <a href="https://faculty.washington.edu/tuthill/aniposeviz.html">here</a>). These features allow users to analyze 3D animal movement by extracting behavior and kinematics from videos in a unified software framework. Below, we demonstrate the value of 3D tracking with Anipose for analysis of mice, fly, and human body kinematics (<a href="#fig-datasets">Figure&nbsp;1</a>). Applying 3D tracking to estimate joint angles of walking <em>Drosophila</em>, we find that flies move their middle legs primarily by rotating their coxa and femur, whereas the front and rear legs are driven primarily by femur-tibia flexion. We then show how Anipose can be used to quantify differences between successful and unsuccessful trajectories in a mouse reaching task. Finally, we visualize how specific leg joint angles map onto a manifold of human walking.</p>
<p>We designed Anipose to make 3D tracking accessible for a broad community of scientists. Because it is built on DeepLabCut, Anipose allows users to easily upgrade from 2D to 3D tracking, as well as take advantage of the DeepLabCut community, documentation, and continued support. To help users get started, we provide in-depth tutorials and documentation at <a href="http://anipose.org" class="uri">http://anipose.org</a>. The release of Anipose as free and open-source Python software facilitates adoption, promotes ongoing contributions by community developers, and supports open science.</p>
<div id="fig-datasets" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Four experimental datasets were used for evaluating 3D calibration and tracking with Anipose.</figcaption><p></p>
<!-- <img src="figs/fig_datasets.png" class="img-fluid figure-img"> -->
<iframe width="100%" height="1000px" src="fig_datasets.html">
</iframe>
<!-- ![](figs/fig_datasets_compressed.pdf) -->
<!-- (A) To evaluate tracking errors, a $2 \times 2$ mm precision manufactured ChArUco board was simultaneously filmed from 6 cameras focused on the same point in space. We manually annotated and tracked 9 keypoints on the ChArUco board, a subset of the points that can be detected automatically with OpenCV.  -->
<!-- (B) Adult mice were trained to reach for food pellets through an opening in a clear acrylic box. After training, reach attempts were captured from 2 cameras. To quantify reach kinematics, we labeled and tracked 3 keypoints on each hand. -->
<!-- (C) Fruit flies were tethered and positioned on a spherical treadmill, where they were able to walk, groom, etc. Fly behavior was filmed from 6 cameras evenly distributed around the treadmill. We labeled and tracked 5 keypoints on each of the 6 legs, one keypoint for each of the major leg joints. -->
<!-- (D) As part of the Human 3.6M dataset, professional actors performing a range of actions were filmed from 4 cameras. We tracked 17 joints on each human, covering the major joints of the human body. -->
</figure>
</div>
</section>
<section id="results" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Results</h1>
<p>We implement 3D tracking in a series of steps: estimation of calibration parameters from calibration videos, detection and refinement of 2D joint keypoints, triangulation and refinement of keypoints to obtain 3D joint positions, and computation of joint angles (<a href="#fig-pipeline">Figure&nbsp;2</a>). In addition to the processing pipeline, the key innovations of Anipose are a robust 3D calibration module, spatiotemporal filters that refine pose estimation in both 2D and 3D, and a visualization incorporating videos, tracked keypoints, and behavioral annotations in one interface. We evaluated the calibration and triangulation modules without filters by testing their ability to accurately estimate lengths and angles of a calibration board with known dimensions (<a href="#fig-datasets">Figure&nbsp;1</a> A) and to track the hand of a mouse reaching for a food pellet (<a href="#fig-datasets">Figure&nbsp;1</a> B). We then evaluated how filtering improves estimation in 3D of position and time derivative of walking flies (<a href="#fig-datasets">Figure&nbsp;1</a> C) and humans (<a href="#fig-datasets">Figure&nbsp;1</a> D). Representative examples of tracking from each dataset are shown in Video&nbsp;S1.</p>
<div id="fig-pipeline" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Overview of the Anipose 3D tracking pipeline.</figcaption><p></p>
<!-- ![](figs/fig_pipeline.png) -->
<iframe width="100%" height="1100px" src="fig_pipeline.html">
</iframe>
<!-- (A) The user collects simultaneous video of a calibration board from multiple cameras. -->
<!-- (B) Calibration board keypoints are detected from calibration videos and processed to calculate intrinsic and extrinsic parameters for each camera using iterative bundle adjustment (See Figure S1).  -->
<!-- (C) With the same hardware setup as in A, the user collects behavior videos. -->
<!-- (D) Behavior videos are processed by a neural network (e.g., DeepLabCut) to detect 2D keypoints.  -->
<!-- (E) 2D keypoints are refined with 2D filters to obtain refined 2D detections (@fig-filter2d).  -->
<!-- (F) The filtered 2D keypoints are triangulated to estimate 3D poses. -->
<!-- (G) The estimated 3D poses are passed through an additional spatiotemporal filtering step to obtain refined 3D poses (@fig-optim). -->
<!-- (H) Joint angles are extracted from the refined 3D poses for further analysis. -->
</figure>
</div>
<section id="robust-calibration-of-multiple-camera-views" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="robust-calibration-of-multiple-camera-views"><span class="header-section-number">2.1</span> Robust calibration of multiple camera views</h2>
<p>An essential step in accurate 3D pose estimation is precise camera calibration, which determines the relative location and parameters of each camera (i.e., the focal length and distortions). We implemented an automated procedure that calibrates the cameras from simultaneously acquired videos of a standard calibration board (e.g., checkerboard or ChArUco board) moved by hand through the cameras’ fields of view (<a href="#fig-pipeline">Figure&nbsp;2</a> A). We recommend the ChArUco board because its keypoints may be detected even with partial occlusion and its rotation can be determined uniquely from multiple views. The pipeline starts by detecting keypoints on the calibration board automatically using OpenCV&nbsp;<span class="citation" data-cites="opencv_library">(<a href="#ref-opencv_library" role="doc-biblioref">Bradski 2000</a>)</span>, based on the board’s geometric regularities (e.g., checkerboard grid pattern, specific black and white markers). These board detections are used first to initialize camera calibration parameters from arbitrary positions through a greedy algorithm which adds edges between cameras one by one until it reaches a fully connected tree (Figure&nbsp;S1A).</p>
<p>Although some tracking tools (e.g., <span class="citation" data-cites="cao_openpose_2018 dunn_geometric_2021">(<a href="#ref-cao_openpose_2018" role="doc-biblioref">Cao et al. 2019</a>; <a href="#ref-dunn_geometric_2021" role="doc-biblioref">Dunn et al. 2021</a>)</span>) stop at the initial estimate of camera parameters based on estimated calibration board orientation from different cameras, we found that this is often not sufficient to obtain accurate camera calibrations, especially when there are few frames with a detected board. To resolve this issue, we implemented procedures that optimize the camera calibration parameters to minimize the reprojection error of the calibration board keypoints, referred to as bundle adjustment in the camera registration literature&nbsp;<span class="citation" data-cites="goos_bundle_2000">(<a href="#ref-goos_bundle_2000" role="doc-biblioref">Triggs et al. 2000</a>)</span>. We implemented bundle adjustment with standard (least-squares) as well as robust losses (Huber and soft L1). Furthermore, we developed an iterative procedure we term "iterative bundle adjustment", which performs bundle adjustment in multiple stages, using only a random subsample of detected keypoints points in each stage (see Methods for a detailed description). This procedure automatically tunes the outlier thresholds and minimizes the impact of erroneous keypoint detections and bad camera initialization. Each of these bundle adjustment procedures improves the reprojection error from the initial estimate (Figure&nbsp;S1B). Iterative bundle adjustment produced marginally better results, but with no parameter tuning, so we use this as the default in Anipose.</p>
</section>
<section id="accurate-reconstruction-of-physical-lengths-and-angles-in-3d" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="accurate-reconstruction-of-physical-lengths-and-angles-in-3d"><span class="header-section-number">2.2</span> Accurate reconstruction of physical lengths and angles in 3D</h2>
<p>An important test of any calibration method is whether it can accurately reconstruct an object with known dimensions. We evaluated the Anipose calibration and triangulation toolkit by asking whether it could estimate the lengths and angles of a precisely manufactured ChArUco board&nbsp;<span class="citation" data-cites="garrido-jurado_automatic_2014">(<a href="#ref-garrido-jurado_automatic_2014" role="doc-biblioref">Garrido-Jurado et al. 2014</a>)</span>.</p>
<p>We first compared the accuracy of tracking the 9 corners of the ChArUco board (<a href="#fig-raw">Figure&nbsp;3</a> A) with three methods: manual annotation, neural network detections, and OpenCV detections (example detections in <a href="#fig-raw">Figure&nbsp;3</a> B). Although manual annotations are typically assumed to be the ground truth in tracking animal kinematics, we started by assessing the reliability of manual annotations relative to high-precision, sub-pixel resolution keypoint detection based on the geometry of the ChArUco board with OpenCV&nbsp;<span class="citation" data-cites="opencv_library garrido-jurado_automatic_2014">(<a href="#ref-opencv_library" role="doc-biblioref">Bradski 2000</a>; <a href="#ref-garrido-jurado_automatic_2014" role="doc-biblioref">Garrido-Jurado et al. 2014</a>)</span>. Relative to the OpenCV points, the manual keypoint annotations had a mean error of (<span class="math inline">0.52</span>, <span class="math inline">-0.75</span>) pixels and standard deviation of (<span class="math inline">2.57</span>, <span class="math inline">2.39</span>) pixels, in the (x, y) directions, respectively (<a href="#fig-raw">Figure&nbsp;3</a> C). These observations provide a useful baseline of manual annotation accuracy.</p>
<p>We evaluated the accuracy of reconstructing ChArUco board lengths and angles as estimated by three methods: manual keypoint annotations, OpenCV keypoint detections, and neural network keypoint detections (see Methods for detailed descriptions). As our ground-truth dataset, we chose the known physical lengths and angles between all pairs of 9 corners on the ChArUco board. The ChArUco board was manufactured with precise tolerance (<span class="math inline">&lt;</span> 2&nbsp;μm), which allowed us to evaluate the accuracy of lengths and angles from manual keypoint annotations and OpenCV keypoint detections, which are commonly taken to be the ground truth. As expected, OpenCV detections had the lowest error in length and angle, as they leveraged prior knowledge of the ChArUco board geometry to make high-precision corner estimates (<a href="#fig-raw">Figure&nbsp;3</a> D). Surprisingly, neural network (trained with DeepLabCut) predictions had a lower error than manual annotations, despite the network itself being trained on manual annotations. More than 90% of poses estimated by Anipose had an error of less than 20 <span class="math inline">\mu</span>m in length and 1 degree in angle, relative to the true dimensions of the ChArUco board (<a href="#fig-raw">Figure&nbsp;3</a> D). These results demonstrate the efficacy of camera calibration with Anipose and serve as useful bounds of expected performance.</p>
</section>
<section id="animal-tracking-in-3d" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="animal-tracking-in-3d"><span class="header-section-number">2.3</span> Animal tracking in 3D</h2>
<p>We evaluated the triangulation of markerless tracking on three different animal datasets (<a href="#fig-raw">Figure&nbsp;3</a> E-G). For each dataset, we computed the error of estimated joint positions and angles on labeled animals withheld from the training data. The error in estimated joint angles was <span class="math inline">&lt;16^{\circ}</span> in over 90% of frames, and <span class="math inline">&lt;10^{\circ}</span> in over 75% of frames. Furthermore, the error in the estimated joint position was <span class="math inline">&lt;</span> 18 pixels (approximately 1.6mm, 0.14mm, 86mm for mouse, fly, and human datasets respectively) in over 90% of frames and $&lt;$12 pixels (approximately 1mm, 0.09mm, 57mm for mouse, fly, and human datasets respectively) in over 75% of frames. Importantly, the position error in units of camera pixels is roughly comparable across these three datasets, spanning more than 3 orders-of-magnitude in spatial scale. Therefore, we believe these errors are representative of what can currently be expected for accuracy of 3D markerless tracking.</p>
<p>Although triangulation usually resulted in accurate estimates of joint positions and angles, there were still some frames where it failed due to missing keypoint detections (as in <a href="#fig-raw">Figure&nbsp;3</a> E). In other cases, incorrect keypoint detections led to erroneous 3D joint position estimates (as in <a href="#fig-raw">Figure&nbsp;3</a> F). Even though these issues occurred in a small minority of frames, tracking errors are especially problematic for analyzing movement trajectories. For instance, missing estimates complicate the estimation of derivatives, whereas erroneous estimates bias the distribution of summary statistics. To minimize these issues, we leveraged complementary temporal and spatial information within each dataset to refine tracking performance in 3D.</p>
<div id="fig-raw" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Anipose can consistently estimate positions and angles of keypoints across four different datasets.</figcaption><p></p>
<iframe width="100%" height="800px" src="fig_raw.html">
</iframe>
<!-- (A) We identified 9 corners as keypoints on the ChArUco board in 200 frames from each of 6 cameras. -->
<!-- (B) For comparison, we used manual annotation of the same ChArUco board dataset to train a neural network. We then compared tracking errors of the manual annotations, the neural network, and OpenCV. -->
<!-- (C) Error in manually annotated keypoints relative to the sub-pixel precision of OpenCV detections. Manually annotated keypoints had a mean error of (0.52, -0.75) pixels and standard deviation of (2.57, 2.39) pixels. -->
<!-- (D) Lengths between all possible pairs of keypoints were computed and compared to the physical lengths. Similarly, all possible angles between triplets of keypoints were computed and compared to known physical angles. OpenCV keypoints provided the most reliable estimates, followed by neural network predictions, then manual annotations. Note that OpenCV generally detected only a small fraction of the keypoints detected by the neural network or through manual annotation (19.3\% of keypoints detected by OpenCV, compared to 78.1\% by the neural network and 75\% by manual annotations). -->
<!-- (E) At this stage, prior to filtering, outlier and missing keypoint detections are apparent. Shown at left is an example trace of the tracked 3D position of the base of the mouse hand, projected onto the direction of the reach. On the right, we quantified the distribution of errors when estimating all joint positions and angles, relative to manual annotations. For the mouse dataset, 1 pixel corresponds to approximately 0.09 mm. -->
<!-- (F) Same layout as A, but for 3D position of the fly hind-leg tibia-tarsus joint, projected onto the longitudinal axis of the fruit fly. For the fly dataset, 1 pixel $\approx$.0075 mm.  -->
<!-- (G) Same layout as A, but for tracked 3D position of a human wrist, projected onto an arbitrary axis. Note that the human (and their wrist) is moving throughout the room. For the human dataset, 1 pixel $\approx$ 4.8 mm. -->
</figure>
</div>
</section>
<section id="addition-of-filters-to-improve-tracking-accuracy" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="addition-of-filters-to-improve-tracking-accuracy"><span class="header-section-number">2.4</span> Addition of filters to improve tracking accuracy</h2>
<p>Naturally behaving animals present unique challenges for 3D pose estimation. Animals can contort their bodies into many different configurations, which means that each behavioral session may include unique poses that have not been previously encountered, even across multiple animals. Our approach to tackling these challenges is to leverage prior knowledge that animal movements are usually smooth and continuous, and that rigid limbs do not change in length over short timescales. In particular, we developed and implemented a set of 2D and 3D filters that refine keypoints, remove errors in keypoint detections, and constrain the set of reconstructed kinematic trajectories. We demonstrate that both sets of filters work together to significantly improve pose estimation. Here we focus on detailed quantification of these filters in tracking flies and humans, where our datasets included keypoints at every limb joint tracked with at least 4 camera views.</p>
<section id="refining-keypoints-in-2d" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="refining-keypoints-in-2d"><span class="header-section-number">2.4.1</span> Refining keypoints in 2D</h3>
<p>We implemented three distinct algorithms to remove or correct errors in 2D keypoint detection: a median filter, a Viterbi filter, and an autoencoder filter. The median and Viterbi filters operate on each tracked joint across frames, and the autoencoder filter refines keypoints using learned correlates among all joints. The median filter removes any point that deviates from a median filtered trajectory of user-specified length, then interpolates the missing data. The Viterbi filter finds the most likely path of keypoint detections for each joint across frames from a set of top (e.g., 20) detections per frame, given the expected standard deviation of joint movement in pixels as a prior. Finally, the autoencoder filter corrects the estimated score of each joint based on the scores of the other joints, with no parameters set by the user. Where errors in tracking cannot be corrected by filtering, the keypoint is removed altogether, since the missing joint can be inferred from other camera views, but an erroneous keypoint can produce large discrepancies in triangulation. We document the parameters we used to produce results across the paper in Table&nbsp;S1. Anipose users are encouraged to evaluate the effect these filtering parameters may have on their analyses. Depending on the particulars of the experimental setup, including the spatial and temporal resolution of the videos, the parameters may need to be adjusted.</p>
<p>The addition of each filtering step noticeably improved the tracking of fly leg joints (<a href="#fig-filter2d">Figure&nbsp;4</a> A). The median and Viterbi filters both reduced spurious jumps in keypoint position, which may occur if the neural network detects a similar keypoint on a different limb or at another location in the frame. The Viterbi filter is able to remove small erroneous jumps in detected keypoint trajectories while also preserving high frequency dynamics, whereas the median filter may mistakenly identify fast movements as an error and remove them. The autoencoder filter removed detections for keypoints which were typically not visible from a given view, which improved 3D position estimates after triangulation (Figure&nbsp;S2).</p>
<p>For each of the 2D filters, we quantified the performance improvement of estimating the joint position and angle on manually annotated validation datasets. The 2D median filter significantly reduced error in joint position and angle estimation on the human dataset ( <span class="math inline">t=-14.8, p&lt;0.001</span> for position, <span class="math inline">t=-7.7, p&lt;0.001</span>, paired t-test) but not on the fly dataset (<span class="math inline">t=-1.2, p = 0.2</span> for position, <span class="math inline">t=-0.98, p=0.3</span>, paired t-test). The Viterbi filter reduced error on both fly and human datasets (<span class="math inline">t=-4.4</span> and <span class="math inline">t=-4.1</span> for fly position and angle, <span class="math inline">t=-10.9</span> and <span class="math inline">t=-8.7</span> for human position, with <span class="math inline">p&lt;0.001</span> for all, paired t-test). The autoencoder filter also reduced error in joint positions and angles on the fly dataset (<span class="math inline">t=-5.4, p&lt;0.001</span> for positions, <span class="math inline">t=-2.16, p = 0.03</span> for angles, paired t-test). We did not apply the autoencoder filter to human tracking, since all occluded points are annotated in the training dataset. In the fly dataset, applying the autoencoder filter after the Viterbi filter further improved the joint position and angle estimates above the autoencoder (<span class="math inline">t=-3.97, p&lt;0.001</span> for positions, <span class="math inline">t=-3.44, p&lt;0.001</span> for angles, paired t-test). In summary, we found the addition of these three filters improved the ability of Anipose to accurately estimate joint positions and angles.</p>
<div id="fig-filter2d" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;4: 2D filters improve accuracy of 2D pose estimation by taking advantage of the temporal structure of animal behavior.</figcaption><p></p>
<iframe width="100%" height="750px" src="fig_2d_filters.html">
</iframe>
<!-- (A) An example trace of the x-coordinate of the 2D position of a fly's tibia-tarsus joint before and after each step in filtering. Filtering reduces spurious jumps while preserving correct keypoint detections. See Figure S2 for a demonstration of the autoencoder filter.  -->
<!-- (B) Comparison of error in joint position before and after filtering. The mean difference in error for the same tracked points is plotted, along with the 95\% confidence interval. Viterbi and autoencoder filters significantly improved the estimation of joint position in flies ($p < 0.001$, paired t-test). The Viterbi filter significantly improved estimation of joint position in humans ($p < 0.001$, paired t-test). For the fly dataset, 1 pixel $\approx$ 0.0075 mm. For the human dataset, 1 pixel $\approx$ 4.8 mm. The absolute error values are indicated in parentheses above the 0 tick mark for each dataset. -->
<!-- (C) Comparison of angle estimates before and after filtering. The mean difference is plotted as in B. Viterbi and autoencoder filters significantly improved the estimation of angles in flies and humans ($p < 0.001$, paired t-test).  -->

<!-- The results in (B) and (C) are evaluated on a validation dataset withheld from the training (1200 frames for the fly, 8608 frames for the humans). See Table S1 for filter parameters. -->
</figure>
</div>
</section>
<section id="refining-poses-and-trajectories-in-3d" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="refining-poses-and-trajectories-in-3d"><span class="header-section-number">2.4.2</span> Refining poses and trajectories in 3D</h3>
<p>To further refine joint position and angle estimates in 3D, we developed a triangulation optimization that takes advantage of the spatiotemporal structure of animal pose and behavior. Specifically, our optimization produces pose estimates that are smooth in time using temporal regularization, and limbs demarcated by adjacent keypoints that are constant in length with spatial regularization. The length for each limb is automatically estimated in the optimization. The relative strengths of the temporal and spatial regularization terms may be balanced and tuned independently. As with the 2D filters, we empirically determined default strengths that worked across multiple datasets. A complete description of each filter, along with all the parameters, is detailed in the Methods. For illustration, we compared the performance of these filters (<a href="#fig-optim">Figure&nbsp;5</a> A) to other commonly used methods from the literature (Random sample consensus, or RANSAC, triangulation and 3D median filter) on the walking fly dataset. We applied the 3D filters on kinematic trajectories partially corrected with 2D filtering (Viterbi then autoencoder filters for the fly dataset, and Viterbi filter only for the human dataset), to evaluate how much the 3D filters improved the accuracy. Spatiotemporal regularization substantially improved pose estimation. The temporal regularization noticeably reduced jitter in the trajectory (<a href="#fig-optim">Figure&nbsp;5</a> A), while the spatial regularization stabilized the estimate of limb length (Figure&nbsp;S3B). These improvements are also obvious in example videos of reconstructed pose before and after filtering (Video 2).</p>
<p>For each of the 3D filters, we quantified the improvement in position and angle error relative to tracking with 2D filters alone (<a href="#fig-optim">Figure&nbsp;5</a> C and S3C). We found that RANSAC triangulation did not improve position and angle error. The 3D median filter significantly reduced position and angle errors relative to only 2D filters for the human dataset (<span class="math inline">t=-11.8</span> for position, <span class="math inline">t=-7.3</span> for angle, <span class="math inline">p &lt; 0.001</span> for both, paired t-test), but not for the fly dataset. Spatial and temporal regularization applied together provided the largest reduction in tracking error (<span class="math inline">t=-18.7</span> and <span class="math inline">t=-6.1</span> for human positions and angles, <span class="math inline">t=-10.8</span> and <span class="math inline">t=5.8</span> for fly positions and angles, <span class="math inline">p &lt; 0.001</span> for all, paired t-test). Overall, we find that the 3D filters implemented in Anipose significantly improve pose estimation.</p>
<div id="fig-optim" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;5: Spatiotemporal filters further improve 3D pose estimation. See Figure S3 for example angle and segment length traces with different filters. See Figure S4 for detailed evaluation of temporal regularization on a synthetic dataset.</figcaption><p></p>
<!-- ![](figs/fig_3d_filters_deriv.png) -->
<iframe width="100%" height="900px" src="fig_optim.html">
</iframe>
<!-- (A) An example trace of the tracked 3D position of the fly tibia-tarsus joint, before and after filtering. To plot a single illustrative position value, the 3D x-y-z coordinate is projected onto the longitudinal axis of the fly. Also included are comparisons with standard 3D filtering algorithms RANSAC and a 3D median filter, along with manual annotations. -->
<!-- Filtering leads to reduction of sudden jumps and keypoint jitters, even compared to 2D filters alone. -->
<!-- (B) An example trace of the derivative of the 3D position of the fly tibia-tarsus joint, before and after filtering. To plot a single illustrative derivative value, the 3D x-y-z joint coordinates is projected onto the longitudinal axis of the fly. Spatiotemporal regularization produces smooth derivative estimates, which are closer to the manual annotations compared to other filtering approaches. -->
<!-- (C) Comparison of error in joint position before and after filtering. The mean difference in error for the same tracked points is plotted, along with the 95\% confidence interval. The absolute error values are indicated in parentheses above the 0 tick mark for each dataset. The 2D filters are the Viterbi filter followed by the autoencoder for the fly dataset and Viterbi filter alone for the human dataset. Spatiotemporal regularization improves the estimation of joint position significantly above 2D filters in both datasets (p < 0.001, paired t-test). The 3D median filter improves pose estimation on the human dataset (p < 0.001, paired t-test) but not on the fly dataset. RANSAC triangulation does not improve pose estimation for either dataset. For the fly dataset, 1 pixel corresponds to 0.0075 mm. -->
<!--  the human dataset, 1 pixel corresponds to 4.8 mm. -->
<!-- (D) Comparison of error in joint position derivative before and after filtering. The mean difference in error for the same tracked points is plotted, along with the 95\% confidence interval. The absolute error values are indicated in parentheses above the 0 tick mark for each dataset. The 2D filters are the Viterbi filter followed by the autoencoder for the fly dataset and Viterbi filter alone for the human dataset. For the human dataset, due to the large number of labeled points, the confidence intervals are smaller than the size of the points. Adding filters significantly improves the estimate of the derivative. -->
</figure>
</div>
</section>
<section id="improving-estimation-of-derivatives" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="improving-estimation-of-derivatives"><span class="header-section-number">2.4.3</span> Improving estimation of derivatives</h3>
<p>In addition to tracking body pose, it is often valuable to track the speed of body movements. We compared the temporal derivative of 3D joint positions estimated with Anipose to the derivative computed from manual annotations (<a href="#fig-optim">Figure&nbsp;5</a> B and D) and found both qualitative and quantitative improvements to estimation of body movement speed.</p>
<p>Filtered trajectories produced smoother derivatives, due to the fact that tracking errors are corrected through 2D and 3D filtering, and the temporal regularization explicitly penalizes deviations from smoothness (<a href="#fig-optim">Figure&nbsp;5</a> B). It is challenging to evaluate the accuracy of Anipose derivative estimates because computing finite difference derivatives of manual annotations amplifies known errors in these annotations. Given that manual annotations deviate from the ground truth tracking with a standard deviation of at most <span class="math inline">3.5</span> pixels in distance (<a href="#fig-raw">Figure&nbsp;3</a> C), we expect computing the finite difference derivative of such annotations to produce derivatives with error of <span class="math inline">4.95</span> pixels (about 0.037 mm corresponding to 11.1 mm/s over one frame in the fly dataset). Therefore, the manual annotations (dark green trace in <a href="#fig-optim">Figure&nbsp;5</a> B) do not represent the true derivative, but rather a noisy approximation of the true derivative. The temporally regularized trajectory resembles this estimate of the derivative but is more smooth because of temporal regularization. The strength of this regularization, and the subsequent smoothness of the tracked keypoints, is a parameter that users may fine-tune (see <span class="citation" data-cites="breugel_numerical_2020">(<a href="#ref-breugel_numerical_2020" role="doc-biblioref">Breugel, Kutz, and Brunton 2020</a>)</span> for a systematic way to tune this parameter). We suggest some default values and provide guidance on choosing parameters in the Discussion.</p>
<p>We found that the 2D filters (Viterbi and autoencoder in fly, only Viterbi in human) improved the error in derivative by 2.78 mm/s for the fly dataset (<span class="math inline">t=-9.4, p&lt;0.001</span>, paired t-test) and by 30.0 mm/s on the human dataset (<span class="math inline">t=-28.0, p&lt;0.001</span>, paired t-test) relative to no filters. The 3D median filter improved the error in derivative by 1.65 mm/s in the fly dataset (<span class="math inline">t=-4.8, p&lt;0.001</span>, paired t-test) and by 177.3 mm/s in the human dataset (<span class="math inline">t=-324, p\ll0.001</span>, paired t-test) RANSAC improved error in the derivative estimate by 2.16 mm/s in the fly dataset (<span class="math inline">t=-7.07, p&lt;0.001</span>, paired t-test) but did not improve the error in the human dataset. The spatiotemporal regularization improved the error in derivative by an additional 0.67 mm/s for the fly dataset (<span class="math inline">t=-4.10, p&lt;0.001</span>, paired t-test) and by 217.7 mm/s on the human dataset (<span class="math inline">t=-213, p\ll0.001</span>, paired t-test) relative to the 2D filters. Overall, we found that the filters implemented in Anipose significantly improved the estimation of body movement in the fly and human datasets.</p>
</section>
</section>
<section id="structured-processing-of-videos" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="structured-processing-of-videos"><span class="header-section-number">2.5</span> Structured processing of videos</h2>
<p>Animal behavior experiments are often high-throughput, meaning that large numbers of videos are recorded over many repeated sessions with different experimental conditions. To make the process of 3D tracking scalable to large datasets, we designed a specific file structure (Figure&nbsp;S5) to organize and process behavior videos, configuration files, and calibration data. This file structure also facilitates scalable analysis of body kinematics across individual animals and experimental conditions. For example, the command <code>anipose analyze</code> detects keypoints for each video in the project folder, and <code>anipose calibrate</code> obtains calibration parameters for all the cameras in all calibration folders. Each command operates on all videos in the project, circumventing the need to process each video individually. In addition, this design allows the user to easily reanalyze the same dataset using different filtering parameters or with different 2D tracking libraries (e.g., to compare DeepLabCut and SLEAP). For the users that prefer to set up their own pipelines, we also package the calibration, triangulation, and filtering functions in a separate library called aniposelib.</p>
</section>
<section id="visualization-of-tracking" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="visualization-of-tracking"><span class="header-section-number">2.6</span> Visualization of tracking</h2>
<p>The large number of videos and keypoints tracked in many behavior experiments make it challenging to visualize the resulting data. In addition, the large files created with high-speed video often make it impractical to store and visualize an entire dataset on a laptop. To facilitate evaluation and interpretation of data tracked with Anipose, we developed a web-based visualization tool (<a href="#fig-viz">Figure&nbsp;6</a>). The tool shows, for a given trial, each camera view, 3D tracking, and 2D projections of the tracked keypoints. The user can speed up and slow down the speed at which the videos play and rotate the tracked keypoints in 3D. By taking advantage of the standardized file structure, the interface provides a dropdown menu to navigate between trials and sessions. The interface also allows the user to annotate the behaviors in each video, which is particularly useful for isolating specific behaviors for further analysis. As this tool is web-based, it may be run on a server, allowing users to preview videos and inspect tracking from any computer. Furthermore, if the server is public, users may easily share links to particular trials with collaborators to point out specific behaviors (example <a href="https://faculty.washington.edu/tuthill/aniposeviz.html">here</a>) .</p>
<div id="fig-viz" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;6: A web tool for visualizing 3D kinematics tracked with Anipose.</figcaption><p></p>
<img src="figs/fig_viz.png" class="img-fluid figure-img">
<style type="text/css">
.scaled-frame {
    -moz-transform:scale(0.66);
    -moz-transform-origin: 0 0;
    -o-transform: scale(0.66);
    -o-transform-origin: 0 0;
    -webkit-transform: scale(0.66);
    -webkit-transform-origin: 0 0;
}
</style>
<iframe class="scaled-frame" width="152%" height="900px" src="https://faculty.washington.edu/tuthill/aniposeviz.html">
</iframe>
<p>The videos from all views are displayed synchronously, with overlaid projections of 3D keypoints from Anipose. To the right of the videos, a dynamic 3D visualization allows the user to interact with the 3D keypoints by rotating or zooming in. Above the videos, the user can alter the playback speed or jump to different time points in the video. The user can also annotate the behavior of the animal for further analysis. Menus at the top allow the user to select specific recording dates, experimental trials, or filter trials by a specific behavior. The tool takes advantage of the Anipose file structure shown in Figure S5.</p>
</figure>
</div>
</section>
<section id="d-tracking-with-anipose-provides-insights-into-motor-control-of-drosophila-walking" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="d-tracking-with-anipose-provides-insights-into-motor-control-of-drosophila-walking"><span class="header-section-number">2.7</span> 3D tracking with Anipose provides insights into motor control of <em>Drosophila</em> walking</h2>
<p>We first used 3D tracking with Anipose to analyze the leg joint kinematics of fruit flies walking on a spherical treadmill. Although fly walking has been studied in great detail from a 2D perspective&nbsp;<span class="citation" data-cites="deangelis_manifold_2019 mendes_quantification_2013 berendes_speed-dependent_2016">(<a href="#ref-deangelis_manifold_2019" role="doc-biblioref">DeAngelis, Zavatone-Veth, and Clark 2019</a>; <a href="#ref-mendes_quantification_2013" role="doc-biblioref">Mendes et al. 2013</a>; <a href="#ref-berendes_speed-dependent_2016" role="doc-biblioref">Berendes et al. 2016</a>)</span>, 3D joint kinematics of walking flies have not previously been analyzed. Thus, it was not clear how fly leg joints move during walking. Specifically, we sought to understand the relative contributions of leg joint flexion and rotation.</p>
<p>Some limb joints are not restricted to movement in a single plane, but can also rotate around the long axis of a limb segment. Whereas the importance of rotation angles has long been recognized for human gait analysis&nbsp;<span class="citation" data-cites="roberts_biomechanical_2017">(<a href="#ref-roberts_biomechanical_2017" role="doc-biblioref">Roberts, Mongeon, and Prince 2017</a>)</span>, rotation angles have been comparatively understudied in other animals. This gap exists largely because estimating rotation angles requires precise tracking of joint kinematics in 3D.</p>
<p>The fly leg consists of five segments, whose movements are defined by 8 angles (1 abduction, 3 rotation, 4 flexion). We observed significant rotations between the coxa and femur segments during walking. <a href="#fig-flydemo">Figure&nbsp;7</a> A shows trajectories of coxa rotation, femur rotation, and femur-tibia flexion angles for one walking bout.</p>
<p>Interestingly, the magnitude of joint rotation varied across different legs. Although the femur-tibia flexion angle has a high range of motion in the front and back legs, the femur-tibia flexion angle has a comparatively smaller range of motion in the middle legs (<a href="#fig-flydemo">Figure&nbsp;7</a> B). In contrast, the middle legs are primarily driven by coxa and femur rotation. Furthermore, the coxa joints of contralateral legs rotate in opposing directions. These results suggest that the circuitry that coordinates walking (e.g., the central pattern generator) cannot be the same for all six legs. Rather, walking circuits must control different motor neurons and muscles to generate unique joint kinematics for each leg.</p>
<p>In addition to comparing joint angle distributions across legs, we analyzed trajectories of 3D leg kinematics across flies. We used the UMAP nonlinear embedding method&nbsp;<span class="citation" data-cites="mcinnes2018umap-software">(<a href="#ref-mcinnes2018umap-software" role="doc-biblioref">McInnes et al. 2018</a>)</span> to embed coxa rotation, femur rotation, and femur-tibia flexion angles and their derivatives of all legs (<a href="#fig-flydemo">Figure&nbsp;7</a> C). The three-dimensional embedding of joint kinematics formed a mushroom-shaped manifold. Individual flies reside at specific regions of the manifold, but for all flies, step phase is distributed along the circumference of the cap (<a href="#fig-flydemo">Figure&nbsp;7</a> D). These results are consistent with the existence of a continuum of walking gaits across flies&nbsp;<span class="citation" data-cites="deangelis_manifold_2019">(<a href="#ref-deangelis_manifold_2019" role="doc-biblioref">DeAngelis, Zavatone-Veth, and Clark 2019</a>)</span>, but also suggest that different flies have slightly distinct walking kinematics. This analysis also demonstrates how 3D tracking can be used to dissect the contributions of specific joints to complex motor behaviors. Visualizing a manifold of 3D joint kinematics provides a means to understand how joint kinematics vary within the high-dimensional space of a motor control task (<a href="#fig-flydemo">Figure&nbsp;7</a> E, Figure&nbsp;S6B).</p>
<div id="fig-flydemo" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;7: 3D tracking enables quantification of fly, mouse, and human joint position and angles to reveal structure in behavior. See also Figures S6 and S7.</figcaption><p></p>
<iframe width="100%" height="1000px" src="fig_flymousehuman.html">
</iframe>
<!-- ![](figs/fig_flymousehuman.png) -->
<!-- (A) Representative traces of coxa rotation, femur rotation, and femur-tibia flexion angles from tethered-walking flies. The median angle value is indicated for each angle as a reference point.  -->
<!-- (B) Probability distribution functions of coxa rotation, femur rotation, and femur-tibia flexion angles from 39 flies (1480 total seconds of walking). Only walking bouts are included. The distribution of femur-tibia flexion angles is broader for the front and rear legs, whereas the distribution of femur rotation angles is broader for the middle legs. -->
<!-- (C) UMAP embedding of coxa rotation, femur rotation, femur-tibia flexion angles across all legs, and their derivatives. Axis units are arbitrary. Although each fly has a characteristic gait, there is a continuum across most flies, with some flies offset from the rest.  -->
<!-- (D) UMAP embedding as in C, colored by the phase of the step cycle, revealing the match between the circular structure of the embedding and the step phase. -->
<!-- (E) UMAP embedding as in C, colored by front-right leg femur-tibia flexion and femur rotation, and middle right leg femur-tibia flexion and femur rotation.  -->
<!-- Across multiple flies, the dynamics of the middle legs are dominated by femur rotation, whereas the dynamics of the front legs are dominated by femur-tibia flexion. -->
<!-- (F) Example 3D trajectories of a mouse reaching for a food pellet. The pellet is indicated as a black dot. -->
<!-- (G) Mean distance to pellet holder as a function of time across all 4 mice (88 hits, 69 bumps, 28 misses). Shaded areas are 95\% confidence intervals. When reaches are aligned to the grasp attempt (0 ms), the hand is farther from the pellet holder on miss trials compared to hit or bump trials. Averaging across all mice reveals a clear difference between reach types. -->
<!-- (H) Representative trace of knee flexion from a walking human, tracked with Anipose. Data is from the Human 3.6M dataset. The median angle value is indicated at left as a reference point. -->
<!-- (I) Probability distribution function of knee flexion angle from 7 humans. Only sessions that include walking are included.  -->
<!-- (J) UMAP embedding of knee flexion, hip rotation, and hip flexion angles across all legs, and their derivatives. Axis units are arbitrary. Although each human subject has a characteristic gait, most of the walking patterns map onto a common cylinder manifold. -->
<!-- (K) UMAP embedding as in E but colored by knee flexion for each leg. -->
<!-- Coloring by knee flexion angle reveals the common phase alignment of the circles across subjects. -->
</figure>
</div>
</section>
<section id="analysis-of-3d-mouse-reaching-and-human-walking-kinematics" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="analysis-of-3d-mouse-reaching-and-human-walking-kinematics"><span class="header-section-number">2.8</span> Analysis of 3D mouse reaching and human walking kinematics</h2>
<p>To illustrate the value of 3D tracking with Anipose for studying other animal species, we analyzed data from reaching mice and walking humans. Joint positions and angles have long been used to quantify movement in both healthy and impaired animals &nbsp;<span class="citation" data-cites="koch_ror_2017 balbinot_post-stroke_2018 fukuchi_public_2018">(<a href="#ref-koch_ror_2017" role="doc-biblioref">Koch et al. 2017</a>; <a href="#ref-balbinot_post-stroke_2018" role="doc-biblioref">Balbinot et al. 2018</a>; <a href="#ref-fukuchi_public_2018" role="doc-biblioref">Fukuchi, Fukuchi, and Duarte 2018</a>)</span>. However, previous quantification has relied primarily on laborious manual tracking or marker-based tracking with extensive manual corrections. Here we demonstrate analysis of mouse and human behavior using fully automated 3D tracking with the Anipose toolkit.</p>
<p>We first analyzed 3D hand trajectories from mice trained to reach for and grasp a pellet. This task has been extensively used to study neural circuits for sensorimotor control underlying skilled limb movements&nbsp;<span class="citation" data-cites="azim2014skilled becker_cerebellar_2019 guo_cortex_2015 low_precision_2018 farr_quantitative_2002 esposito_brainstem_2014">(<a href="#ref-azim2014skilled" role="doc-biblioref">Azim et al. 2014</a>; <a href="#ref-becker_cerebellar_2019" role="doc-biblioref">Becker and Person 2019</a>; <a href="#ref-guo_cortex_2015" role="doc-biblioref">Guo et al. 2015</a>; <a href="#ref-low_precision_2018" role="doc-biblioref">Low et al. 2018</a>; <a href="#ref-farr_quantitative_2002" role="doc-biblioref">Farr and Whishaw 2002</a>; <a href="#ref-esposito_brainstem_2014" role="doc-biblioref">Esposito, Capelli, and Arber 2014</a>)</span>. Using the Anipose visualization tool, we labeled the reach outcome and start/end frame for each trial. We labeled the trial a “hit” if the mouse successfully grasped the pellet, a “miss” if the mouse missed the pellet holder, and a “bump” if the mouse bumped into the pellet holder or the pellet but failed to grasp the pellet. Each of the four mice in the dataset had multiple instances of each outcome. <a href="#fig-flydemo">Figure&nbsp;7</a> F shows example 3D reaching trajectories, which demonstrate that reaching movements vary significantly from trial to trial (see also Figure&nbsp;S7A). Although reaching is a challenging behavior to track, due to its speed and variability, Anipose was able to accurately reconstruct forelimb reaching trajectories. The trajectory of each movement was variable, but plotting the distance to the pellet holder as a function of time to contact revealed that each reach type has a stereotyped trajectory (<a href="#fig-flydemo">Figure&nbsp;7</a> G and Figure S7B). Interestingly, the hit/bump and miss trajectories diverged around 50 ms prior to pellet contact, suggesting that mice are unable to correct their reaching trajectories in this period.</p>
<p>We next analyzed 3D walking kinematics reconstructed from the human dataset using methods similar to our analysis of fly walking. We extracted knee flexion, hip rotation, and hip flexion angles from 3D joint positions tracked with Anipose (<a href="#fig-flydemo">Figure&nbsp;7</a> H and figure S7C). The distributions of these joint angles are symmetric across the two legs (<a href="#fig-flydemo">Figure&nbsp;7</a> I and Figure S7D) and match previous characterizations of human gait&nbsp;<span class="citation" data-cites="fukuchi_public_2018">(<a href="#ref-fukuchi_public_2018" role="doc-biblioref">Fukuchi, Fukuchi, and Duarte 2018</a>)</span>. To characterize the structure of walking across the subjects, we used the UMAP nonlinear embedding method&nbsp;<span class="citation" data-cites="mcinnes2018umap-software">(<a href="#ref-mcinnes2018umap-software" role="doc-biblioref">McInnes et al. 2018</a>)</span> to embed knee flexion, hip rotation, hip flexion, and their derivatives into a 3D space, as for the fly dataset above. The UMAP embedding reveals a manifold of angle coordination across subjects (<a href="#fig-flydemo">Figure&nbsp;7</a> J). The manifold forms a cylindrical structure with the knee flexion angle mapping circularly along the cylinder (<a href="#fig-flydemo">Figure&nbsp;7</a> K). The two trials that are to the left outside the main cylinder have lower variation of left leg hip rotation (Figure&nbsp;S7E). These examples illustrate the ease and utility of tracking and analyzing human walking behavior with Anipose. In the future, this approach could be used to automatically identify individuals with distinct walking gaits or other motor patterns.</p>
</section>
</section>
<section id="discussion" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Discussion</h1>
<p>In this paper, we introduce Anipose, an open-source toolkit to accurately track animal movement in 3D. Anipose is designed to augment DeepLabCut, a toolkit for 2D markerless tracking&nbsp;<span class="citation" data-cites="mathis_deeplabcut_2018">(<a href="#ref-mathis_deeplabcut_2018" role="doc-biblioref">A. Mathis et al. 2018</a>)</span>, with calibration, filters, and a visualization tool to facilitate robust 3D tracking and analysis. Current users of DeepLabCut can easily upgrade to 3D tracking with Anipose by adding and calibrating additional cameras to an existing behavioral setup. We validated each optimization module and the full pipeline against ground truth data from four different experimental datasets and three organisms, demonstrating accurate reconstruction of 3D joint positions and angles. To help users get started, we developed detailed tutorials for both the Anipose pipeline and aniposelib at <a href="www.anipose.org">anipose.org</a>.</p>
<p>The Anipose tracking pipeline is designed to streamline structured processing of videos recorded in high-throughput experiments. Users do not need to know Python to use the Anipose pipeline. All that is required to get started is editing a small configuration file and running the provided commands from a terminal. Although we designed Anipose to leverage 2D tracking with DeepLabCut&nbsp;<span class="citation" data-cites="mathis_deeplabcut_2018">(<a href="#ref-mathis_deeplabcut_2018" role="doc-biblioref">A. Mathis et al. 2018</a>)</span>, it can be made compatible with other 2D markerless tracking methods, including SLEAP&nbsp;<span class="citation" data-cites="pereira_sleap_2020">(<a href="#ref-pereira_sleap_2020" role="doc-biblioref">Pereira et al. 2020</a>)</span> and DeepPoseKit&nbsp;<span class="citation" data-cites="graving_deepposekit_2019">(<a href="#ref-graving_deepposekit_2019" role="doc-biblioref">Graving et al. 2019</a>)</span> by modifying a single file. Users with programming experience can convert their 2D tracked data to the Anipose structure (see Figure&nbsp;S5) to take advantage of the calibration, filters, and visualization tools. We also provide access to individual functions via a separate library, aniposelib.</p>
<section id="impact-of-robust-markerless-3d-tracking" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="impact-of-robust-markerless-3d-tracking"><span class="header-section-number">3.1</span> Impact of robust markerless 3D tracking</h2>
<p>A key technical advantage of tracking with Anipose is the ability to interpret and analyze movement speed from 3D pose trajectories that are smooth in space and time, due to filtering and interpolation from multiple camera views. The resulting improvements in tracking smoothness make it easier to analyze pose and movement dynamics. Specifically, interpolated data enables the user to obtain better estimates of behavior statistics, such as mean and variance, and to perform dimensionality reduction techniques, such as principal component analysis (PCA). Additionally, temporal regularization reduces noise in the first derivative and thus enables the user to obtain more precise estimates of movement speed (<a href="#fig-optim">Figure&nbsp;5</a> D and Figure S4).</p>
<p>This ability to analyze 3D pose trajectories may open up opportunities for behavioral neuroscience, where key insights have been gained through carefully controlled behavioral paradigms. In particular, experiments are often designed to accommodate the practical limitations of movement tracking, recording neural activity, and perturbing the animal in real time (e.g., <span class="citation" data-cites="tzschentke_review_2007 dhooge_applications_2001 olton_mazes_1979 branson_high-throughput_2009 berman_mapping_2014">(<a href="#ref-tzschentke_review_2007" role="doc-biblioref">Tzschentke 2007</a>; <a href="#ref-dhooge_applications_2001" role="doc-biblioref">D’Hooge and De Deyn 2001</a>; <a href="#ref-olton_mazes_1979" role="doc-biblioref">Olton 1979</a>; <a href="#ref-branson_high-throughput_2009" role="doc-biblioref">Branson et al. 2009</a>; <a href="#ref-berman_mapping_2014" role="doc-biblioref">Berman et al. 2014</a>)</span>). Recent advances in experimental technologies (e.g., high-density extracellular recording probes&nbsp;<span class="citation" data-cites="jun_fully_2017">(<a href="#ref-jun_fully_2017" role="doc-biblioref">Jun et al. 2017</a>)</span>, optical imaging of fluorescent reporters&nbsp;<span class="citation" data-cites="dana_high-performance_2018 abdelfattah_bright_2018">(<a href="#ref-dana_high-performance_2018" role="doc-biblioref">Dana et al. 2019</a>; <a href="#ref-abdelfattah_bright_2018" role="doc-biblioref">Abdelfattah et al. 2019</a>)</span>, and optogenetics&nbsp;<span class="citation" data-cites="bernstein_optogenetics_2012">(<a href="#ref-bernstein_optogenetics_2012" role="doc-biblioref">Bernstein, Garrity, and Boyden 2012</a>)</span>) have made it feasible to precisely record and perturb neural activity from animals behaving freely in three dimensions. Complementing these technologies, a comprehensive toolbox for high-throughput 3D tracking will not only enable deeper analysis of current experiments, but also make it possible to study more natural behaviors.</p>
<p>A robust 3D markerless tracking solution could also greatly expand the accessibility of quantitative movement analysis in humans. Many neurological disorders, including some commonly thought of as cognitive disorders, affect walking gait&nbsp;<span class="citation" data-cites="stolze_prevalence_2005 wittwer_longitudinal_2010">(<a href="#ref-stolze_prevalence_2005" role="doc-biblioref">Stolze et al. 2005</a>; <a href="#ref-wittwer_longitudinal_2010" role="doc-biblioref">Wittwer, Webster, and Menz 2010</a>)</span> and upper limb coordination&nbsp;<span class="citation" data-cites="solaro_subtle_2007 tippett_visuomotor_2007">(<a href="#ref-solaro_subtle_2007" role="doc-biblioref">Solaro et al. 2007</a>; <a href="#ref-tippett_visuomotor_2007" role="doc-biblioref">Tippett, Krajewski, and Sergio 2007</a>)</span>. Many clinicians and basic researchers currently rely on qualitative evaluations or expensive clinical systems to diagnose motor disorders and assess recovery after treatment. While clinical approaches are commercially available&nbsp;<span class="citation" data-cites="windolf_systematic_2008">(<a href="#ref-windolf_systematic_2008" role="doc-biblioref">Windolf, Götzen, and Morlock 2008</a>)</span>, they are costly, require proprietary hardware, rely on the addition of markers to the patient, and cannot assess walking gait in natural contexts such as a patient’s home. Anipose could be used as a tool in the diagnosis, assessment, and rehabilitative treatment of movement and neurodegenerative disorders.</p>
</section>
<section id="insights-into-the-motor-control-of-drosophila-walking" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="insights-into-the-motor-control-of-drosophila-walking"><span class="header-section-number">3.2</span> Insights into the motor control of <em>Drosophila</em> walking</h2>
<p>By analyzing 3D joint kinematics of tethered walking <em>Drosophila</em>, we found that each leg has a unique set of joint angle distributions. One valuable insight, which was not evident from 2D tracking alone, is that the movement of the middle legs is driven primarily by femur rotation, in contrast to the front and hind legs, which are driven primarily by femur-tibia flexion. We also observed small differences in femur-tibia flexion and femur rotation distributions between front and hind legs (<a href="#fig-flydemo">Figure&nbsp;7</a> B). Thus, the neural circuits that move each leg during walking must be specialized for controlling joints with distinct forces and dynamics within each leg. Previous models of <em>Drosophila</em> walking have used an identical control architecture for intra-leg joint coordination for all six legs <span class="citation" data-cites="aminzare_gait_2018 goldsmith_neurodynamic_2020">(<a href="#ref-aminzare_gait_2018" role="doc-biblioref">Aminzare, Srivastava, and Holmes 2018</a>; <a href="#ref-goldsmith_neurodynamic_2020" role="doc-biblioref">Goldsmith, Szczecinski, and Quinn 2020</a>)</span>. Our results provide a framework for constructing more biologically plausible neuromechanical models using distinct architectures for controlling different joints within each leg.</p>
<p>Inter-leg differences in joint kinematics also raise questions about limb proprioception. Proprioceptors in the fly femoral chordotonal organ (FeCO) encode femur-tibia flexion and movement <span class="citation" data-cites="mamiya_neural_2018">(<a href="#ref-mamiya_neural_2018" role="doc-biblioref">Mamiya, Gurung, and Tuthill 2018</a>)</span>. Does the role of the FeCO differ for the middle legs, for which the femur-tibia generally does not flex in a rhythmic pattern during walking? Which proprioceptors, if any, are used to sense femur and coxa rotation of the middle legs? Answering these questions will be facilitated by combining Anipose with <em>in vivo</em> measurements and perturbations of proprioceptive neural circuits&nbsp;<span class="citation" data-cites="dallmann_leg_2021">(<a href="#ref-dallmann_leg_2021" role="doc-biblioref">Dallmann et al. 2021</a>)</span>.</p>
<p>Rythmic motor behaviors, such as walking, are thought to be controlled by central pattern generators (CPGs): neural circuits that generate intrinsic rhythmic activity <span class="citation" data-cites="bidaye_six-legged_2017">(<a href="#ref-bidaye_six-legged_2017" role="doc-biblioref">Bidaye, Bockemühl, and Büschges 2017</a>)</span>. If fly walking is controlled by CPGs, our results suggest that the CPG for each leg must control different muscles. For example, we would predict that a walking CPG for the front legs would connect to motor neurons that control the tibia flexor and extensor muscles in the femur <span class="citation" data-cites="azevedo_size_2019">(<a href="#ref-azevedo_size_2019" role="doc-biblioref">Azevedo et al. 2020</a>)</span>. In contrast, a CPG for the middle legs might connect to motor neurons innervating muscles in the trochanter that control femur rotation. These insights will be useful in guiding ongoing efforts to trace motor control circuits using connectomic reconstruction of the <em>Drosophila</em> ventral nerve cord <span class="citation" data-cites="maniates-selvin_reconstruction_2020">(<a href="#ref-maniates-selvin_reconstruction_2020" role="doc-biblioref">Maniates-Selvin et al. 2020</a>)</span> and leg <span class="citation" data-cites="kuan_dense_2020">(<a href="#ref-kuan_dense_2020" role="doc-biblioref">Kuan et al. 2020</a>)</span>.</p>
<p>Femur rotation is also likely to be important for walking in other insect species. Fransevich and Wang tested the passive rotation of the trochanter-femur articulation in 23 insect species and found rotation ranges from 10to 120, depending on the species <span class="citation" data-cites="frantsevich_gimbals_2009">(<a href="#ref-frantsevich_gimbals_2009" role="doc-biblioref">Frantsevich and Wang 2009</a>)</span>. Our estimate for the physiological range for walking <em>Drosophila</em> is about 70 degrees (<a href="#fig-flydemo">Figure&nbsp;7</a> B), which falls within the trochanter-femur articulation range observed in other insects. Thus, it is plausible that articulation of the trochanter-femur joint is sufficient to account for the femur rotation we measured during walking, and that other insects rely on femur rotation during walking as well. As an example, Bender et al.&nbsp;reported different kinematics across legs in walking cockroaches, with larger femur rotation and smaller femur-tibia flexion in the middle legs relative to the hind legs <span class="citation" data-cites="bender_computer-assisted_2010">(<a href="#ref-bender_computer-assisted_2010" role="doc-biblioref">Bender, Simpson, and Ritzmann 2010</a>)</span>. The application of Anipose to track 3D joint kinematics in other species will enable further comparative studies of the biomechanics and neural control of walking.</p>
</section>
<section id="potential-for-future-improvement-based-on-related-work" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="potential-for-future-improvement-based-on-related-work"><span class="header-section-number">3.3</span> Potential for future improvement based on related work</h2>
<p>Camera calibration has long been a rich topic in computer vision research. The most commonly used calibration code, based on Zhang’s work&nbsp;<span class="citation" data-cites="zhang_flexible_2000">(<a href="#ref-zhang_flexible_2000" role="doc-biblioref">Z. Zhang 2000</a>)</span> and part of OpenCV&nbsp;<span class="citation" data-cites="opencv_library">(<a href="#ref-opencv_library" role="doc-biblioref">Bradski 2000</a>)</span>, can calibrate up to 2 cameras using images of checkerboards from multiple angles. Although this method can be used to calibrate 3 or more cameras by calibrating pairs of cameras, in practice, precise calibration requires an additional optimization step called bundle adjustment&nbsp;<span class="citation" data-cites="goos_bundle_2000">(<a href="#ref-goos_bundle_2000" role="doc-biblioref">Triggs et al. 2000</a>)</span>. Bundle adjustment has been a key part of structure from motion toolkits <span class="citation" data-cites="agarwal_building_2011 schonberger_robust_2018">(<a href="#ref-agarwal_building_2011" role="doc-biblioref">Agarwal et al. 2011</a>; <a href="#ref-schonberger_robust_2018" role="doc-biblioref">Schönberger 2018</a>)</span>, but the method has received comparatively little attention as a solution to camera calibration for markerless tracking. An exception is DeepFly3D, which supports calibration based on animal keypoints but not based on a calibration board, which hinders its ability to handle setups with arbitrary camera positions&nbsp;<span class="citation" data-cites="gunel_deepfly3d_2019">(<a href="#ref-gunel_deepfly3d_2019" role="doc-biblioref">Günel et al. 2019</a>)</span>. Our key innovation is to provide an open source implementation of sparse bundle adjustment targeted for camera calibration for motion tracking. Our current implementation could eventually benefit from incorporating other methods from the literature. For instance, using a neural network to detect the calibration board may yield more detected keypoints and lead to more robust calibration under difficult conditions&nbsp;<span class="citation" data-cites="hu_deep_2018">(<a href="#ref-hu_deep_2018" role="doc-biblioref">Hu et al. 2018</a>)</span>. Currently, Anipose requires a calibration board to initialize camera parameters (even with animal calibration), but it may be possible to initialize camera parameters based on commonly detected points, as is commonly done in the structure from motion literature&nbsp;<span class="citation" data-cites="agarwal_building_2011 schonberger_robust_2018">(<a href="#ref-agarwal_building_2011" role="doc-biblioref">Agarwal et al. 2011</a>; <a href="#ref-schonberger_robust_2018" role="doc-biblioref">Schönberger 2018</a>)</span>, or perhaps by using a neural network directly&nbsp;<span class="citation" data-cites="ummenhofer_demon_2017">(<a href="#ref-ummenhofer_demon_2017" role="doc-biblioref">Ummenhofer et al. 2017</a>)</span>. Bundle adjustment itself may be made more robust by incorporating gauge constraints in the optimization function, further reducing the number of parameters&nbsp;<span class="citation" data-cites="goos_bundle_2000">(<a href="#ref-goos_bundle_2000" role="doc-biblioref">Triggs et al. 2000</a>)</span>. Finally, the calibration process itself may be streamlined if it were made interactive&nbsp;<span class="citation" data-cites="richardson_aprilcal_2013">(<a href="#ref-richardson_aprilcal_2013" role="doc-biblioref">Richardson, Strom, and Olson 2013</a>)</span>.</p>
<p>There has been extensive recent work to improve markerless tracking based on deep learning approaches. One common approach has been to improve the neural network architecture for training. For instance, this approach has been used to induce priors in the neural network based on occlusions <span class="citation" data-cites="sarandi_synthetic_2018 cheng_occlusion-aware_2019">(<a href="#ref-sarandi_synthetic_2018" role="doc-biblioref">Sárándi et al. 2018</a>; <a href="#ref-cheng_occlusion-aware_2019" role="doc-biblioref">Cheng et al. 2019</a>)</span>, multi-view geometry <span class="citation" data-cites="iskakov_learnable_2019 zimmermann_freipose_2020 dunn_geometric_2021 yao_monet_2019">(<a href="#ref-iskakov_learnable_2019" role="doc-biblioref">Iskakov et al. 2019</a>; <a href="#ref-zimmermann_freipose_2020" role="doc-biblioref">Zimmermann et al. 2020</a>; <a href="#ref-dunn_geometric_2021" role="doc-biblioref">Dunn et al. 2021</a>; <a href="#ref-yao_monet_2019" role="doc-biblioref">Yao, Jafarian, and Park 2019</a>)</span>, limb lengths&nbsp;<span class="citation" data-cites="zhou_towards_2017">(<a href="#ref-zhou_towards_2017" role="doc-biblioref">X. Zhou et al. 2017</a>)</span>, or time&nbsp;<span class="citation" data-cites="nunez_multiview_2019">(<a href="#ref-nunez_multiview_2019" role="doc-biblioref">Núñez et al. 2019</a>)</span>. We note that this approach is complementary to our work, as the Anipose filters could be used with keypoint detection by any neural network. Another approach is to resolve tracking by using pictorial structures to add priors on limb lengths&nbsp;<span class="citation" data-cites="yang_end-end_2016 amin_multi-view_2013 gunel_deepfly3d_2019">(<a href="#ref-yang_end-end_2016" role="doc-biblioref">Yang et al. 2016</a>; <a href="#ref-amin_multi-view_2013" role="doc-biblioref">Amin et al. 2013</a>; <a href="#ref-gunel_deepfly3d_2019" role="doc-biblioref">Günel et al. 2019</a>)</span> or motion&nbsp;<span class="citation" data-cites="wu_deep_2020">(<a href="#ref-wu_deep_2020" role="doc-biblioref">A. Wu et al. 2020</a>)</span> or both&nbsp;<span class="citation" data-cites="zhang_animal_2021">(<a href="#ref-zhang_animal_2021" role="doc-biblioref">L. Zhang et al. 2021</a>)</span>. The Viterbi filter used in Anipose is analogous to the motion based pictorial structures and could be further extended to handle priors on limb lengths based on insights from these papers. Beyond tracking single animals, toolboxes like SLEAP&nbsp;<span class="citation" data-cites="pereira_fast_2019">(<a href="#ref-pereira_fast_2019" role="doc-biblioref">Pereira et al. 2019</a>)</span>, OpenPose&nbsp;<span class="citation" data-cites="cao_openpose_2018">(<a href="#ref-cao_openpose_2018" role="doc-biblioref">Cao et al. 2019</a>)</span>, and DeepLabCut&nbsp;<span class="citation" data-cites="nath_deeplabcut_2019">(<a href="#ref-nath_deeplabcut_2019" role="doc-biblioref">Nath et al. 2019</a>)</span> have some support for multi-animal pose estimation in 2D. For tracking multiple animals in 3D, a promising approach is to build correspondences based on geometry and appearance&nbsp;<span class="citation" data-cites="dong_fast_2019">(<a href="#ref-dong_fast_2019" role="doc-biblioref">Dong et al. 2019</a>)</span> across multiple views. As automated, high-throughput tracking of animal behavior grows in scale, new methods for data analysis, visualization, and modeling will also be needed to gain insight into the neural control of dynamic behavior <span class="citation" data-cites="york_treble_2020 marshall_2021 berman_mapping_2014 dallmann_leg_2021">(<a href="#ref-york_treble_2020" role="doc-biblioref">York, Giocomo, and Clandinin 2020</a>; <a href="#ref-marshall_2021" role="doc-biblioref">Marshall et al. 2021</a>; <a href="#ref-berman_mapping_2014" role="doc-biblioref">Berman et al. 2014</a>; <a href="#ref-dallmann_leg_2021" role="doc-biblioref">Dallmann et al. 2021</a>)</span>.</p>
</section>
<section id="limitations-and-practical-recommendations" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="limitations-and-practical-recommendations"><span class="header-section-number">3.4</span> Limitations and practical recommendations</h2>
<p>There are several common scenarios under which Anipose may fail to produce accurate 3D tracking. Below, we enumerate some of the scenarios we have encountered in applying Anipose on different datasets and suggest practical strategies for troubleshooting.</p>
<p>As is the case for any tracking system, the ability of Anipose to track and estimate body pose is fundamentally limited by the quality of the underlying data. High quality videos are well illuminated, contain minimal motion blur, and provide coverage of each keypoint from different views. A common failure mode we encountered was when the neural network misplaced 2D keypoints in some frames. If the errors are uncorrelated across camera views, then the Anipose filters can compensate and still produce accurate tracking in 3D. But in some cases, multiple views have correlated errors or these errors persist in time. These type of errors most commonly arise when the neural network has not been trained on a subset of rare behaviors, so that the animal adopts poses unseen by the trained network. One solution to reducing the frequency of such errors involves systematically identifying outlier frames, manually relabeling them, then retraining the network. Anipose supports this functionality, as do other tracking toolboxes&nbsp;<span class="citation" data-cites="mathis_deeplabcut_2018 pereira_fast_2019 graving_deepposekit_2019 gunel_deepfly3d_2019">(<a href="#ref-mathis_deeplabcut_2018" role="doc-biblioref">A. Mathis et al. 2018</a>; <a href="#ref-pereira_fast_2019" role="doc-biblioref">Pereira et al. 2019</a>; <a href="#ref-graving_deepposekit_2019" role="doc-biblioref">Graving et al. 2019</a>; <a href="#ref-gunel_deepfly3d_2019" role="doc-biblioref">Günel et al. 2019</a>)</span>.</p>
<p>Poor multi-camera calibration also results in tracking errors. A good calibration should have an average reprojection error of less than 3 pixels, and ideally less than 1 pixel. To obtain a quality calibration, the calibration videos should be recorded so that the board is clearly visible from multiple angles and locations on each camera. If it is not possible to achieve this, we suggest exploring a preliminary calibration module in Anipose that refines an initial calibration based on the detected points on the animal itself. This module was inspired by the animal based calibration in DeepFly3D&nbsp;<span class="citation" data-cites="gunel_deepfly3d_2019">(<a href="#ref-gunel_deepfly3d_2019" role="doc-biblioref">Günel et al. 2019</a>)</span>, but our implementation uses the initial calibration from a calibration board as a starting guess, permitting generalization in different setups. It also takes advantage of our iterative calibration procedure to yield robust calibration even with errors in tracking.</p>
<p>An effective experimental setup needs to have an appropriate number of cameras to track all keypoints across possible pose configurations. In particular, each joint must be visible from at least 2 cameras at all times. Thus, for tracking multiple limbs or body parts, we recommend at least 3 equally spaced cameras, so that half of the body is visible from any single camera. We evaluated this quantitatively in the human dataset (Table S2), where there is a dramatic reduction in error from 2 to 3 cameras.</p>
<p>The mouse reaching dataset is one example where tracking was reasonably accurate without filters, but filters did not further improve tracking accuracy. There are several potential explanations for this result. The reaches are very short (about 40-100 frames or 200-500ms) and the hand is hard to see when it is on the ground, so temporal filters such as the Viterbi filter or temporal regularization lack the information to resolve tracking errors. There are very few keypoints (only 3 per hand) and these can change in distance relative to each other, so the spatial regularization cannot impose strong constraints. With only 2 cameras, the spatiotemporal regularization cannot fully leverage multiple views to remove outliers (Table&nbsp;S2) and the autoencoder has limited utility. In this situation, using basic linear least-squares triangulation works well enough for analysis (<a href="#fig-flydemo">Figure&nbsp;7</a> F and G). The accuracy of tracking mouse reaching might be improved by labeling more keypoints on each hand, increasing the camera frame rate, and adding more cameras.</p>
<p>As a practical starting point, we recommend users start with no filters to first evaluate the quality of the tracking. If outliers or missing data impede data analysis, then we recommend enabling the default filter parameters in Anipose, which we have found to produce good tracking results across multiple datasets. In some cases, some additional tuning of parameters may be required, especially on datasets with unique constraints or when studying behaviors with unusual dynamics. If any joints are not visible for an extended period of time in certain videos, we recommend disabling the spatiotemporal optimization, as it can hallucinate trajectories, increasing overall error (as in Table S2). We provide suggestions for tuning parameters in our documentation at <a href="www.anipose.org">anipose.org</a>.</p>
</section>
<section id="outlook" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="outlook"><span class="header-section-number">3.5</span> Outlook</h2>
<p>We designed Anipose to make markerless 3D tracking simple and broadly accessible for the scientific community. With this goal in mind, we built Anipose on DeepLabCut, a widely used 2D tracking toolkit. As many labs develop machine learning tools for behavior tracking and analysis, we advocate for pooling efforts around common frameworks that emphasize usability&nbsp;<span class="citation" data-cites="kane_real-time_2020 Saunders_autopilot_2019">(<a href="#ref-kane_real-time_2020" role="doc-biblioref">Kane et al. 2020</a>; <a href="#ref-Saunders_autopilot_2019" role="doc-biblioref">Saunders and Wehr 2019</a>)</span>. In particular, we suggest that tools be built in a modular way, so that code can be extended and reused in other frameworks. We hope that the Anipose toolkit contributes to these community efforts. We welcome contributions to improve and extend the Anipose toolkit and conversely are ready to contribute the ideas and code from Anipose to other toolkits.</p>
</section>
</section>
<section id="people-involved" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> People involved</h1>
<section id="acknowledgments" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="acknowledgments">Acknowledgments</h2>
<p>We thank Su-Yee Lee and Chris Dallmann for help with annotating keypoints on flies, John So for help with annotating keypoints on the ChArUco board, and Sam Mak for help with annotating mice keypoints and fly behavior. We thank Stephen Huston for loan of his calibration board and Julian Pitney for contributing code to check calibration board detections to Anipose. Finally, we thank the Tuthill and Brunton labs and Mackenzie and Alexander Mathis for support, suggestions, and feedback on the manuscript.</p>
<p>PK was supported by a National Science Foundation Graduate Research Fellowship. KLR was supported by fellowships from the University of Washington’s Institute for Neuroengineering (UWIN) and Center for Neurotechnology (CNT). ESD was supported by a fellowship from University of Washington’s Institute for Neuroengineering. ES was supported by the National Institutes of Health (F31NS115477). EA was supported by the National Institutes of Health (R00 NS088193, DP2NS105555, R01NS111479, and U19NS112959), the Searle Scholars Program, The Pew Charitable Trusts, and the McKnight Foundation. BWB was supported by a Sloan Research Fellowship and the Washington Research Foundation. JCT was supported by the Searle Scholar Program, the Pew Biomedical Scholar Program, the McKnight Foundation, and National Institute of Health grants R01NS102333 and U19NS104655. JCT is a New York Stem Cell Foundation – Robertson Investigator.</p>
</section>
<section id="contributions" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="contributions">Contributions</h2>
<p>PK, BWB, and JCT conceived the project. PK designed, implemented, and evaluated the Anipose toolkit. KR wrote the Anipose documentation, contributed Tensorpack data augmentation to DeepLabCut, and wrote key parts of the Anipose visualizer. ESD and SWB collected the ChArUco and fly datasets. ES and EA collected the mouse dataset. PK, BWB, and JCT wrote the paper, with input from KR, ESD, ES, and EA.</p>
</section>
<section id="declaration-of-interests" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="declaration-of-interests">Declaration of Interests</h2>
<p>PK is a founder and the Chief Science Officer of Evolution Devices, a company that uses motion tracking to help people with walking disorders.</p>
</section>
<section id="inclusion-and-diversity" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="inclusion-and-diversity">Inclusion and Diversity</h2>
<p>One or more of the authors of this paper self-identifies as an underrepresented ethnic minority in science. One or more of the authors of this paper self-identifies as a member of the LGBTQ+ community.</p>
</section>
</section>
<section id="methods" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Methods</h1>
<section id="resource-availability" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="resource-availability"><span class="header-section-number">5.1</span> Resource availability</h2>
<section id="lead-contact" class="level4" data-number="5.1.0.1">
<h4 data-number="5.1.0.1" class="anchored" data-anchor-id="lead-contact"><span class="header-section-number">5.1.0.1</span> Lead contact</h4>
<p>Further information and requests for resources should be directed to and will be fulfilled by the lead contact, John Tuthill (tuthill@uw.edu).</p>
</section>
<section id="materials-availability" class="level4" data-number="5.1.0.2">
<h4 data-number="5.1.0.2" class="anchored" data-anchor-id="materials-availability"><span class="header-section-number">5.1.0.2</span> Materials availability</h4>
<p>This study did not generate new unique reagents.</p>
</section>
<section id="data-and-code-availability" class="level4" data-number="5.1.0.3">
<h4 data-number="5.1.0.3" class="anchored" data-anchor-id="data-and-code-availability"><span class="header-section-number">5.1.0.3</span> Data and code availability</h4>
<ul>
<li><p>Data has been deposited at https://doi.org/10.5061/dryad.nzs7h44s4 and are publicly available as of the date of publication. DOIs are listed in the key resources table.</p></li>
<li><p>All original code has been deposited at <a href="https://doi.org/10.5281/zenodo.5224213" class="uri">https://doi.org/10.5281/zenodo.5224213</a>. Documentation for the software is available at <a href="www.anipose.org">anipose.org</a>. DOIs are listed in the key resources table.</p></li>
<li><p>Any additional information required to reanalyze the data reported in this paper is available from the lead contact upon request.</p></li>
</ul>
</section>
</section>
<section id="experimental-model-and-subject-details" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="experimental-model-and-subject-details"><span class="header-section-number">5.2</span> Experimental model and subject details</h2>
<section id="mouse" class="level4" data-number="5.2.0.1">
<h4 data-number="5.2.0.1" class="anchored" data-anchor-id="mouse"><span class="header-section-number">5.2.0.1</span> Mouse</h4>
<p>Reaching data were obtained from four adult C57BL/6 mice (JAX:000664, <span class="math inline">\sim</span> 8-12 weeks old, two male and two female) trained to reach for a pellet. Procedures performed in this study were conducted according to US National Institutes of Health guidelines for animal research and were approved by the Institutional Animal Care and Use Committee of The Salk Institute for Biological Studies.</p>
</section>
<section id="fly" class="level4" data-number="5.2.0.2">
<h4 data-number="5.2.0.2" class="anchored" data-anchor-id="fly"><span class="header-section-number">5.2.0.2</span> Fly</h4>
<p>Male and female Berlin wild type <em>Drosophila melanogaster</em> (RRID:BDSC_8522), 4 days post-eclosion, were used for all experiments. Flies were reared on standard cornmeal agar food on a 14 hr/10 hr light-dark cycle at 25 C in 70% relative humidity.</p>
</section>
<section id="human" class="level4" data-number="5.2.0.3">
<h4 data-number="5.2.0.3" class="anchored" data-anchor-id="human"><span class="header-section-number">5.2.0.3</span> Human</h4>
<p>We evaluated 3D tracking with Anipose on the Human 3.6M dataset&nbsp;<span class="citation" data-cites="h36m_pami IonescuSminchisescu11">(<a href="#ref-h36m_pami" role="doc-biblioref">Ionescu et al. 2014</a>; <a href="#ref-IonescuSminchisescu11" role="doc-biblioref">Catalin Ionescu 2011</a>)</span>. The Human 3.6M dataset contains data from 5 subjects as a training dataset (2 female and 3 male), 2 subjects as a validation dataset, and 4 subjects as a testing dataset (2 female and 2 male).</p>
</section>
</section>
<section id="method-details" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="method-details"><span class="header-section-number">5.3</span> Method details</h2>
<section id="charuco-dataset." class="level4" data-number="5.3.0.1">
<h4 data-number="5.3.0.1" class="anchored" data-anchor-id="charuco-dataset."><span class="header-section-number">5.3.0.1</span> ChArUco dataset.</h4>
<p>To evaluate the performance of Anipose compared to physical ground truth, we collected videos of a precision-manufactured ChArUco board&nbsp;<span class="citation" data-cites="garrido-jurado_automatic_2014">(<a href="#ref-garrido-jurado_automatic_2014" role="doc-biblioref">Garrido-Jurado et al. 2014</a>)</span>. The ChArUco board was manufactured by Applied Image Inc (Rochester, NY) with a tolerance of 2&nbsp;μm in length and 2&nbsp;in angle. It is a 2 mm <span class="math inline">\times</span> 2 mm etching of opal and blue chrome, on a 5 mm <span class="math inline">\times</span> 5 mm board. The ChArUco pattern itself has 6 <span class="math inline">\times</span> 6 squares, with 4 bit markers and a dictionary size of 50 markers. With these parameters, the size of each marker is 0.375 mm and the size of each square is 0.5 mm. We filmed the ChArUco board from 6 cameras (Basler acA800-510) evenly distributed around the board (<a href="#fig-datasets">Figure&nbsp;1</a> A), at 30Hz and with a resolution of 832 x 632 pixels, for 2-3 minutes each day over 2 separate days. While filming, we manually rotated the ChArUco board within the field of view of the cameras. These videos were used as calibration videos for both the ChArUco dataset and the fly dataset detailed below.</p>
<p>We chose 9 of the corners as keypoints for manual annotation and detection (<a href="#fig-datasets">Figure&nbsp;1</a> A and <a href="#fig-raw">Figure&nbsp;3</a> A). We extracted and manually annotated 200 frames from each camera from day 1, and an additional 200 cameras per camera from day 2 (1200 frames per day, 2400 frames total). We used the frames for day 1 for training the neural network and the frames from day 2 for evaluation of all methods.</p>
</section>
<section id="mouse-dataset." class="level4" data-number="5.3.0.2">
<h4 data-number="5.3.0.2" class="anchored" data-anchor-id="mouse-dataset."><span class="header-section-number">5.3.0.2</span> Mouse dataset.</h4>
<p>The reaching task is described in detail elsewhere <span class="citation" data-cites="azim2014skilled">(<a href="#ref-azim2014skilled" role="doc-biblioref">Azim et al. 2014</a>)</span>. Briefly, the training protocol consisted of placing the mouse in a 20 cm tall <span class="math inline">\times</span> 8.5 cm wide <span class="math inline">\times</span> 19.5 cm long clear acrylic box with an opening in the front of the box measuring 0.9 cm wide and 9 cm tall. A 3D-printed, 1.8 cm tall pedestal designed to hold a food pellet (20 mg, 3 mm diameter; Bio-Serv) was placed 1 cm away from the front of the box opening and displaced to one side by 0.5 cm (to encourage mice to use their preferred forelimb), and food pellets were placed on top as the reaching target (<a href="#fig-datasets">Figure&nbsp;1</a> B). Mice were food deprived to $<span class="math inline">85% of their original body weight and trained to reach for food pellets for either 20 minutes or until 20 successful reaches (defined as pellet retrieval) were accomplished. Mice were trained in this setup for 14 consecutive days before reaches were captured with 2 cameras (Sentech STC-MBS241U3V with Tamron M112FM16 16mm lens) placed in front and to the side of the mouse (</span>85^$ apart). Videos were acquired at a frame rate of 200 Hz at a resolution of 1024 <span class="math inline">\times</span> 768 pixels.</p>
<p>We chose 6 points on the mouse hands as keypoints (<a href="#fig-datasets">Figure&nbsp;1</a> B). On each mouse hand, we labeled 3 points: the dorsal wrist, the base of digit 5, and the proximal end of digit 3. In total, we manually labeled 2200 frames (1100 frames per camera) for training the neural network from 2 mice. For test data to evaluate the post estimation performance, we labeled an additional 400 frames (200 frames per camera) taken from videos of 2 mice that were not in the training set.</p>
</section>
<section id="fly-dataset." class="level4" data-number="5.3.0.3">
<h4 data-number="5.3.0.3" class="anchored" data-anchor-id="fly-dataset."><span class="header-section-number">5.3.0.3</span> Fly dataset.</h4>
<p>We next evaluated 3D tracking with Anipose on walking fruit flies. The flies’ wings were clipped 24-48 hours prior to the experiment in order to increase walking and prevent visual obstruction of the legs and thorax. For all experiments, a tungsten wire was tethered to the dorsal thorax of a cold-anesthetized fly with UV cured glue. Flies were starved with access to water for 2–15 hours before they were tethered. After &nbsp;20 minutes of recovery, tethered flies were positioned on a frictionless spherical treadmill&nbsp;<span class="citation" data-cites="buchner_elementary_1976 gotz_visual_1973">(<a href="#ref-buchner_elementary_1976" role="doc-biblioref">Buchner 1976</a>; <a href="#ref-gotz_visual_1973" role="doc-biblioref">Götz 1973</a>)</span> (hand-milled foam ball, density: 7.3 mg/mm<span class="math inline">^3</span>, diameter: 9.46 mm) suspended on a stream of compressed air (5 L/min). Six cameras (imaging at 300 Hz, Basler acA800-510<span class="math inline">\SI{}{\micro\meter}</span> with Computar zoom lens MLM3X-MP) were evenly distributed around the fly, providing full video coverage of all six legs (<a href="#fig-datasets">Figure&nbsp;1</a> C). Fly behavior was recorded in 2 second trials, capturing a range of behaviors such as walking, turning, grooming, and pushing against the ball. The recording region of each video was cropped slightly so that the fly filled the frame and the camera was able to acquire at 300Hz. For all training and test evaluation data, the interval between trials was 25 seconds. For some of the flies in the larger walking dataset used in <a href="#fig-flydemo">Figure&nbsp;7</a> , the interval between trials was set to 9 seconds.</p>
<p>We selected 30 points on the fly as keypoints (<a href="#fig-datasets">Figure&nbsp;1</a> C). On each fly leg, we labeled 5 points: the body-coxa, coxa-femur, femur-tibia, and tibia-tarsus joints, as well as the tip of the tarsus. In total, we manually labeled 6632 frames (about 1105 frames per camera) for training the neural network. For test data to evaluate the post estimation performance, we labeled an additional 1200 frames (200 frames per camera) taken from videos of 5 flies that were not in the training set. For analyzing flexion and rotation of angles during walking in <a href="#fig-flydemo">Figure&nbsp;7</a> , we used a larger dataset of videos from 39 flies, all collected with the methods described above.</p>
</section>
<section id="human-dataset." class="level4" data-number="5.3.0.4">
<h4 data-number="5.3.0.4" class="anchored" data-anchor-id="human-dataset."><span class="header-section-number">5.3.0.4</span> Human dataset.</h4>
<p>We evaluated 3D tracking with Anipose on the Human 3.6M dataset&nbsp;<span class="citation" data-cites="h36m_pami IonescuSminchisescu11">(<a href="#ref-h36m_pami" role="doc-biblioref">Ionescu et al. 2014</a>; <a href="#ref-IonescuSminchisescu11" role="doc-biblioref">Catalin Ionescu 2011</a>)</span>. Because this dataset has been used extensively for human pose estimation, it provides a useful comparison to existing computer vision methods. It consists of 11 professional actors performing a range of actions, including greeting, posing, sitting, and smoking. The actors were filmed in a 4m <span class="math inline">\times</span> 3m space with 4 video cameras (Basler piA1000) at a resolution of 1000 <span class="math inline">\times</span> 1000 pixels at 50Hz (<a href="#fig-datasets">Figure&nbsp;1</a> D). To gather ground-truth pose data, the actors were also outfitted with reflective body markers and tracked with a separate motion capture system, using 10 Vicon cameras at 200 Hz. Leveraging these recordings, the authors derived the precise 3D positions of 32 body joints and their 2D projections onto the videos. For camera calibration, we used the camera parameters from the Human 3.6M dataset, converted by Martinez et al.&nbsp;<span class="citation" data-cites="martinez_simple_2017">(<a href="#ref-martinez_simple_2017" role="doc-biblioref">Martinez et al. 2017</a>)</span>.</p>
<p>To compare the performance of Anipose against previous methods, we used a protocol from the literature&nbsp;<span class="citation" data-cites="iskakov_learnable_2019">(<a href="#ref-iskakov_learnable_2019" role="doc-biblioref">Iskakov et al. 2019</a>)</span>. We used frames from the training dataset to train the network and evaluated the predictions on the validation dataset. We also removed frames from the training dataset in which the subject did not move relative to the previous frame (<span class="math inline">&lt; 40</span>mm movement of all joints from the previous frame). We evaluated the tracked human dataset on every 64th frame. We used 17 of the 32 provided joints as keypoints (<a href="#fig-datasets">Figure&nbsp;1</a> D). Iskakov et al.&nbsp;<span class="citation" data-cites="iskakov_learnable_2019">(<a href="#ref-iskakov_learnable_2019" role="doc-biblioref">Iskakov et al. 2019</a>)</span> showed that some scenes from the S9 validation actor (parts of the Greeting, SittingDown, and Waiting actions) have ground-truth shifted in global coordinates compared to the actual position&nbsp;<span class="citation" data-cites="iskakov_learnable_2019">(<a href="#ref-iskakov_learnable_2019" role="doc-biblioref">Iskakov et al. 2019</a>)</span>, so we exclude these scenes from the evaluation set. Furthermore, for subject S11, one of the videos is corrupted (part of the "Directions" action), so we exclude this from the dataset as well. In total, we obtained 636,724 frames (159,181 per camera) for training the neural network, and 8608 frames (2152 per camera) frames for evaluation.</p>
</section>
<section id="manual-annotation-of-datasets." class="level4" data-number="5.3.0.5">
<h4 data-number="5.3.0.5" class="anchored" data-anchor-id="manual-annotation-of-datasets."><span class="header-section-number">5.3.0.5</span> Manual annotation of datasets.</h4>
<p>To produce neural network training data, we annotated the fly dataset using Fiji&nbsp;<span class="citation" data-cites="schindelin_fiji_2012">(<a href="#ref-schindelin_fiji_2012" role="doc-biblioref">Schindelin et al. 2012</a>)</span> and the VGG Image Annotator (VIA)&nbsp;<span class="citation" data-cites="dutta2016via dutta2019vgg">(<a href="#ref-dutta2016via" role="doc-biblioref">A. Dutta, Gupta, and Zissermann 2016</a>; <a href="#ref-dutta2019vgg" role="doc-biblioref">Abhishek Dutta and Zisserman 2019</a>)</span>. All the images in the fly test set were annotated with VIA. We annotated all the images in the ChArUco dataset and mouse dataset with VIA.</p>
</section>
</section>
<section id="quantification-and-statistical-analysis" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="quantification-and-statistical-analysis"><span class="header-section-number">5.4</span> Quantification and statistical analysis</h2>
<section id="neural-network-keypoint-detections" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="neural-network-keypoint-detections"><span class="header-section-number">5.4.1</span> Neural network keypoint detections</h3>
<p>Detection of keypoints in each of the datasets was performed with DeepLabCut 2.1.4&nbsp;<span class="citation" data-cites="nath_deeplabcut_2019">(<a href="#ref-nath_deeplabcut_2019" role="doc-biblioref">Nath et al. 2019</a>)</span>. Briefly, to produce training data, we used k-means clustering to pick out unique frames from each of the views, then manually annotated the keypoints in each frame. We trained a single Resnet-50&nbsp;<span class="citation" data-cites="he_deep_2015">(<a href="#ref-he_deep_2015" role="doc-biblioref">He et al. 2016</a>)</span> network for all camera views for the fly, mouse, and ChArUco datasets, starting from a network pretrained on Imagenet. For the human dataset, we started with a Resnet-101 network pretrained on the MPII human pose dataset <span class="citation" data-cites="insafutdinov_deepercut_2016">(<a href="#ref-insafutdinov_deepercut_2016" role="doc-biblioref">Insafutdinov et al. 2016</a>)</span>. During training, we augmented the training dataset with cropping, rotation, brightness, blur, and scaling augmentations using Tensorpack&nbsp;<span class="citation" data-cites="wu2016tensorpack">(<a href="#ref-wu2016tensorpack" role="doc-biblioref">Y. Wu et al. 2016</a>)</span>. We then used the Anipose pipeline to run the network on each video. For each keypoint, the network produced a list of predicted positions, each associated with a confidence score (between 0 and 1). We saved the top-n most likely predictions of each joint location for each frame for use in Viterbi filtering of likely keypoints in 2D, as described below.</p>
</section>
<section id="filtering-of-2d-keypoint-detections" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="filtering-of-2d-keypoint-detections"><span class="header-section-number">5.4.2</span> Filtering of 2D keypoint detections</h3>
<p>The raw keypoint detections obtained with DeepLabCut were often noisy or erroneous (<a href="#fig-filter2d">Figure&nbsp;4</a>). Thus, filtering the detections from each camera was necessary before triangulating the points. Anipose contains 3 main algorithms to filter keypoint detections; we elaborate on each algorithm below. Example applications of these filters and results are compared in <a href="#fig-filter2d">Figure&nbsp;4</a>.</p>
<section id="median-filter." class="level4" data-number="5.4.2.1">
<h4 data-number="5.4.2.1" class="anchored" data-anchor-id="median-filter."><span class="header-section-number">5.4.2.1</span> Median filter.</h4>
<p>The first algorithm identifies outlier keypoint detections by comparing the raw detected trajectories to median filtered trajectories for each joint. We started by computing a median filter on the detected trajectory for each joint’s x and y positions, which smooths the trajectory estimate. We then compared the offset of each point in the raw trajectory to the median filtered trajectory. If a point deviated by some threshold number of pixels, then we denoted this point as an outlier and removed it from the data. The missing points were then interpolated by fitting a cubic spline to the neighboring points. The median filter is simple and intuitive, but it cannot correct errors spanning multiple frames.</p>
</section>
<section id="viterbi-filter." class="level4" data-number="5.4.2.2">
<h4 data-number="5.4.2.2" class="anchored" data-anchor-id="viterbi-filter."><span class="header-section-number">5.4.2.2</span> Viterbi filter.</h4>
<p>To correct for errors that persist over multiple frames, we implemented the Viterbi algorithm to obtain a single most consistent path in time from the top-n predicted keypoints in each frame for each joint. To be specific, we expressed this problem as a hidden Markov model for each joint, wherein the possible values at each frame are the multiple possible detections of this keypoint. To obtain a cleaner model, we removed duplicate detections (within 7 pixels of each other) within each frame. To compensate for missed detected keypoints over many frames, we augmented the possible values at each frame with all detections up to <span class="math inline">F</span> previous frames, weighted in time elapsed by multiplying their probability <span class="math inline">2^{-F}</span>. We then identified the best path through the hidden Markov model using the Viterbi algorithm&nbsp;<span class="citation" data-cites="david_g._forney_viterbi_1973">(<a href="#ref-david_g._forney_viterbi_1973" role="doc-biblioref">David G. Forney 1973</a>)</span>. This procedure estimates a consistent path, even with missed detections of up to <span class="math inline">F</span> frames.</p>
</section>
<section id="autoencoder-filter." class="level4" data-number="5.4.2.3">
<h4 data-number="5.4.2.3" class="anchored" data-anchor-id="autoencoder-filter."><span class="header-section-number">5.4.2.3</span> Autoencoder filter.</h4>
<p>We found that the network would often try to predict a joint location even when the joint was occluded in that view. This type of error is particularly problematic when used in subsequent 3D triangulation. The convolutional neural network confidence scores associated with these predictions can be high, making them difficult to distinguish from correct, high-confidence predictions. To remove these errors, inspired by&nbsp;<span class="citation" data-cites="murphy_markerless_2019">(<a href="#ref-murphy_markerless_2019" role="doc-biblioref">Murphy 2019</a>)</span>, we implemented a neural network that takes in a set of confidence scores from all keypoints in one frame, and outputs a corrected set of confidence scores. To generate a training set, we made use of the fact that human annotators do not label occluded joints but label all of the visible joints in each frame. Thus, we generated artificial scores from biased distributions to mimic what the convolutional neural network might predict for each frame, with visible joints given a higher probability on average. Specifically, we sample the scores from a normal distribution, with standard deviation of 0.3 and mean 0 for invisible and 1 for visible joints, clipped to be between 0 and 1. To mimic false positive or false negative detections, we flip 5% of the scores (<span class="math inline">x \to 1 - x</span>) at random. The task of the network is to predict a high score for each joint that is truly visible in that frame and a low score for any occluded joint. The network is a multilayer perceptron network with a single hidden layer and tanh activation units to perform this task. The size of the hidden layer is the number of joints (e.g.&nbsp;if there are 10 joint scores to predict, we set the hidden layer to 10 units). We trained the network using the Adam optimizer&nbsp;<span class="citation" data-cites="kingma_adam_2017">(<a href="#ref-kingma_adam_2017" role="doc-biblioref">Kingma and Ba 2017</a>)</span> implemented in the scikit-learn library&nbsp;<span class="citation" data-cites="pedregosa_scikit-learn:_2011">(<a href="#ref-pedregosa_scikit-learn:_2011" role="doc-biblioref">Pedregosa et al. 2011</a>)</span></p>
</section>
</section>
<section id="calibration-of-multiple-cameras." class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="calibration-of-multiple-cameras."><span class="header-section-number">5.4.3</span> Calibration of multiple cameras.</h3>
<section id="camera-model." class="level4" data-number="5.4.3.1">
<h4 data-number="5.4.3.1" class="anchored" data-anchor-id="camera-model."><span class="header-section-number">5.4.3.1</span> Camera model.</h4>
<p>A camera captures 2D images of light reflecting from 3D objects; thus, we can think of each camera as a projection, transforming 3D vectors to 2D vectors. To establish our notation, for a point <span class="math inline">\bm{p}= (x, y, z)^T</span> or <span class="math inline">\bm{u}= (x, y)^T</span>, we use a tilde to denote that point in homogeneous coordinates (with a 1 at the end), so that <span class="math inline">\bm{\tilde{p}}= (x, y, z, 1)^T</span> or <span class="math inline">\bm{\tilde{u}}= (x, y, 1)^T</span>.</p>
<p>A camera model specifies a transformation from a 3D point <span class="math inline">\bm{\tilde{p}}</span> to a 2D point <span class="math inline">\bm{\tilde{u}}</span>. We use the camera model described by Zhang&nbsp;<span class="citation" data-cites="zhang_flexible_2000">(<a href="#ref-zhang_flexible_2000" role="doc-biblioref">Z. Zhang 2000</a>)</span>, which consists of a product of an intrinsics matrix <span class="math inline">\mathbf{A}</span>, an extrinsics matrix <span class="math inline">\mathbf{P}</span>, and a distortion function <span class="math inline">\mathcal{D}</span>.</p>
<p>The extrinsics matrix <span class="math inline">\mathbf{P}\in \mathbb{R}^{4 \times 3}</span> describes how the camera is positioned relative to the world. We represent <span class="math inline">\mathbf{P}</span> as the product of a rotation matrix and a translation matrix. Both rotations and translations may be fully specified with 3 parameters each, for 6 parameters total in <span class="math inline">\mathbf{P}</span>.</p>
<p>The intrinsics matrix <span class="math inline">\mathbf{A}\in \mathbb{R}^{3 \times 3}</span> describes the internal coordinate system of the camera. It is often modeled using 5 parameters: focal length terms <span class="math inline">f_x</span> and <span class="math inline">f_y</span>, offset terms <span class="math inline">c_x</span> and <span class="math inline">c_y</span>, and a skew parameter <span class="math inline">s</span>: <span class="math display">A =
\begin{bmatrix}
  f_x &amp; s &amp; c_x \\
  0 &amp; f_y &amp; c_y \\
  0 &amp; 0 &amp; 1 \\
\end{bmatrix}.</span> In practice, we found that we obtain a more robust calibration by reducing the number of parameters, setting <span class="math inline">f = f_x = f_y</span>, <span class="math inline">s = 0</span>, and <span class="math inline">(c_x, c_y)</span> to be at the center of the image, so that we need to estimate only the focal length parameter <span class="math inline">f</span> for the intrinsics matrix.</p>
<p>The distortion function models nonlinear distortions in the camera pixel grid. This distortion is typically modeled with 3 parameters as <span class="math display">\begin{aligned}
  &amp; \mathcal{D}([x, y]) = \\
  &amp; \begin{bmatrix}
    x + x\left(k_1 (x^2 + y^2) + k_2(x^2+y^2)^2 + k_3(x^2+y^2)^4 \right) \\
    y + y\left(k_1 (x^2 + y^2) + k_2(x^2+y^2)^2 + k_3(x^2+y^2)^4 \right)
  \end{bmatrix}.\end{aligned}</span> In practice, we found that the higher-order distortion terms <span class="math inline">k_2</span> and <span class="math inline">k_3</span> are often small for modern cameras, so we assume <span class="math inline">k_2 = k_3 = 0</span> and only estimate a single parameter <span class="math inline">k_1</span>.</p>
<p>Thus, the full mapping may be written as <span class="math display">\begin{aligned}
   \bm{\tilde{u}}= \mathcal{D}(\mathbf{A}\mathbf{P}\bm{\tilde{p}}).\end{aligned}</span> In total, the camera model involves estimating 8 parameters per camera: 6 for extrinsics, 1 for intrinsics, and 1 for distortion.</p>
<p>For the camera calibration and triangulation methods described below, we define the projection <span class="math inline">\mathcal{T}</span> from <span class="math inline">\bm{\tilde{p}}</span> to <span class="math inline">\bm{\tilde{u}}</span> as <span class="math display">\begin{aligned}
   \mathcal{T}(\bm{\tilde{p}}, \bm{\theta}_c) =  \bm{\tilde{u}}= \mathcal{D}(\mathbf{A}\mathbf{P}\bm{\tilde{p}}),\end{aligned}</span> where <span class="math inline">\bm{\theta}_c</span> are the 8 parameters for the camera model of camera <span class="math inline">c</span>.</p>
</section>
<section id="initial-estimate-of-camera-parameters." class="level4" data-number="5.4.3.2">
<h4 data-number="5.4.3.2" class="anchored" data-anchor-id="initial-estimate-of-camera-parameters."><span class="header-section-number">5.4.3.2</span> Initial estimate of camera parameters.</h4>
<p>In order to calibrate the cameras and estimate parameters of the camera models, we start by obtaining an initial estimate of the camera parameters. We detected calibration board keypoints in videos simultaneously captured from all cameras. We then initialized intrinsics based on these detections following the algorithm from Zhang&nbsp;<span class="citation" data-cites="zhang_flexible_2000">(<a href="#ref-zhang_flexible_2000" role="doc-biblioref">Z. Zhang 2000</a>)</span>. We initialized the distortion coefficients to zero.</p>
<p>We developed the following method to initialize camera extrinsics from arbitrary locations. For each pair of cameras, the number of frames in which the board is seen simultaneously is counted and used to build a graph of cameras. To be specific, each node is a camera, and edges represent pairs of cameras whose relation we will use to seed the initialization.</p>
<p>The greedy graph construction algorithm is as follows. Starting with the pair of cameras for which the number of frames the board is simultaneously detected is the largest, connect the two camera nodes with an edge. Next, proceed with iterations in decreasing order of the number of boards simultaneously detected. At each iteration, if the two nodes (cameras) are not already connected through some path, connect them with an edge. Processing iteratively through all pairs of cameras in this manner, a graph of camera connectivity is produced. Full 3D calibration is possible if and only if the graph is fully connected.</p>
<p>To initialize the extrinsics using this graph, we start with any camera and set its rotation and translation to zero. Then, we initialize its neighbors from the estimated relative pose of the calibration board between them using the initial intrinsics. This procedure is continued recursively until all cameras are initialized. A diagram of the camera initialization for an example dataset is provided in Figure S1A.</p>
</section>
<section id="bundle-adjustment." class="level4" data-number="5.4.3.3">
<h4 data-number="5.4.3.3" class="anchored" data-anchor-id="bundle-adjustment."><span class="header-section-number">5.4.3.3</span> Bundle adjustment.</h4>
<p>To refine the camera parameters from initial estimates, we performed a bundle adjustment by implementing a nonlinear least-squares optimization to minimize the reprojection error&nbsp;<span class="citation" data-cites="goos_bundle_2000">(<a href="#ref-goos_bundle_2000" role="doc-biblioref">Triggs et al. 2000</a>)</span>. Given all <span class="math inline">\bm{\tilde{u}}_{c,j,t}</span>, the detected <span class="math inline">j^\text{th}</span> keypoints from the calibration board at cameras <span class="math inline">c</span> in frames <span class="math inline">t</span>, we solve for the best camera parameters <span class="math inline">\bm{\theta}_c</span> and 3D points <span class="math inline">\bm{\tilde{p}}_{j,t}</span> such that the reprojection loss <span class="math inline">\mathcal{L}</span> is minimized: <span class="math display">\begin{aligned}
\mathcal{L} = \sum_c \sum_j \sum_t E\left( \bm{\tilde{u}}_{c,j,t} - \mathcal{T}(\bm{\tilde{p}}_{j,t}, \bm{\theta}_c) \right).   \end{aligned}</span> Here, <span class="math inline">E(\cdot)</span> denotes the norm using which the error is computed. This norm may be the least squares norm, but in practice, we used a robust norm, such as the Huber or soft <span class="math inline">\ell_1</span> norm, to minimize the influence of outliers.</p>
<p>This optimization is nonlinear because the camera projection function <span class="math inline">\mathcal{T}</span> is nonlinear. We recognized that it is a nonlinear least-squares problem with a sparse Jacobian and thus solved it efficiently using the Trust Region Reflective algorithm&nbsp;<span class="citation" data-cites="byrd_approximate_1988 branch_subspace_1999">(<a href="#ref-byrd_approximate_1988" role="doc-biblioref">Byrd, Schnabel, and Shultz 1988</a>; <a href="#ref-branch_subspace_1999" role="doc-biblioref">Branch, Coleman, and Li 1999</a>)</span>, as implemented in SciPy&nbsp;<span class="citation" data-cites="scipy">(<a href="#ref-scipy" role="doc-biblioref">Virtanen et al. 2020</a>)</span>.</p>
</section>
<section id="iterative-bundle-adjustment." class="level4" data-number="5.4.3.4">
<h4 data-number="5.4.3.4" class="anchored" data-anchor-id="iterative-bundle-adjustment."><span class="header-section-number">5.4.3.4</span> Iterative bundle adjustment.</h4>
<p>When calibrating cameras, we found that outliers have an outsized impact on calibration results, even when using robust losses such as the Huber loss or soft <span class="math inline">\ell_1</span> loss. Thus, we designed an iterative calibration algorithm, inspired by the fast global registration algorithm from Zhou et al.&nbsp;<span class="citation" data-cites="zhou_fast_2016">(<a href="#ref-zhou_fast_2016" role="doc-biblioref">Q.-Y. Zhou, Park, and Koltun 2016</a>)</span>, which solves a minimization with a robust loss efficiently through an alternating optimization scheme.</p>
<p>We approximate this alternating optimization in the camera calibration setting through an iterative threshold scheme. In our algorithm, at each iteration, a reprojection error threshold is defined and the subset of points <span class="math inline">\bm{u}_{c,i}</span> with reprojection error below this threshold is chosen. Bundle adjustment is then performed on these points alone. The threshold decreases exponentially with each iteration, to refine the points to be calibrated. The pseudocode for the algorithm is listed in box below.</p>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Iterative bundle adjustment algorithm
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p><strong>Input:</strong><br>
Initial camera parameters <span class="math inline">\bm{\theta}</span><br>
Keypoint detections <span class="math inline">\bm{u}</span> from multiple cameras<br>
Starting and ending thresholds <span class="math inline">\mu_{\text{start}}</span> and <span class="math inline">\mu_{\text{end}}</span></p>
<p>Run the following algorithm:</p>
<p><span class="math inline">\bm{u}_{\text{eval}} \leftarrow</span> sample(<span class="math inline">\bm{u}</span>)<br>
<span class="math inline">\textbf{errors}_{\text{eval}} \leftarrow</span> reprojection_errors(<span class="math inline">\bm{u}^{\text{eval}}, \bm{\theta}</span>)<br>
<span class="math inline">\textbf{low} \leftarrow \text{percentile}(\textbf{errors}_{\text{eval}}, 15\%)</span><br>
<span class="math inline">\textbf{high} \leftarrow \text{percentile}(\textbf{errors}_{\text{eval}}, 75\%)</span><br>
<span class="math inline">\mu_i \leftarrow \left(  \frac{\mu_{\text{end}}}{\mu_{\text{start}}}\right)^{i / N_{\text{iter}}}</span> &nbsp; <span class="math inline">\mu_i \leftarrow \max(\textbf{low}, \min(\mu_i, \textbf{high}))</span><br>
<span class="math inline">\mu_{\text{picked}} \leftarrow</span> points from <span class="math inline">\bm{u}_{\text{eval}}</span> for which reprojection error is below <span class="math inline">\mu_i</span><br>
<span class="math inline">\bm{\theta}\leftarrow</span> bundle_adjust(<span class="math inline">\bm{\theta}</span>, <span class="math inline">\bm{u}_{\text{picked}}</span>)<br>
Return <span class="math inline">\bm{\theta}</span></p>
</div>
</div>
</div>
</section>
</section>
<section id="triangulation-and-3d-filtering" class="level3" data-number="5.4.4">
<h3 data-number="5.4.4" class="anchored" data-anchor-id="triangulation-and-3d-filtering"><span class="header-section-number">5.4.4</span> Triangulation and 3D filtering</h3>
<p>The 3D triangulation task seeks 3D points <span class="math inline">\bm{p}_{j,t}</span> for joint <span class="math inline">j</span> at frame <span class="math inline">t</span>, given a set of detected 2D points <span class="math inline">\bm{u}_{c, j, t}</span> from cameras <span class="math inline">c</span> with camera parameters <span class="math inline">\bm{\theta}_c</span>. There are several common methods for solving this triangulation task. Below, we describe 3 of these methods, then describe our method for spatiotemporally constrained triangulation. For illustration, a comparison of the performance of these methods is shown on an example dataset in <a href="#fig-optim">Figure&nbsp;5</a> .</p>
<section id="linear-least-squares-triangulation." class="level4" data-number="5.4.4.1">
<h4 data-number="5.4.4.1" class="anchored" data-anchor-id="linear-least-squares-triangulation."><span class="header-section-number">5.4.4.1</span> Linear least-squares triangulation.</h4>
<p>The first method triangulates 3D points by using linear least-squares&nbsp;<span class="citation" data-cites="hartley_triangulation_1997">(<a href="#ref-hartley_triangulation_1997" role="doc-biblioref">Hartley and Sturm 1997</a>)</span>. Linear least-squares is the fastest method for multi-camera triangulation, but it may lead to poor results when the 2D inputs contain noisy or inaccurate keypoint detections. To be specific, we start with a camera model with parameters estimated from the calibration procedure described above, so that the extrinsics matrix <span class="math inline">\mathbf{P}_c</span>, intrinsics matrix <span class="math inline">\mathbf{A}_c</span>, and distortion function <span class="math inline">\mathcal{D}_c</span> are known for each camera <span class="math inline">c</span>. By rearranging the camera model, we may write the following relationship: <span class="math display">\begin{aligned}
    \mathcal{D}_c^{-1}(\bm{\tilde{u}}_{c,j,t}) = \mathbf{A}_c \mathbf{P}_c \bm{\tilde{p}}_{j,t}.\end{aligned}</span> We solved this linear system of equations using the singular value decomposition (SVD) of the product <span class="math inline">\mathbf{A}_c \mathbf{P}_c</span> to approximate the solutions for the unknown <span class="math inline">\bm{\tilde{p}}_{j,t}</span> &nbsp;<span class="citation" data-cites="hartley_triangulation_1997">(<a href="#ref-hartley_triangulation_1997" role="doc-biblioref">Hartley and Sturm 1997</a>)</span>.</p>
</section>
<section id="median-filtered-least-squares-triangulation." class="level4" data-number="5.4.4.2">
<h4 data-number="5.4.4.2" class="anchored" data-anchor-id="median-filtered-least-squares-triangulation."><span class="header-section-number">5.4.4.2</span> Median-filtered least-squares triangulation.</h4>
<p>As a simple extension of least-square triangulation to correct some of the noisy detections, we applied a median filter to the resulting 3D points tracked across frames. This filtering improves the tracking, but at the cost of losing high frequency dynamics. Furthermore, a median filter does not improve triangulation if the original tracking is consistently poor.</p>
</section>
<section id="ransac-triangulation." class="level4" data-number="5.4.4.3">
<h4 data-number="5.4.4.3" class="anchored" data-anchor-id="ransac-triangulation."><span class="header-section-number">5.4.4.3</span> RANSAC triangulation.</h4>
<p>Random sample consensus (RANSAC) triangulation aims to reduce the influence of outlier 2D keypoint detections on the triangulated 3D point, by finding the subset of keypoint detections that minimizes the reprojection error. We implemented RANSAC triangulation by triangulating all possible pairs of keypoints detected from multiple views and picking the resulting 3D point with the smallest reprojection error.</p>
<p>Formally, let <span class="math inline">\bm{\tilde{p}}^{a,b}_{j,t}</span> be the triangulated 3D point for keypoint <span class="math inline">j</span> at frame <span class="math inline">t</span> computed using the 2D keypoint detections from cameras <span class="math inline">a</span> and <span class="math inline">b</span>, then our algorithm finds <span class="math inline">\bm{\tilde{p}}_{jt}</span> using the following relation: <span class="math display">\begin{aligned}
\bm{\tilde{p}}_{j,t} = \mathop{\mathrm{arg\,min}}_{\bm{\tilde{p}}^{a,b}_{j,t}} ~ &amp; \left\Vert \mathcal{T}\left(\bm{\tilde{p}}_{j,t}^{a,b}, \bm{\theta}_a \right) - \bm{\tilde{u}}_{a,j,t} \right\Vert_2 + \\
&amp; \left\Vert \mathcal{T}\left(\bm{\tilde{p}}_{j,t}^{a,b}, \bm{\theta}_b \right) - \bm{\tilde{u}}_{b,j,t} \right\Vert_2.\end{aligned}</span></p>
</section>
<section id="spatiotemporally-regularized-triangulation." class="level4" data-number="5.4.4.4">
<h4 data-number="5.4.4.4" class="anchored" data-anchor-id="spatiotemporally-regularized-triangulation."><span class="header-section-number">5.4.4.4</span> Spatiotemporally regularized triangulation.</h4>
<p>We formulated triangulation as an optimization problem, which allowed us to specify soft spatiotemporal constraints (i.e.&nbsp;regularization) on the triangulated points. We propose that the points must satisfy three soft constraints: (1) the projection of the 3D points onto each camera should be close to the tracked 2D points, (2) the 3D points should be smooth in time, and (3) the lengths of specified limbs in 3D should not vary too much. Each of these constraints may be formulated as a regularization in the full objective function.</p>
<p>First, the <strong>reprojection loss</strong> is written as <span class="math display">\begin{aligned}
L_{\text{proj}} = \sum_c \sum_j \sum_t E\left( \mathcal{T}\left(\bm{\tilde{p}}_{j,t}, \bm{\theta}_c \right) - \bm{\tilde{u}}_{c,j,t} \right).\end{aligned}</span> Here, <span class="math inline">E(\cdot)</span> is a robust norm function such as the Huber or soft-<span class="math inline">\ell_1</span> norm, to minimize the influence of outlier detections.</p>
<p>Second, the <strong>temporal loss</strong> is formulated as follows: <span class="math display">\begin{aligned}
    L_{\text{time}} = \sum_j \sum_t \left\Vert \bm{\tilde{p}}_{j,t} - \bm{\tilde{p}}_{j,(t-1)} \right\Vert_2.\end{aligned}</span> We extend this penalty to minimize higher-order (e.g.&nbsp;2nd or 3rd) finite-difference derivatives, which produces smoother trajectories but has less impact on important high frequency dynamics (see Figure S4).</p>
<p>Third, the <strong>limb loss</strong> may be formulated by adding an additional parameter <span class="math inline">d_l</span> for each limb <span class="math inline">l</span>, defined to consist of joints <span class="math inline">j_1</span> and <span class="math inline">j_2</span>: <span class="math display">\begin{aligned}
    L_{\text{limb}} =  \sum_{l, j_1, j_2 \in \text{limbs}} ~~ \sum_t
\left( \frac{\left\Vert \bm{\tilde{p}}_{j_1,t} - \bm{\tilde{p}}_{j_2,t} \right\Vert_2 - d_l}{d_l} \right)^2.\end{aligned}</span> The limb error is normalized relative to the limb length so that each limb contributes equally to the error.</p>
<p>Given each of the losses above, the overall objective function to minimize may be written as: <span class="math display">\mathcal{L} =  L_{\text{proj}} + \alpha_{\text{time}} L_{\text{time}} + \alpha_{\text{limb}} L_{\text{limb}}.</span> We solve this sparse nonlinear least-squares problem efficiently using the Trust Region Reflective algorithm&nbsp;<span class="citation" data-cites="byrd_approximate_1988 branch_subspace_1999">(<a href="#ref-byrd_approximate_1988" role="doc-biblioref">Byrd, Schnabel, and Shultz 1988</a>; <a href="#ref-branch_subspace_1999" role="doc-biblioref">Branch, Coleman, and Li 1999</a>)</span>, as implemented in SciPy&nbsp;<span class="citation" data-cites="scipy">(<a href="#ref-scipy" role="doc-biblioref">Virtanen et al. 2020</a>)</span>, similarly to the bundle adjustment optimization. To initialize the optimization, we use linear least-squares triangulation. When formulated as a sparse nonlinear least-squares problem, the time and memory requirements of the optimization scale linearly relative to the number of input time points.</p>
<p>The parameters <span class="math inline">\alpha_{\text{time}}</span> and <span class="math inline">\alpha_{\text{limb}}</span> may be tuned to adjust the strength of the temporal or limb loss, respectively. Note, however, that the temporal loss is in units of distance, which may vary substantially across datasets. Thus, to standardize these parameters, we break down the parameter <span class="math inline">\alpha_{\text{time}}</span> in terms of a user-tunable parameter <span class="math inline">\beta_{\text{time}}</span> and an automatically computed scale <span class="math inline">\gamma</span> such that <span class="math display">\alpha_{\text{time}} = \beta_{\text{time}} \gamma.</span> We compute the scale <span class="math inline">\gamma</span> as <span class="math display">\gamma = \frac{N}{\sum_j \sum_t \left\Vert \bm{\tilde{p}}_{j,t} - \bm{\tilde{p}}_{j,(t-1)} \right\Vert_2},</span> where <span class="math inline">\bm{\tilde{p}}_{j,t}</span> is an initial estimate obtained from linear least-squares triangulation. We found that the parameters <span class="math inline">\beta_{\text{time}} = 2</span> and <span class="math inline">\alpha_{\text{limb}} = 2</span> work well across a variety of datasets, and we used these parameters for tracking all four datasets in this manuscript. The user may additionally specify weaker constraints for the lengths of certain limbs to allow for some flexibility, such as the shoulder length in humans or the length of the tarsus in flies.</p>
</section>
<section id="estimating-joint-angles." class="level4" data-number="5.4.4.5">
<h4 data-number="5.4.4.5" class="anchored" data-anchor-id="estimating-joint-angles."><span class="header-section-number">5.4.4.5</span> Estimating joint angles.</h4>
<p>We estimated joint angles from the tracked 3D positions. To compute the flexion angle defined by the three 3D points surrounding the joint <span class="math inline">(\bm{p}_i, \bm{p}_j, \bm{p}_k)</span>, where point <span class="math inline">\bm{p}_j</span> lies at the joint, the angle <span class="math inline">\phi_j</span> is <span class="math display">\phi_j = \arccos \left( (\bm{p}_i - \bm{p}_j) \cdot (\bm{p}_k - \bm{p}_j) \right).</span></p>
<p>To estimate rotation and abduction angles, we solve an inverse kinematics problem treating the set of limb joints as a kinematic chain. When estimating limb angles from 3D coordinates of joints, the rotation of a joint is indistinguishable from the abduction of the next joint in the chain. We observed that fly and human limbs can be approximated to only have abduction at the joint closest to the body, so we resolve this ambiguity by assuming that only the first (most proximal) joint may abduct and the last (most distal) joint may not rotate.</p>
<p>The solution proceeds in two stages. In the first stage, we estimate the absolute rotation of each joint based on its {x, y, z} coordinate axes. The axes of the first joint match the coordinate system for the body. For other joints, the z axis is in the direction of the limb segment pointing from that joint away from the body, the x axis is in direction of proximal limb segment (towards the body) orthogonalized to the z-axis, and the y-axis is the cross product of the z-axis with the x-axis. In the second stage, the relative rotation between joints is computed and transformed to an Euler angle with an order of {z, y, x} for axis rotations. The rotations about the {z, y, x} axis represent rotation, flexion, and abduction angles, respectively. For more details of the implementation, see the accompanying code.</p>
</section>
</section>
<section id="evaluation" class="level3" data-number="5.4.5">
<h3 data-number="5.4.5" class="anchored" data-anchor-id="evaluation"><span class="header-section-number">5.4.5</span> Evaluation</h3>
<section id="comparison-of-bundle-adjustment-algorithms" class="level4" data-number="5.4.5.1">
<h4 data-number="5.4.5.1" class="anchored" data-anchor-id="comparison-of-bundle-adjustment-algorithms"><span class="header-section-number">5.4.5.1</span> Comparison of bundle adjustment algorithms</h4>
<p>To evaluate the different bundle adjustment algorithms (Figures&nbsp;S1B and C), we ran the algorithms with different parameters on the calibration videos from the fly setup. There were 4475 frames where the calibration board was detected in 2 or more cameras. To demonstrate the usefulness of our iterative bundle adjustment procedure with lower number of detections, we evaluated all bundle adjustment algorithms after subsampling the frames with board detections to 313 (7%) and 4475 (100%). At each of these frame counts, we initialized the camera parameters and then ran our iterative bundle adjustment procedure, as well as traditional bundle adjustment with a linear least-squares loss, a Huber loss, and soft L1 loss. As the Huber and soft L1 losses are sensitive to the outlier threshold parameter, we evaluated them at multiple outlier thresholds on our dataset (Figure&nbsp;S1C). We picked the loss with the best outlier threshold, as evaluated by the reprojection error at the 75th percentile, to plot in the main calibration figure. The iterative bundle adjustment procedure was run with the default parameters in Anipose: <span class="math inline">N_{iter} = 12, \mu_{\text{start}} = 15, \mu_{\text{end}} = 1</span>.</p>
</section>
<section id="evaluation-against-physical-ground-truth." class="level4" data-number="5.4.5.2">
<h4 data-number="5.4.5.2" class="anchored" data-anchor-id="evaluation-against-physical-ground-truth."><span class="header-section-number">5.4.5.2</span> Evaluation against physical ground truth.</h4>
<p>To evaluate the calibration and triangulation, we compared the accuracy of manual keypoint annotations, neural network keypoint detections, and OpenCV keypoint detections (<a href="#fig-raw">Figure&nbsp;3</a> ). The ground truth was considered to be known physical length and angles of the ChArUco board. The physical lengths were calculated between all pairs of keypoints by taking the length between the known positions of pairs of corners. Similarly, the physical angles were estimated between all triplets of non-collinear keypoints. The subpixel OpenCV detections were done using the Aruco module&nbsp;<span class="citation" data-cites="garrido-jurado_automatic_2014">(<a href="#ref-garrido-jurado_automatic_2014" role="doc-biblioref">Garrido-Jurado et al. 2014</a>)</span>. The manual annotation and neural network methods are detailed above. Given the keypoint detections from each method, we used linear least-squares triangulation to obtain 3D points and computed angles using the dot product method detailed above. If a keypoint was detected in fewer than 2 cameras at any time, we could not triangulate it and therefore did not estimate the error at that frame.</p>
</section>
<section id="evaluation-of-3d-tracking-error-for-different-filters." class="level4" data-number="5.4.5.3">
<h4 data-number="5.4.5.3" class="anchored" data-anchor-id="evaluation-of-3d-tracking-error-for-different-filters."><span class="header-section-number">5.4.5.3</span> Evaluation of 3D tracking error for different filters.</h4>
<p>To evaluate the contribution of 2D and 3D filters, we applied each filter and measured the reduction in error. For the 2D filters, we applied each of the filters (2D median filter, Viterbi filter, and autoencoder filter) and computed the 3D position using linear least-squares triangulation. We could not train the autoencoder filter on the human dataset, as the filter relies on occluded keypoints not being present in the annotated dataset and, due to the nature of the human dataset, all keypoints are annotated from every view at every frame. When applying the spatiotemporal regularization, we assumed a low variance in length of the coxa, femur, and tibia in flies and of the arm, the forearm, pelvis, femur, and tibia in the human. We assumed a slightly higher variance for the length of the tarsus in each fly and of the neck and shoulders in each human, because these body segments are more flexible. The parameters for each filter are listed in Table S1. We measured the error in joint positions and angles relative to those computed from manual annotations, using the <span class="math inline">\ell_2</span> norm. To evaluate the effect of the filter addition, as there was a lot of variance in error across points, we computed the difference in error for each point tracked. We treated points with reprojection error above 20 pixels as missing. The procedure for evaluating the 3D filters was similar, except that we compared the error in joint position and angle relative to the error from 3D points obtained with a Viterbi filter and autoencoder filter with linear least-squares triangulation.</p>
</section>
<section id="evaluation-of-derivative-error-for-different-filters." class="level4" data-number="5.4.5.4">
<h4 data-number="5.4.5.4" class="anchored" data-anchor-id="evaluation-of-derivative-error-for-different-filters."><span class="header-section-number">5.4.5.4</span> Evaluation of derivative error for different filters.</h4>
<p>To evaluate the contribution of different 2D and 3D filters to the error in derivative estimation, we applied each filter to the 3D trajectory of each joint and estimated the derivative by using the finite difference method. For each joint, each frame, and each filter, we obtain a 3D vector representing a derivative. We compare the error between this derivative vector and the true derivative vector from manual annotations by using the <span class="math inline">\ell_2</span> norm, as in the previous section.</p>
</section>
<section id="evaluation-of-3d-tracking-error-for-different-number-of-cameras" class="level4" data-number="5.4.5.5">
<h4 data-number="5.4.5.5" class="anchored" data-anchor-id="evaluation-of-3d-tracking-error-for-different-number-of-cameras"><span class="header-section-number">5.4.5.5</span> Evaluation of 3D tracking error for different number of cameras</h4>
<p>To evaluate how the number of cameras contributes to the estimate of error, we ran Anipose on all combinations of 2, 3, and 4 cameras for the human dataset. We measured the error in joint position and angles relative to manual annotations as described above. We plotted the mean error across all joint positions or angles and across all possible combinations of cameras (Table&nbsp;S2) at each number of cameras.</p>
</section>
<section id="evaluation-of-temporal-regularization-on-synthetic-dataset" class="level4" data-number="5.4.5.6">
<h4 data-number="5.4.5.6" class="anchored" data-anchor-id="evaluation-of-temporal-regularization-on-synthetic-dataset"><span class="header-section-number">5.4.5.6</span> Evaluation of temporal regularization on synthetic dataset</h4>
<p>To evaluate how minimizing higher order derivatives affects tracking of high frequency movement dynamics, we evaluated the temporal regularization on a synthetic dataset (Figure&nbsp;S4). We synthesized 30 ground-truth keypoint trajectories, each of length 500, by applying a low-pass filter with a cutoff of 0.12 cycles/sample on white noise. We then corrupted these trajectories by adding white noise and removing 10% of the points, simulating observed triangulated points (for example, as in the "No filters" trace in <a href="#fig-optim">Figure&nbsp;5</a> A). We reconstructed the signal using temporal regularization and minimizing the 1st, 2nd, or 3rd derivative across different levels of smoothing factor <span class="math inline">\beta_{\text{time}}</span>. We estimated the power spectrum of the ground truth, corrupted, and reconstructed signals by taking the average power spectral density at each frequency across all 30 simulated trajectories. We estimated the power spectral density using the Welch’s method as implemented in SciPy&nbsp;<span class="citation" data-cites="scipy">(<a href="#ref-scipy" role="doc-biblioref">Virtanen et al. 2020</a>)</span>. We computed the root mean squared error (RMSE) between the ground truth and reconstructed signals for each derivative minimized at different levels of smoothing. We evaluated the RMSE of median filters with window size of 3 to 25 samples on the same trajectories, and found the median filter with a window size of 9 samples to have the lowest RMSE, which we plot as a reference.</p>
</section>
</section>
<section id="analysis-of-kinematics" class="level3" data-number="5.4.6">
<h3 data-number="5.4.6" class="anchored" data-anchor-id="analysis-of-kinematics"><span class="header-section-number">5.4.6</span> Analysis of kinematics</h3>
<section id="analysis-of-fly-walking-kinematics" class="level4" data-number="5.4.6.1">
<h4 data-number="5.4.6.1" class="anchored" data-anchor-id="analysis-of-fly-walking-kinematics"><span class="header-section-number">5.4.6.1</span> Analysis of fly walking kinematics</h4>
<p>For the analysis in <a href="#fig-flydemo">Figure&nbsp;7</a> , we used data from 39 wild-type Berlin flies on a spherical treadmill (details of experimental setup above). We tracked the flies using Anipose with spatiotemporal regularization and Viterbi and autoencoder filters. We confirmed by visual inspection and by checking reprojection errors that all flies were well tracked.</p>
<p>To restrict the data to only walking, we manually labeled fly behavior for a random subset of videos using the VGG Image Annotation tool&nbsp;<span class="citation" data-cites="dutta2019vgg">(<a href="#ref-dutta2019vgg" role="doc-biblioref">Abhishek Dutta and Zisserman 2019</a>)</span>. The categories of behaviors labeled were abdomen grooming, antennae grooming, ball push, ball tapping, eye grooming, head grooming, standing, t1 grooming, t3 grooming, walking. To detect walking behavior across the entire dataset, we fit a logistic classifier to predict the type of behavior. The input data to the classifier for each time point was a chunk of 24 samples around that time of 3D joint positions and angles and the Fourier transform of the 24 samples of each variable. The confusion matrix for the classifier on a test set is shown in Figure&nbsp;S6C. The false negative rate was 0%, whereas the false positive rate was about 3%. To detect bouts of walking, we used the classifier to predict a walking probability for each sample in a video, applied a mean filter with a window of 16 samples to the probability, then kept bouts where the probability was above 0.5 for at least 40 consecutive samples. To further reduce spurious walking bout detections, we removed any bout where the femur-tibia flexion of the left front and hind legs varied less than 10 degrees over the full bout. We confirmed with visual inspection that all bouts removed in this way did not include walking.</p>
<p>To perform the UMAP embeddings, we followed a procedure inspired by DeAngelis et al <span class="citation" data-cites="deangelis_manifold_2019">(<a href="#ref-deangelis_manifold_2019" role="doc-biblioref">DeAngelis, Zavatone-Veth, and Clark 2019</a>)</span>, which mapped the manifold structure of <em>Drosophila</em> walking from 2D tracking data. We took chunks of 32 samples, advancing by 8 samples, of the coxa rotation, femur rotation, and femur-tibia flexion angles and their derivatives. Thus, we obtained a set of vectors of size 1152 (32 samples * 6 legs * 3 angles * 2 raw &amp; derivatives), which we standardized by subtracting the mean and dividing by the standard deviation along each dimension. We embedded this set of vectors in 3 dimensions using the UMAP algorithm&nbsp;<span class="citation" data-cites="mcinnes2018umap-software">(<a href="#ref-mcinnes2018umap-software" role="doc-biblioref">McInnes et al. 2018</a>)</span>, with effective minimum distance of 0.4 and 30 neighbors as parameters. To compute the phase of the step cycle, we applied a band-pass filter (1st order Butterworth over 3–60Hz) to front left leg femur-tibia flexion and estimated the phase from the analytic signal obtained using the Hilbert transform.</p>
</section>
<section id="analysis-of-mouse-reaching-kinematics" class="level4" data-number="5.4.6.2">
<h4 data-number="5.4.6.2" class="anchored" data-anchor-id="analysis-of-mouse-reaching-kinematics"><span class="header-section-number">5.4.6.2</span> Analysis of mouse reaching kinematics</h4>
<p>In <a href="#fig-flydemo">Figure&nbsp;7</a> and Figure S7, we analyzed videos from 4 mice recorded over 2 different days (details of experimental setup above). We tracked 3 keypoints on the hand for each mouse using Anipose with no filters. To obtain accurate 3D tracking for all trajectories, we removed all points with reprojection error above 10 pixels, then filled in missing data (about 11% of the data) using linear interpolation. We used the proximal end of digit 3 as a marker for the overall hand position. Mice 1 and 3 reached with their left hand, whereas mice 2 and 4 reached with their right hand. Accordingly, we quantified the movement of the hand each mouse reached with. We labeled the start and end of each reach, along with the reach type using the Anipose visualizer (<a href="#fig-viz">Figure&nbsp;6</a>). To obtain the 3D position of the pellet holder, we labeled the pellet holder for each mouse and day from both views using the VGG Image Annotation tool&nbsp;<span class="citation" data-cites="dutta2019vgg">(<a href="#ref-dutta2019vgg" role="doc-biblioref">Abhishek Dutta and Zisserman 2019</a>)</span>, then triangulated the labeled points for each pair of views using aniposelib. We measured the distance of the hand (proximal end of digit 3) to the pellet holder by using the <span class="math inline">\ell_2</span> norm.</p>
</section>
<section id="analysis-of-human-walking-kinematics" class="level4" data-number="5.4.6.3">
<h4 data-number="5.4.6.3" class="anchored" data-anchor-id="analysis-of-human-walking-kinematics"><span class="header-section-number">5.4.6.3</span> Analysis of human walking kinematics</h4>
<p>In <a href="#fig-flydemo">Figure&nbsp;7</a> and S7, we analyzed videos from all 7 publicly available subjects in the Human 3.6M dataset (dataset described above). We tracked 17 keypoints for each human using Anipose with spatiotemporal regularization and Viterbi filters.</p>
<p>To focus on walking, we restricted our analysis on the “Walking-1”, “Walking-2”, “WalkingTogether-1”, and “WalkingTogether-2” actions in the dataset. We estimated the knee flexion, hip flexion, and hip rotation angles as described in the “Estimating joint angles” section above. For the UMAP embedding, we followed a procedure similar to our analysis of fly kinematics. Specifically, we took chunks of 24 samples, advancing by 8 samples, of the knee flexion, hip rotation, and hip flexion angles and their derivatives. Thus, we obtained a set of vectors of size 288 (24 samples * 2 legs * 3 angles * 2 raw &amp; derivatives), which we standardized by subtracting the mean and dividing by the standard deviation along each dimension. We embedded this set of vectors in 3 dimensions using the UMAP algorithm&nbsp;<span class="citation" data-cites="mcinnes2018umap-software">(<a href="#ref-mcinnes2018umap-software" role="doc-biblioref">McInnes et al. 2018</a>)</span>, with effective minimum distance of 0.4 and 30 neighbors as parameters.</p>
<script>
document.addEventListener('DOMContentLoaded', function(){
    setTimeout(function(){
        console.log("reload!");
        frames = document.getElementsByTagName("iframe");
        for(var i=0; i<frames.length; i+=1) {
            frames[i].contentDocument.location.reload(true);
        }
    }, 5000);
});
</script>

</section>
</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-abdelfattah_bright_2018" class="csl-entry" role="doc-biblioentry">
Abdelfattah, Ahmed S., Takashi Kawashima, Amrita Singh, Ondrej Novak, Hui Liu, Yichun Shuai, Yi-Chieh Huang, et al. 2019. <span>“Bright and Photostable Chemigenetic Indicators for Extended in Vivo Voltage Imaging.”</span> <em>Science</em> 365 (6454): 699–704. <a href="https://doi.org/10.1126/science.aav6416">https://doi.org/10.1126/science.aav6416</a>.
</div>
<div id="ref-agarwal_building_2011" class="csl-entry" role="doc-biblioentry">
Agarwal, Sameer, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M. Seitz, and Richard Szeliski. 2011. <span>“Building <span>Rome</span> in a Day.”</span> <em>Communications of the ACM</em> 54 (10): 105–12. <a href="https://doi.org/10.1145/2001269.2001293">https://doi.org/10.1145/2001269.2001293</a>.
</div>
<div id="ref-alexander_natures_2017" class="csl-entry" role="doc-biblioentry">
Alexander, David E. 2017. <em>Nature’s <span>Machines</span>: <span>An</span> <span>Introduction</span> to <span>Organismal</span> <span>Biomechanics</span></em>. 1st ed. Academic Press.
</div>
<div id="ref-amin_multi-view_2013" class="csl-entry" role="doc-biblioentry">
Amin, Sikandar, Mykhaylo Andriluka, Marcus Rohrbach, and Bernt Schiele. 2013. <span>“Multi-View <span>Pictorial Structures</span> for <span>3D Human Pose Estimation</span>.”</span> In <em>Procedings of the <span>British Machine Vision Conference</span> 2013</em>, 45.1–11. <span>Bristol</span>: <span>British Machine Vision Association</span>. <a href="https://doi.org/10.5244/C.27.45">https://doi.org/10.5244/C.27.45</a>.
</div>
<div id="ref-aminzare_gait_2018" class="csl-entry" role="doc-biblioentry">
Aminzare, Zahra, Vaibhav Srivastava, and Philip Holmes. 2018. <span>“Gait <span>Transitions</span> in a <span>Phase Oscillator Model</span> of an <span>Insect Central Pattern Generator</span>.”</span> <em>SIAM Journal on Applied Dynamical Systems</em> 17 (1): 626–71. <a href="https://doi.org/10.1137/17M1125571">https://doi.org/10.1137/17M1125571</a>.
</div>
<div id="ref-azevedo_size_2019" class="csl-entry" role="doc-biblioentry">
Azevedo, Anthony W, Evyn S Dickinson, Pralaksha Gurung, Lalanti Venkatasubramanian, Richard S Mann, and John C Tuthill. 2020. <span>“A Size Principle for Recruitment of <span>Drosophila</span> Leg Motor Neurons.”</span> Edited by Ronald L Calabrese and Chris Q Doe. <em>eLife</em> 9 (June): e56754. <a href="https://doi.org/10.7554/eLife.56754">https://doi.org/10.7554/eLife.56754</a>.
</div>
<div id="ref-azim2014skilled" class="csl-entry" role="doc-biblioentry">
Azim, Eiman, Juan Jiang, Bror Alstermark, and Thomas M Jessell. 2014. <span>“Skilled Reaching Relies on a <span>V2a</span> Propriospinal Internal Copy Circuit.”</span> <em>Nature</em> 508 (7496): 357–63.
</div>
<div id="ref-bala_openmonkeystudio_2020" class="csl-entry" role="doc-biblioentry">
Bala, Praneet C., Benjamin R. Eisenreich, Seng Bum Michael Yoo, Benjamin Y. Hayden, Hyun Soo Park, and Jan Zimmermann. 2020. <span>“Automated Markerless Pose Estimation in Freely Moving Macaques with <span>OpenMonkeyStudio</span>.”</span> <em>Nature Communications</em> 11 (1): 4560. <a href="https://doi.org/10.1038/s41467-020-18441-5">https://doi.org/10.1038/s41467-020-18441-5</a>.
</div>
<div id="ref-balbinot_post-stroke_2018" class="csl-entry" role="doc-biblioentry">
Balbinot, Gustavo, Clarissa Pedrini Schuch, Matthew S. Jeffers, Matthew W. McDonald, Jessica M. Livingston-Thomas, and Dale Corbett. 2018. <span>“Post-Stroke Kinematic Analysis in Rats Reveals Similar Reaching Abnormalities as Humans.”</span> <em>Scientific Reports</em> 8 (1): 8738. <a href="https://doi.org/10.1038/s41598-018-27101-0">https://doi.org/10.1038/s41598-018-27101-0</a>.
</div>
<div id="ref-becker_cerebellar_2019" class="csl-entry" role="doc-biblioentry">
Becker, Matthew I., and Abigail L. Person. 2019. <span>“Cerebellar <span>Control</span> of <span>Reach Kinematics</span> for <span>Endpoint Precision</span>.”</span> <em>Neuron</em> 103 (2): 335–348.e5. <a href="https://doi.org/10.1016/j.neuron.2019.05.007">https://doi.org/10.1016/j.neuron.2019.05.007</a>.
</div>
<div id="ref-bender_computer-assisted_2010" class="csl-entry" role="doc-biblioentry">
Bender, John A., Elaine M. Simpson, and Roy E. Ritzmann. 2010. <span>“Computer-<span>Assisted 3D Kinematic Analysis</span> of <span>All Leg Joints</span> in <span>Walking Insects</span>.”</span> Edited by Björn Brembs. <em>PLoS ONE</em> 5 (10): e13617. <a href="https://doi.org/10.1371/journal.pone.0013617">https://doi.org/10.1371/journal.pone.0013617</a>.
</div>
<div id="ref-berendes_speed-dependent_2016" class="csl-entry" role="doc-biblioentry">
Berendes, Volker, Sasha N. Zill, Ansgar Büschges, and Till Bockemühl. 2016. <span>“Speed-Dependent Interplay Between Local Pattern-Generating Activity and Sensory Signals During Walking in <span>Drosophila</span>.”</span> <em>Journal of Experimental Biology</em>, January, jeb.146720. <a href="https://doi.org/10.1242/jeb.146720">https://doi.org/10.1242/jeb.146720</a>.
</div>
<div id="ref-berman_mapping_2014" class="csl-entry" role="doc-biblioentry">
Berman, Gordon J., Daniel M. Choi, William Bialek, and Joshua W. Shaevitz. 2014. <span>“Mapping the Stereotyped Behaviour of Freely Moving Fruit Flies.”</span> <em>Journal of The Royal Society Interface</em> 11 (99): 20140672. <a href="https://doi.org/10.1098/rsif.2014.0672">https://doi.org/10.1098/rsif.2014.0672</a>.
</div>
<div id="ref-bernstein_optogenetics_2012" class="csl-entry" role="doc-biblioentry">
Bernstein, Jacob G., Paul A. Garrity, and Edward S. Boyden. 2012. <span>“Optogenetics and Thermogenetics: Technologies for Controlling the Activity of Targeted Cells Within Intact Neural Circuits.”</span> <em>Current Opinion in Neurobiology</em> 22 (1): 61–71. <a href="https://doi.org/10.1016/j.conb.2011.10.023">https://doi.org/10.1016/j.conb.2011.10.023</a>.
</div>
<div id="ref-bidaye_six-legged_2017" class="csl-entry" role="doc-biblioentry">
Bidaye, Salil S., Till Bockemühl, and Ansgar Büschges. 2017. <span>“Six-Legged Walking in Insects: How <span>CPGs</span>, Peripheral Feedback, and Descending Signals Generate Coordinated and Adaptive Motor Rhythms.”</span> <em>Journal of Neurophysiology</em> 119 (2): 459–75. <a href="https://doi.org/10.1152/jn.00658.2017">https://doi.org/10.1152/jn.00658.2017</a>.
</div>
<div id="ref-opencv_library" class="csl-entry" role="doc-biblioentry">
Bradski, G. 2000. <span>“<span>The OpenCV Library</span>.”</span> <em>Dr. Dobb’s Journal of Software Tools</em>.
</div>
<div id="ref-branch_subspace_1999" class="csl-entry" role="doc-biblioentry">
Branch, Mary Ann, Thomas F. Coleman, and Yuying Li. 1999. <span>“A Subspace, Interior, and Conjugate Gradient Method for Large-Scale Bound-Constrained Minimization Problems.”</span> <em><span>SIAM</span> Journal on Scientific Computing</em> 21 (1): 1–23.
</div>
<div id="ref-branson_apt_2020" class="csl-entry" role="doc-biblioentry">
Branson, Kristin. (2015) 2020. <em>Animal Part Tracker</em>. <a href="https://github.com/kristinbranson/APT" class="uri">https://github.com/kristinbranson/APT</a>.
</div>
<div id="ref-branson_high-throughput_2009" class="csl-entry" role="doc-biblioentry">
Branson, Kristin, Alice A. Robie, John Bender, Pietro Perona, and Michael H. Dickinson. 2009. <span>“High-Throughput Ethomics in Large Groups of Drosophila.”</span> <em>Nature Methods</em> 6 (6): 451–57. <a href="https://doi.org/10.1038/nmeth.1328">https://doi.org/10.1038/nmeth.1328</a>.
</div>
<div id="ref-breugel_numerical_2020" class="csl-entry" role="doc-biblioentry">
Breugel, F. V. Van, J. N. Kutz, and B. W. Brunton. 2020. <span>“Numerical <span>Differentiation</span> of <span>Noisy Data</span>: <span>A Unifying Multi</span>-<span>Objective Optimization Framework</span>.”</span> <em>IEEE Access</em> 8: 196865–77. <a href="https://doi.org/10.1109/ACCESS.2020.3034077">https://doi.org/10.1109/ACCESS.2020.3034077</a>.
</div>
<div id="ref-buchner_elementary_1976" class="csl-entry" role="doc-biblioentry">
Buchner, E. 1976. <span>“Elementary Movement Detectors in an Insect Visual System.”</span> <em>Biological Cybernetics</em> 24 (2): 85–101. <a href="https://doi.org/10.1007/BF00360648">https://doi.org/10.1007/BF00360648</a>.
</div>
<div id="ref-byrd_approximate_1988" class="csl-entry" role="doc-biblioentry">
Byrd, Richard H., Robert B. Schnabel, and Gerald A. Shultz. 1988. <span>“Approximate Solution of the Trust Region Problem by Minimization over Two-Dimensional Subspaces.”</span> <em>Mathematical Programming</em> 40 (1): 247–63.
</div>
<div id="ref-cao_openpose_2018" class="csl-entry" role="doc-biblioentry">
Cao, Z., G. Hidalgo Martinez, T. Simon, S.-E. Wei, and Y. A. Sheikh. 2019. <span>“<span>OpenPose</span>: <span>Realtime Multi</span>-<span>Person 2D Pose Estimation</span> Using <span>Part Affinity Fields</span>.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 1–1. <a href="https://doi.org/10.1109/TPAMI.2019.2929257">https://doi.org/10.1109/TPAMI.2019.2929257</a>.
</div>
<div id="ref-IonescuSminchisescu11" class="csl-entry" role="doc-biblioentry">
Catalin Ionescu, Cristian Sminchisescu, Fuxin Li. 2011. <span>“Latent Structured Models for Human Pose Estimation.”</span> In <em>International Conference on Computer Vision</em>.
</div>
<div id="ref-cheng_occlusion-aware_2019" class="csl-entry" role="doc-biblioentry">
Cheng, Yu, Bo Yang, Bo Wang, Yan Wending, and Robby Tan. 2019. <span>“Occlusion-<span>Aware Networks</span> for <span>3D Human Pose Estimation</span> in <span>Video</span>.”</span> <em>ICCV</em>, October, 723–32. <a href="https://doi.org/10.1109/ICCV.2019.00081">https://doi.org/10.1109/ICCV.2019.00081</a>.
</div>
<div id="ref-chiba_differential_2005" class="csl-entry" role="doc-biblioentry">
Chiba, Hiroshi, Satoru Ebihara, Naoki Tomita, Hidetada Sasaki, and James P Butler. 2005. <span>“Differential Gait Kinematics Between Fallers and Non-Fallers in Community-Dwelling Elderly People.”</span> <em>Geriatrics and Gerontology International</em> 5 (2): 127–34. <a href="https://doi.org/10.1111/j.1447-0594.2005.00281.x">https://doi.org/10.1111/j.1447-0594.2005.00281.x</a>.
</div>
<div id="ref-dhooge_applications_2001" class="csl-entry" role="doc-biblioentry">
D’Hooge, Rudi, and Peter P. De Deyn. 2001. <span>“Applications of the Morris Water Maze in the Study of Learning and Memory.”</span> <em>Brain Research Reviews</em> 36 (1): 60–90.
</div>
<div id="ref-dallmann_leg_2021" class="csl-entry" role="doc-biblioentry">
Dallmann, Chris J., Pierre Karashchuk, Bingni W. Brunton, and John C. Tuthill. 2021. <span>“A Leg to Stand on: Computational Models of Proprioception.”</span> <em>Current Opinion in Physiology</em>, March. <a href="https://doi.org/10.1016/j.cophys.2021.03.001">https://doi.org/10.1016/j.cophys.2021.03.001</a>.
</div>
<div id="ref-dana_high-performance_2018" class="csl-entry" role="doc-biblioentry">
Dana, Hod, Yi Sun, Boaz Mohar, Brad K. Hulse, Aaron M. Kerlin, Jeremy P. Hasseman, Getahun Tsegaye, et al. 2019. <span>“High-Performance Calcium Sensors for Imaging Activity in Neuronal Populations and Microcompartments.”</span> <em>Nature Methods</em> 16 (7, 7): 649–57. <a href="https://doi.org/10.1038/s41592-019-0435-6">https://doi.org/10.1038/s41592-019-0435-6</a>.
</div>
<div id="ref-david_g._forney_viterbi_1973" class="csl-entry" role="doc-biblioentry">
David G. Forney. 1973. <span>“The Viterbi Algorithm.”</span> <em>Proceedings of the IEEE</em> 61 (3): 268–78.
</div>
<div id="ref-deangelis_manifold_2019" class="csl-entry" role="doc-biblioentry">
DeAngelis, Brian D, Jacob A Zavatone-Veth, and Damon A Clark. 2019. <span>“The Manifold Structure of Limb Coordination in Walking <span>Drosophila</span>.”</span> Edited by Ronald L Calabrese. <em>eLife</em> 8 (June): e46409. <a href="https://doi.org/10.7554/eLife.46409">https://doi.org/10.7554/eLife.46409</a>.
</div>
<div id="ref-dong_fast_2019" class="csl-entry" role="doc-biblioentry">
Dong, Junting, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. 2019. <span>“Fast and <span>Robust Multi</span>-<span>Person 3D Pose Estimation</span> from <span>Multiple Views</span>.”</span> <em>CVPR</em>, January. <a href="https://arxiv.org/abs/1901.04111">https://arxiv.org/abs/1901.04111</a>.
</div>
<div id="ref-dunn_geometric_2021" class="csl-entry" role="doc-biblioentry">
Dunn, Timothy W., Jesse D. Marshall, Kyle S. Severson, Diego E. Aldarondo, David G. C. Hildebrand, Selmaan N. Chettih, William L. Wang, et al. 2021. <span>“Geometric Deep Learning Enables <span>3D</span> Kinematic Profiling Across Species and Environments.”</span> <em>Nature Methods</em> 18 (5): 564–73. <a href="https://doi.org/10.1038/s41592-021-01106-6">https://doi.org/10.1038/s41592-021-01106-6</a>.
</div>
<div id="ref-dutta2019vgg" class="csl-entry" role="doc-biblioentry">
Dutta, Abhishek, and Andrew Zisserman. 2019. <span>“The <span>VIA</span> Annotation Software for Images, Audio and Video.”</span> In <em>Proceedings of the 27th ACM International Conference on Multimedia</em>. MM ’19. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3343031.3350535">https://doi.org/10.1145/3343031.3350535</a>.
</div>
<div id="ref-dutta2016via" class="csl-entry" role="doc-biblioentry">
Dutta, A., A. Gupta, and A. Zissermann. 2016. <span>“<span>VGG</span> Image Annotator (<span>VIA</span>).”</span> <a href="http://www.robots.ox.ac.uk/~vgg/software/via/" class="uri">http://www.robots.ox.ac.uk/~vgg/software/via/</a>.
</div>
<div id="ref-esposito_brainstem_2014" class="csl-entry" role="doc-biblioentry">
Esposito, Maria Soledad, Paolo Capelli, and Silvia Arber. 2014. <span>“Brainstem Nucleus <span>MdV</span> Mediates Skilled Forelimb Motor Tasks.”</span> <em>Nature</em> 508 (7496): 351–56. <a href="https://doi.org/10.1038/nature13023">https://doi.org/10.1038/nature13023</a>.
</div>
<div id="ref-farr_quantitative_2002" class="csl-entry" role="doc-biblioentry">
Farr, Tracy D., and Ian Q. Whishaw. 2002. <span>“Quantitative and <span>Qualitative Impairments</span> in <span>Skilled Reaching</span> in the <span>Mouse</span> (<span>Mus</span> Musculus) <span>After</span> a <span>Focal Motor Cortex Stroke</span>.”</span> <em>Stroke</em> 33 (7): 1869–75. <a href="https://doi.org/10.1161/01.STR.0000020714.48349.4E">https://doi.org/10.1161/01.STR.0000020714.48349.4E</a>.
</div>
<div id="ref-frantsevich_gimbals_2009" class="csl-entry" role="doc-biblioentry">
Frantsevich, Leonid, and Weiying Wang. 2009. <span>“Gimbals in the Insect Leg.”</span> <em>Arthropod Structure &amp; Development</em> 38 (1): 16–30. <a href="https://doi.org/10.1016/j.asd.2008.06.002">https://doi.org/10.1016/j.asd.2008.06.002</a>.
</div>
<div id="ref-fukuchi_public_2018" class="csl-entry" role="doc-biblioentry">
Fukuchi, Claudiane A., Reginaldo K. Fukuchi, and Marcos Duarte. 2018. <span>“A Public Dataset of Overground and Treadmill Walking Kinematics and Kinetics in Healthy Individuals.”</span> <em>PeerJ</em> 6 (April): e4640. <a href="https://doi.org/10.7717/peerj.4640">https://doi.org/10.7717/peerj.4640</a>.
</div>
<div id="ref-garrido-jurado_automatic_2014" class="csl-entry" role="doc-biblioentry">
Garrido-Jurado, S., R. Muñoz-Salinas, F. J. Madrid-Cuevas, and M. J. Marín-Jiménez. 2014. <span>“Automatic Generation and Detection of Highly Reliable Fiducial Markers Under Occlusion.”</span> <em>Pattern Recognition</em> 47 (6): 2280–92. <a href="https://doi.org/10.1016/j.patcog.2014.01.005">https://doi.org/10.1016/j.patcog.2014.01.005</a>.
</div>
<div id="ref-goldsmith_neurodynamic_2020" class="csl-entry" role="doc-biblioentry">
Goldsmith, C A, N S Szczecinski, and R D Quinn. 2020. <span>“Neurodynamic Modeling of the Fruit Fly <span><em>Drosophila</em></span><span> <em>Melanogaster</em></span>.”</span> <em>Bioinspiration &amp; Biomimetics</em> 15 (6): 065003. <a href="https://doi.org/10.1088/1748-3190/ab9e52">https://doi.org/10.1088/1748-3190/ab9e52</a>.
</div>
<div id="ref-gotz_visual_1973" class="csl-entry" role="doc-biblioentry">
Götz, K. G. 1973. <span>“Visual Control of Locomotion in the Walking Fruitfly <span>Drosophila</span>.”</span> <em>Journal of Comparative Physiology</em> 85 (3): 235–66. <a href="https://doi.org/10.1007/BF00694232">https://doi.org/10.1007/BF00694232</a>.
</div>
<div id="ref-graving_deepposekit_2019" class="csl-entry" role="doc-biblioentry">
Graving, Jacob M, Daniel Chae, Hemal Naik, Liang Li, Benjamin Koger, Blair R Costelloe, and Iain D Couzin. 2019. <span>“DeepPoseKit, a Software Toolkit for Fast and Robust Animal Pose Estimation Using Deep Learning.”</span> Edited by Ian T Baldwin, Josh W Shaevitz, Josh W Shaevitz, and Greg Stephens. <em>eLife</em> 8 (October): e47994. <a href="https://doi.org/10.7554/eLife.47994">https://doi.org/10.7554/eLife.47994</a>.
</div>
<div id="ref-gunel_deepfly3d_2019" class="csl-entry" role="doc-biblioentry">
Günel, Semih, Helge Rhodin, Daniel Morales, João Campagnolo, Pavan Ramdya, and Pascal Fua. 2019. <span>“DeepFly3D, a Deep Learning-Based Approach for 3D Limb and Appendage Tracking in Tethered, Adult <em>Drosophila</em>.”</span> Edited by Timothy O’Leary, Ronald L Calabrese, and Josh W Shaevitz. <em>eLife</em> 8 (October): e48571. <a href="https://doi.org/10.7554/eLife.48571">https://doi.org/10.7554/eLife.48571</a>.
</div>
<div id="ref-guo_cortex_2015" class="csl-entry" role="doc-biblioentry">
Guo, Jian-Zhong, Austin R Graves, Wendy W Guo, Jihong Zheng, Allen Lee, Juan Rodríguez-González, Nuo Li, et al. 2015. <span>“Cortex Commands the Performance of Skilled Movement.”</span> Edited by Michael Hausser. <em>eLife</em> 4 (December): e10774. <a href="https://doi.org/10.7554/eLife.10774">https://doi.org/10.7554/eLife.10774</a>.
</div>
<div id="ref-halberstadt_incipient_2016" class="csl-entry" role="doc-biblioentry">
Halberstadt, Jamin, Joshua Conrad Jackson, David Bilkey, Jonathan Jong, Harvey Whitehouse, Craig McNaughton, and Stefanie Zollmann. 2016. <span>“Incipient <span>Social</span> <span>Groups</span>: <span>An</span> <span>Analysis</span> via <span>In</span>-<span>Vivo</span> <span>Behavioral</span> <span>Tracking</span>.”</span> <em>PLOS ONE</em> 11 (3): e0149880. <a href="https://doi.org/10.1371/journal.pone.0149880">https://doi.org/10.1371/journal.pone.0149880</a>.
</div>
<div id="ref-hartley_triangulation_1997" class="csl-entry" role="doc-biblioentry">
Hartley, Richard I., and Peter Sturm. 1997. <span>“Triangulation.”</span> <em>Computer Vision and Image Understanding</em> 68 (2): 146–57.
</div>
<div id="ref-he_deep_2015" class="csl-entry" role="doc-biblioentry">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep <span>Residual</span> <span>Learning</span> for <span>Image</span> <span>Recognition</span>.”</span> <em>CVPR</em>. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.
</div>
<div id="ref-hu_deep_2018" class="csl-entry" role="doc-biblioentry">
Hu, Danying, Daniel DeTone, Vikram Chauhan, Igor Spivak, and Tomasz Malisiewicz. 2018. <span>“Deep <span>ChArUco</span>: <span>Dark ChArUco Marker Pose Estimation</span>.”</span> <em>arXiv:1812.03247 [Cs]</em>, December. <a href="https://arxiv.org/abs/1812.03247">https://arxiv.org/abs/1812.03247</a>.
</div>
<div id="ref-insafutdinov_deepercut_2016" class="csl-entry" role="doc-biblioentry">
Insafutdinov, Eldar, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, and Bernt Schiele. 2016. <span>“DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model.”</span> In <em>ECCV</em>. <a href="http://arxiv.org/abs/1605.03170">http://arxiv.org/abs/1605.03170</a>.
</div>
<div id="ref-h36m_pami" class="csl-entry" role="doc-biblioentry">
Ionescu, Catalin, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. 2014. <span>“Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.
</div>
<div id="ref-iskakov_learnable_2019" class="csl-entry" role="doc-biblioentry">
Iskakov, Karim, Egor Burkov, Victor Lempitsky, and Yury Malkov. 2019. <span>“Learnable Triangulation of Human Pose.”</span> In <em>International Conference on Computer Vision (ICCV)</em>.
</div>
<div id="ref-jun_fully_2017" class="csl-entry" role="doc-biblioentry">
Jun, James J., Nicholas A. Steinmetz, Joshua H. Siegle, Daniel J. Denman, Marius Bauza, Brian Barbarits, Albert K. Lee, et al. 2017. <span>“Fully Integrated Silicon Probes for High-Density Recording of Neural Activity.”</span> <em>Nature</em> 551 (7679): 232–36. <a href="https://doi.org/10.1038/nature24636">https://doi.org/10.1038/nature24636</a>.
</div>
<div id="ref-kane_real-time_2020" class="csl-entry" role="doc-biblioentry">
Kane, Gary A, Gonçalo Lopes, Jonny L Saunders, Alexander Mathis, and Mackenzie W Mathis. 2020. <span>“Real-Time, Low-Latency Closed-Loop Feedback Using Markerless Posture Tracking.”</span> Edited by Gordon J Berman, Timothy E Behrens, Gordon J Berman, and Tiago Branco. <em>eLife</em> 9 (December): e61909. <a href="https://doi.org/10.7554/eLife.61909">https://doi.org/10.7554/eLife.61909</a>.
</div>
<div id="ref-kingma_adam_2017" class="csl-entry" role="doc-biblioentry">
Kingma, Diederik P., and Jimmy Ba. 2017. <span>“Adam: <span>A Method</span> for <span>Stochastic Optimization</span>.”</span> <em>arXiv:1412.6980 [Cs]</em>, January. <a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>.
</div>
<div id="ref-koch_ror_2017" class="csl-entry" role="doc-biblioentry">
Koch, Stephanie C., Marta Garcia Del Barrio, Antoine Dalet, Graziana Gatto, Thomas Gunther, Jingming Zhang, Barbara Seidler, Dieter Saur, Roland Schuele, and Martyn Goulding. 2017. <span>“<span>ROR<span class="math inline">\beta</span></span> Spinal Interneurons Gate Sensory Transmission During Locomotion to Secure a Fluid Walking Gait.”</span> <em>Neuron</em> 96 (6): 1419–1431.e5. <a href="https://doi.org/10.1016/j.neuron.2017.11.011">https://doi.org/10.1016/j.neuron.2017.11.011</a>.
</div>
<div id="ref-kuan_dense_2020" class="csl-entry" role="doc-biblioentry">
Kuan, Aaron T., Jasper S. Phelps, Logan A. Thomas, Tri M. Nguyen, Julie Han, Chiao-Lin Chen, Anthony W. Azevedo, et al. 2020. <span>“Dense Neuronal Reconstruction Through <span>X</span>-Ray Holographic Nano-Tomography.”</span> <em>Nature Neuroscience</em> 23 (12): 1637–43. <a href="https://doi.org/10.1038/s41593-020-0704-9">https://doi.org/10.1038/s41593-020-0704-9</a>.
</div>
<div id="ref-low_precision_2018" class="csl-entry" role="doc-biblioentry">
Low, Aloysius Y. T., Ayesha R. Thanawalla, Alaric K. K. Yip, Jinsook Kim, Kelly L. L. Wong, Martesa Tantra, George J. Augustine, and Albert I. Chen. 2018. <span>“Precision of <span>Discrete</span> and <span>Rhythmic Forelimb Movements Requires</span> a <span>Distinct Neuronal Subpopulation</span> in the <span>Interposed Anterior Nucleus</span>.”</span> <em>Cell Reports</em> 22 (9): 2322–33. <a href="https://doi.org/10.1016/j.celrep.2018.02.017">https://doi.org/10.1016/j.celrep.2018.02.017</a>.
</div>
<div id="ref-machado_quantitative_2015" class="csl-entry" role="doc-biblioentry">
Machado, Ana S, Dana M Darmohray, João Fayad, Hugo G Marques, and Megan R Carey. 2015. <span>“A Quantitative Framework for Whole-Body Coordination Reveals Specific Deficits in Freely Walking Ataxic Mice.”</span> Edited by Indira M Raman. <em>eLife</em> 4 (October): e07892. <a href="https://doi.org/10.7554/eLife.07892">https://doi.org/10.7554/eLife.07892</a>.
</div>
<div id="ref-mamiya_neural_2018" class="csl-entry" role="doc-biblioentry">
Mamiya, Akira, Pralaksha Gurung, and John C. Tuthill. 2018. <span>“Neural <span>Coding</span> of <span>Leg Proprioception</span> in <span>Drosophila</span>.”</span> <em>Neuron</em> 100 (3): 636–650.e6. <a href="https://doi.org/10.1016/j.neuron.2018.09.009">https://doi.org/10.1016/j.neuron.2018.09.009</a>.
</div>
<div id="ref-maniates-selvin_reconstruction_2020" class="csl-entry" role="doc-biblioentry">
Maniates-Selvin, Jasper T., David Grant Colburn Hildebrand, Brett J. Graham, Aaron T. Kuan, Logan A. Thomas, Tri Nguyen, Julia Buhmann, et al. 2020. <span>“Reconstruction of Motor Control Circuits in Adult <span>Drosophila</span> Using Automated Transmission Electron Microscopy.”</span> <em>bioRxiv</em>, January, 2020.01.10.902478. <a href="https://doi.org/10.1101/2020.01.10.902478">https://doi.org/10.1101/2020.01.10.902478</a>.
</div>
<div id="ref-marshall_2021" class="csl-entry" role="doc-biblioentry">
Marshall, Jesse D., Diego E. Aldarondo, Timothy W. Dunn, William L. Wang, Gordon J. Berman, and Bence P. Ölveczky. 2021. <span>“Continuous Whole-Body 3D Kinematic Recordings Across the Rodent Behavioral Repertoire.”</span> <em>Neuron</em> 109 (3): 420–437.e8. https://doi.org/<a href="https://doi.org/10.1016/j.neuron.2020.11.016">https://doi.org/10.1016/j.neuron.2020.11.016</a>.
</div>
<div id="ref-martinez_simple_2017" class="csl-entry" role="doc-biblioentry">
Martinez, Julieta, Rayat Hossain, Javier Romero, and James J. Little. 2017. <span>“A Simple yet Effective Baseline for 3d Human Pose Estimation.”</span> In <em>ICCV</em>.
</div>
<div id="ref-mathis_deeplabcut_2018" class="csl-entry" role="doc-biblioentry">
Mathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh N. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018. <span>“<span>DeepLabCut</span>: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning.”</span> <em>Nature Neuroscience</em> 21 (9): 1281. <a href="https://doi.org/10.1038/s41593-018-0209-y">https://doi.org/10.1038/s41593-018-0209-y</a>.
</div>
<div id="ref-mathis_primer_2020" class="csl-entry" role="doc-biblioentry">
Mathis, Alexander, Steffen Schneider, Jessy Lauer, and Mackenzie Weygandt Mathis. 2020. <span>“A <span>Primer</span> on <span>Motion Capture</span> with <span>Deep Learning</span>: <span>Principles</span>, <span>Pitfalls</span>, and <span>Perspectives</span>.”</span> <em>Neuron</em> 108 (1): 44–65. <a href="https://doi.org/10.1016/j.neuron.2020.09.017">https://doi.org/10.1016/j.neuron.2020.09.017</a>.
</div>
<div id="ref-mathis_deep_2020" class="csl-entry" role="doc-biblioentry">
Mathis, Mackenzie Weygandt, and Alexander Mathis. 2020. <span>“Deep Learning Tools for the Measurement of Animal Behavior in Neuroscience.”</span> <em>Current Opinion in Neurobiology</em>, Neurobiology of <span>Behavior</span>, 60 (February): 1–11. <a href="https://doi.org/10.1016/j.conb.2019.10.008">https://doi.org/10.1016/j.conb.2019.10.008</a>.
</div>
<div id="ref-mcinnes2018umap-software" class="csl-entry" role="doc-biblioentry">
McInnes, Leland, John Healy, Nathaniel Saul, and Lukas Grossberger. 2018. <span>“UMAP: Uniform Manifold Approximation and Projection.”</span> <em>The Journal of Open Source Software</em> 3 (29): 861.
</div>
<div id="ref-mendes_quantification_2013" class="csl-entry" role="doc-biblioentry">
Mendes, César S, Imre Bartos, Turgay Akay, Szabolcs Márka, and Richard S Mann. 2013. <span>“Quantification of Gait Parameters in Freely Walking Wild Type and Sensory Deprived <span>Drosophila</span> Melanogaster.”</span> <em>eLife</em> 2 (January). <a href="https://doi.org/10.7554/eLife.00231">https://doi.org/10.7554/eLife.00231</a>.
</div>
<div id="ref-murphy_markerless_2019" class="csl-entry" role="doc-biblioentry">
Murphy, Daniel. 2019. <span>“Markerless 3D Pose Estimation from <span>RGB</span> Data.”</span> Bachelor's Thesis, Brown University.
</div>
<div id="ref-nath_deeplabcut_2019" class="csl-entry" role="doc-biblioentry">
Nath, Tanmay, Alexander Mathis, An Chi Chen, Amir Patel, Matthias Bethge, and Mackenzie Weygandt Mathis. 2019. <span>“Using <span>DeepLabCut</span> for 3D Markerless Pose Estimation Across Species and Behaviors.”</span> <em>Nature Protocols</em> 14 (7): 2152–76. <a href="https://doi.org/10.1038/s41596-019-0176-0">https://doi.org/10.1038/s41596-019-0176-0</a>.
</div>
<div id="ref-nunez_multiview_2019" class="csl-entry" role="doc-biblioentry">
Núñez, Juan Carlos, Raúl Cabido, José F. Vélez, Antonio S. Montemayor, and Juan José Pantrigo. 2019. <span>“Multiview <span>3D</span> Human Pose Estimation Using Improved Least-Squares and <span>LSTM</span> Networks.”</span> <em>Neurocomputing</em> 323 (January): 335–43. <a href="https://doi.org/10.1016/j.neucom.2018.10.009">https://doi.org/10.1016/j.neucom.2018.10.009</a>.
</div>
<div id="ref-olton_mazes_1979" class="csl-entry" role="doc-biblioentry">
Olton, David S. 1979. <span>“Mazes, Maps, and Memory.”</span> <em>American Psychologist</em> 34 (7): 583.
</div>
<div id="ref-pedregosa_scikit-learn:_2011" class="csl-entry" role="doc-biblioentry">
Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. <span>“Scikit-Learn: <span>Machine</span> <span>Learning</span> in <span>Python</span>.”</span> <em>Journal of Machine Learning Research</em> 12: 2825–30.
</div>
<div id="ref-pereira_fast_2019" class="csl-entry" role="doc-biblioentry">
Pereira, Talmo D., Diego E. Aldarondo, Lindsay Willmore, Mikhail Kislin, Samuel S.-H. Wang, Mala Murthy, and Joshua W. Shaevitz. 2019. <span>“Fast Animal Pose Estimation Using Deep Neural Networks.”</span> <em>Nature Methods</em> 16 (1): 117. <a href="https://doi.org/10.1038/s41592-018-0234-5">https://doi.org/10.1038/s41592-018-0234-5</a>.
</div>
<div id="ref-pereira_sleap_2020" class="csl-entry" role="doc-biblioentry">
Pereira, Talmo D., Nathaniel Tabris, Junyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, Z. Yan Wang, David M. Turner, et al. 2020. <span>“<span>SLEAP</span>: <span>Multi</span>-Animal Pose Tracking.”</span> <em>bioRxiv</em>, September, 2020.08.31.276246. <a href="https://doi.org/10.1101/2020.08.31.276246">https://doi.org/10.1101/2020.08.31.276246</a>.
</div>
<div id="ref-richardson_aprilcal_2013" class="csl-entry" role="doc-biblioentry">
Richardson, Andrew, Johannes Strom, and Edwin Olson. 2013. <span>“<span>AprilCal</span>: <span>Assisted</span> and Repeatable Camera Calibration.”</span> In <em>2013 <span>IEEE</span>/<span>RSJ International Conference</span> on <span>Intelligent Robots</span> and <span>Systems</span></em>, 1814–21. <span>Tokyo</span>: <span>IEEE</span>. <a href="https://doi.org/10.1109/IROS.2013.6696595">https://doi.org/10.1109/IROS.2013.6696595</a>.
</div>
<div id="ref-rinehart_examination_2006" class="csl-entry" role="doc-biblioentry">
Rinehart, Nicole J., Mark A. Bellgrove, Bruce J. Tonge, Avril V. Brereton, Debra Howells-Rankin, and John L. Bradshaw. 2006. <span>“An <span>Examination</span> of <span>Movement</span> <span>Kinematics</span> in <span>Young</span> <span>People</span> with <span>High</span>-Functioning <span>Autism</span> and <span>Asperger</span>’s <span>Disorder</span>: <span>Further</span> <span>Evidence</span> for a <span>Motor</span> <span>Planning</span> <span>Deficit</span>.”</span> <em>Journal of Autism and Developmental Disorders</em> 36 (6): 757–67. <a href="https://doi.org/10.1007/s10803-006-0118-x">https://doi.org/10.1007/s10803-006-0118-x</a>.
</div>
<div id="ref-roberts_biomechanical_2017" class="csl-entry" role="doc-biblioentry">
Roberts, Mary, David Mongeon, and Francois Prince. 2017. <span>“Biomechanical Parameters for Gait Analysis: A Systematic Review of Healthy Human Gait.”</span> <em>Physical Therapy and Rehabilitation</em> 4 (1): 6. <a href="https://doi.org/10.7243/2055-2386-4-6">https://doi.org/10.7243/2055-2386-4-6</a>.
</div>
<div id="ref-sarandi_synthetic_2018" class="csl-entry" role="doc-biblioentry">
Sárándi, István, Timm Linder, Kai O. Arras, and Bastian Leibe. 2018. <span>“Synthetic <span>Occlusion Augmentation</span> with <span>Volumetric Heatmaps</span> for the 2018 <span>ECCV PoseTrack Challenge</span> on <span>3D Human Pose Estimation</span>.”</span> In <em>ECCV</em>.
</div>
<div id="ref-Saunders_autopilot_2019" class="csl-entry" role="doc-biblioentry">
Saunders, Jonny L., and Michael Wehr. 2019. <span>“Autopilot: Automating Behavioral Experiments with Lots of Raspberry Pis.”</span> <em>bioRxiv</em>. <a href="https://doi.org/10.1101/807693">https://doi.org/10.1101/807693</a>.
</div>
<div id="ref-schindelin_fiji_2012" class="csl-entry" role="doc-biblioentry">
Schindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. <span>“Fiji: An Open-Source Platform for Biological-Image Analysis.”</span> <em>Nature Methods</em> 9 (7): 676–82. <a href="https://doi.org/10.1038/nmeth.2019">https://doi.org/10.1038/nmeth.2019</a>.
</div>
<div id="ref-schonberger_robust_2018" class="csl-entry" role="doc-biblioentry">
Schönberger, Johannes L. 2018. <span>“Robust <span>Methods</span> for <span>Accurate</span> and <span>Efficient 3D Modeling</span> from <span>Unstructured Imagery</span>.”</span> Doctoral {{Thesis}}, ETH Zurich. <a href="https://doi.org/10.3929/ethz-b-000295763">https://doi.org/10.3929/ethz-b-000295763</a>.
</div>
<div id="ref-schwager_data-driven_2008" class="csl-entry" role="doc-biblioentry">
Schwager, Mac, Carrick Detweiler, Iuliu Vasilescu, Dean M. Anderson, and Daniela Rus. 2008. <span>“Data-Driven Identification of Group Dynamics for Motion Prediction and Control.”</span> <em>Journal of Field Robotics</em> 25 (6-7): 305–24. <a href="https://doi.org/10.1002/rob.20243">https://doi.org/10.1002/rob.20243</a>.
</div>
<div id="ref-seethapathi_movement_2019" class="csl-entry" role="doc-biblioentry">
Seethapathi, Nidhi, Shaofei Wang, Rachit Saluja, Gunnar Blohm, and Konrad P. Kording. 2019. <span>“Movement Science Needs Different Pose Tracking Algorithms.”</span> <em>arXiv:1907.10226 [Cs, q-Bio]</em>. <a href="http://arxiv.org/abs/1907.10226">http://arxiv.org/abs/1907.10226</a>.
</div>
<div id="ref-solaro_subtle_2007" class="csl-entry" role="doc-biblioentry">
Solaro, C., G. Brichetto, M. Casadio, L. Roccatagliata, P. Ruggiu, G. L. Mancardi, P. G. Morasso, P. Tanganelli, and V. Sanguineti. 2007. <span>“Subtle Upper Limb Impairment in Asymptomatic Multiple Sclerosis Subjects.”</span> <em>Multiple Sclerosis Journal</em> 13 (3): 428–32. <a href="https://doi.org/10.1177/1352458506069926">https://doi.org/10.1177/1352458506069926</a>.
</div>
<div id="ref-souza_evidence-based_2016" class="csl-entry" role="doc-biblioentry">
Souza, Richard B. 2016. <span>“An <span>Evidence</span>-<span>Based</span> <span>Videotaped</span> <span>Running</span> <span>Biomechanics</span> <span>Analysis</span>.”</span> <em>Physical Medicine and Rehabilitation Clinics of North America</em> 27 (1): 217–36. <a href="https://doi.org/10.1016/j.pmr.2015.08.006">https://doi.org/10.1016/j.pmr.2015.08.006</a>.
</div>
<div id="ref-stolze_prevalence_2005" class="csl-entry" role="doc-biblioentry">
Stolze, Henning, Stephan Klebe, Christoph Baecker, Christiane Zechlin, Lars Friege, Sabine Pohle, and Günther Deuschl. 2005. <span>“Prevalence of Gait Disorders in Hospitalized Neurological Patients.”</span> <em>Movement Disorders</em> 20 (1): 89–94. <a href="https://doi.org/10.1002/mds.20266">https://doi.org/10.1002/mds.20266</a>.
</div>
<div id="ref-tippett_visuomotor_2007" class="csl-entry" role="doc-biblioentry">
Tippett, William J., Adam Krajewski, and Lauren E. Sergio. 2007. <span>“Visuomotor <span>Integration</span> <span>Is</span> <span>Compromised</span> in <span>Alzheimer</span>’s <span>Disease</span> <span>Patients</span> <span>Reaching</span> for <span>Remembered</span> <span>Targets</span>.”</span> <em>European Neurology</em> 58 (1): 1–11. <a href="https://doi.org/10.1159/000102160">https://doi.org/10.1159/000102160</a>.
</div>
<div id="ref-goos_bundle_2000" class="csl-entry" role="doc-biblioentry">
Triggs, Bill, Philip F. McLauchlan, Richard I. Hartley, and Andrew W. Fitzgibbon. 2000. <span>“Bundle Adjustment — a Modern Synthesis.”</span> In <em>Vision Algorithms: Theory and Practice</em>, edited by Bill Triggs, Andrew Zisserman, and Richard Szeliski, 1883:298–372. Berlin, Heidelberg: Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/3-540-44480-7_21">https://doi.org/10.1007/3-540-44480-7_21</a>.
</div>
<div id="ref-tzschentke_review_2007" class="csl-entry" role="doc-biblioentry">
Tzschentke, Thomas M. 2007. <span>“Review on <span>CPP</span>: Measuring Reward with the Conditioned Place Preference (<span>CPP</span>) Paradigm: Update of the Last Decade.”</span> <em>Addiction Biology</em> 12 (3): 227–462.
</div>
<div id="ref-ummenhofer_demon_2017" class="csl-entry" role="doc-biblioentry">
Ummenhofer, Benjamin, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. 2017. <span>“<span>DeMoN</span>: <span>Depth</span> and <span>Motion Network</span> for <span>Learning Monocular Stereo</span>.”</span> <em>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, July, 5622–31. <a href="https://doi.org/10.1109/CVPR.2017.596">https://doi.org/10.1109/CVPR.2017.596</a>.
</div>
<div id="ref-scipy" class="csl-entry" role="doc-biblioentry">
Virtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, et al. 2020. <span>“<span class="nocase"><span>SciPy</span> 1.0: Fundamental Algorithms for Scientific Computing in Python</span>.”</span> <em>Nature Methods</em> 17: 261–72. <a href="https://doi.org/10.1038/s41592-019-0686-2">https://doi.org/10.1038/s41592-019-0686-2</a>.
</div>
<div id="ref-windolf_systematic_2008" class="csl-entry" role="doc-biblioentry">
Windolf, Markus, Nils Götzen, and Michael Morlock. 2008. <span>“Systematic Accuracy and Precision Analysis of Video Motion Capturing Systems Exemplified on the <span>Vicon</span>-460 System.”</span> <em>Journal of Biomechanics</em> 41 (12): 2776–80. <a href="https://doi.org/10.1016/j.jbiomech.2008.06.024">https://doi.org/10.1016/j.jbiomech.2008.06.024</a>.
</div>
<div id="ref-wittwer_longitudinal_2010" class="csl-entry" role="doc-biblioentry">
Wittwer, Joanne E., Kate E. Webster, and Hylton B. Menz. 2010. <span>“A Longitudinal Study of Measures of Walking in People with <span>Alzheimer</span>’s <span>Disease</span>.”</span> <em>Gait &amp; Posture</em> 32 (1): 113–17. <a href="https://doi.org/10.1016/j.gaitpost.2010.04.001">https://doi.org/10.1016/j.gaitpost.2010.04.001</a>.
</div>
<div id="ref-wu_deep_2020" class="csl-entry" role="doc-biblioentry">
Wu, Anqi, E. Kelly Buchanan, Matthew R. Whiteway, Michael Schartner, Guido Meijer, Jean-Paul Noel, Erica Rodriguez, et al. 2020. <span>“Deep <span>Graph Pose</span>: A Semi-Supervised Deep Graphical Model for Improved Animal Pose Tracking.”</span> <em>bioRxiv</em>, October, 2020.08.20.259705. <a href="https://doi.org/10.1101/2020.08.20.259705">https://doi.org/10.1101/2020.08.20.259705</a>.
</div>
<div id="ref-wu2016tensorpack" class="csl-entry" role="doc-biblioentry">
Wu, Yuxin et al. 2016. <span>“Tensorpack.”</span> <a href="https://github.com/tensorpack/" class="uri">https://github.com/tensorpack/</a>.
</div>
<div id="ref-yang_end-end_2016" class="csl-entry" role="doc-biblioentry">
Yang, Wei, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. 2016. <span>“End-to-<span>End Learning</span> of <span>Deformable Mixture</span> of <span>Parts</span> and <span>Deep Convolutional Neural Networks</span> for <span>Human Pose Estimation</span>.”</span> <em>CVPR</em>, June, 3073–82. <a href="https://doi.org/10.1109/CVPR.2016.335">https://doi.org/10.1109/CVPR.2016.335</a>.
</div>
<div id="ref-yao_monet_2019" class="csl-entry" role="doc-biblioentry">
Yao, Yuan, Yasamin Jafarian, and Hyun Soo Park. 2019. <span>“<span>MONET</span>: Multiview Semi-Supervised Keypoint Detection via Epipolar Divergence.”</span> In <em>International Conference on Computer Vision (ICCV)</em>.
</div>
<div id="ref-york_treble_2020" class="csl-entry" role="doc-biblioentry">
York, Ryan A., Lisa M. Giocomo, and Thomas R. Clandinin. 2020. <span>“<span>TREBLE</span>: A Generalizable Framework for High-Throughput Behavioral Analysis.”</span> <em>bioRxiv</em>, October, 2020.09.30.321406. <a href="https://doi.org/10.1101/2020.09.30.321406">https://doi.org/10.1101/2020.09.30.321406</a>.
</div>
<div id="ref-zhang_animal_2021" class="csl-entry" role="doc-biblioentry">
Zhang, Libby, Tim Dunn, Jesse Marshall, Bence Olveczky, and Scott Linderman. 2021. <span>“Animal Pose Estimation from Video Data with a Hierarchical von <span>Mises</span>-<span>Fisher</span>-<span>Gaussian</span> Model.”</span> In <em>International <span>Conference</span> on <span>Artificial Intelligence</span> and <span>Statistics</span></em>, 2800–2808. <span>PMLR</span>.
</div>
<div id="ref-zhang_flexible_2000" class="csl-entry" role="doc-biblioentry">
Zhang, Z. 2000. <span>“A Flexible New Technique for Camera Calibration.”</span> <em><span>IEEE</span> Transactions on Pattern Analysis and Machine Intelligence</em> 22 (11): 1330–34. <a href="https://doi.org/10.1109/34.888718">https://doi.org/10.1109/34.888718</a>.
</div>
<div id="ref-zhou_fast_2016" class="csl-entry" role="doc-biblioentry">
Zhou, Qian-Yi, Jaesik Park, and Vladlen Koltun. 2016. <span>“Fast Global Registration.”</span> In <em>ECCV</em>. <a href="https://doi.org/10.1007/978-3-319-46475-6\_47">https://doi.org/10.1007/978-3-319-46475-6\_47</a>.
</div>
<div id="ref-zhou_towards_2017" class="csl-entry" role="doc-biblioentry">
Zhou, Xingyi, Qixing Huang, Xiao Sun, Xiangyang Xue, and Yichen Wei. 2017. <span>“Towards <span>3D Human Pose Estimation</span> in the <span>Wild</span>: A <span>Weakly</span>-Supervised <span>Approach</span>.”</span> <em>arXiv:1704.02447 [Cs]</em>, July. <a href="https://arxiv.org/abs/1704.02447">https://arxiv.org/abs/1704.02447</a>.
</div>
<div id="ref-zimmermann_freipose_2020" class="csl-entry" role="doc-biblioentry">
Zimmermann, Christian, Artur Schneider, Mansour Alyahyay, Thomas Brox, and Ilka Diester. 2020. <span>“<span>FreiPose</span>: <span>A Deep Learning Framework</span> for <span>Precise Animal Motion Capture</span> in <span>3D Spaces</span>.”</span> Preprint. <em>bioRxiv</em>. <span>Neuroscience</span>. <a href="https://doi.org/10.1101/2020.02.27.967620">https://doi.org/10.1101/2020.02.27.967620</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>