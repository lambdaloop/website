<?xml version="1.0" encoding="utf-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
<!-- 2021-06-27 Sun 01:44 -->
<meta content="text/html;charset=utf-8" http-equiv="Content-Type"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>CV4Animals 2021 summary</title>
<meta content="Org mode" name="generator"/>
<meta content="lambdaloop" name="author"/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="/css/code-theme.css" rel="stylesheet"/>
<link href="/css/style.css" rel="stylesheet"/>
<script src="/js/footnotes.js"></script>
<link href="https://fonts.googleapis.com/css?family=Source Sans" rel="stylesheet"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div class="status" id="preamble">
<a href="/" id="logo" title="Home">
<img class="logoimg" src="/images/logo_sm.png"/>
    lambdaloop
</a>
<hr/>
</div>
<div id="content">
<h1 class="title">CV4Animals 2021 summary</h1>
<div id="text-table-of-contents">
<ul>
<li><a href="#org8fba09c">1. Animal datasets</a></li>
<li><a href="#org310d4b5">2. Synthetic data</a></li>
<li><a href="#orgfa0526b">3. 3D models for animals</a></li>
<li><a href="#orgcbeff3f">4. Understanding behavior</a></li>
</ul>
</div>
<p>
It’s been an exciting past few years within animal tracking, and the 2021 <a href="https://www.cv4animals.com/">CV4Animals</a> workshop is the culmination of all of this. It was fun to see so many animal tracking people gathered in one place, even if virtually.
While watching all the presentations from my increasingly hot apartment, something clicked for me and I got a glimpse of the whole field and its future.
Or perhaps it was just the heat. Still, here are some of the themes that jumped out to me.
</p>
<div class="outline-2" id="outline-container-org8fba09c">
<h2 id="org8fba09c"><span class="section-number-2">1</span> Animal datasets</h2>
<div class="outline-text-2" id="text-1">
<div class="figure">
<p><img alt="superanimal-datasets.jpg" src="/images/cv4animals-2021/superanimal-datasets.jpg" style="max-height: 350px"/>
</p>
<p><span class="figure-number">Figure 1: </span>A selection of the latest animal datasets, from the <a href="https://drive.google.com/file/d/1TH63UF9ro2w8DY5G3lwPDMtrLsUbA1Hi/view">SuperAnimal</a> poster</p>
</div>
<p>
Datasets featuring animals are becoming increasingly prevalent and rightly so. Modern human pose estimation networks are only possible because of datasets like <a href="https://cocodataset.org/#keypoints-2020">COCO Keypoints</a>, <a href="http://human-pose.mpi-inf.mpg.de/">MPII Human Pose Dataset</a>, and <a href="http://vision.imar.ro/human3.6m/description.php">Human 3.6M</a>. A good collection of common ground truth datasets will be crucial in evaluating the many different algorithms for animal pose estimation.
</p>
<p>
Mackenzie Mathis highlighted a lot of the currently available datasets and how pretraining on these could transfer to other datasets in the <a href="https://drive.google.com/file/d/1TH63UF9ro2w8DY5G3lwPDMtrLsUbA1Hi/view">SuperAnimal</a> work from her lab. Seems like if you start with a network pretrained to predict animal poses (rather than classify images from Imagenet), it generalizes much better on new animals. Based on this idea, they’ve built a <a href="https://contrib.deeplabcut.org/">new interface</a> to crowdsource much more data for even better pretrained networks.
</p>
<p>
There are starting to be a decent amount of datasets, certainly enough to test novel networks. I could see room for more though. There needs to be more cat datasets to match the dog datasets, and there are currently still very few insect or aquatic animal datasets.
</p>
<p>
Some keypoint datasets that were highlighted at the conference were:
</p>
<ul class="org-ul">
<li><a href="http://www.mackenziemathislab.org/horse10">Horse-10</a>: 30 horses annotated for benchmarking out of domain robustness</li>
<li><a href="https://sites.google.com/view/animal-pose/">Animal-pose</a>: annotations on dogs, cats, cows, horses, and sheep</li>
<li><a href="https://github.com/benjiebob/StanfordExtra">StanfordExtra</a>: 12k images of dogs</li>
<li><a href="https://github.com/benjiebob/BADJA">BADJA</a>: 9 videos of different animals</li>
<li><a href="https://cvwc2019.github.io/challenge.html">ATRW</a>: Bounding boxes and keypoints for Amur tigers in the wild</li>
<li><a href="https://marcbadger.github.io/avian-mesh/">3D Cowbirds</a>: 6300 cowbird segmentations, 1000 cowbird keypoints, and a 3D cowbird mesh model</li>
<li><a href="https://github.com/African-Robotics-Unit/AcinoSet">AcinoSet</a> (<a href="https://drive.google.com/file/d/1B9Z_9jadSVAregYWNUu0J5GGH1j9v7gj/view?usp=sharing">poster</a>): multi-view dataset of a free-running cheetah</li>
<li><a href="https://www.frontiersin.org/articles/10.3389/fnbeh.2020.581154/full">MacaquePose</a> (<a href="https://drive.google.com/file/d/1XFtyMa5AD3sTZfmoICQ8XUzPKVPZunWQ/view">poster</a>): 13k images of macaque monkeys in the wild</li>
<li><a href="https://figshare.com/collections/Rat_7M/5295370/3">Rat 7M</a>: 7 million multi-view video frames of rats</li>
</ul>
<p>
Some datasets for tracking whole animals from videos highlighted at the conference:
</p>
<ul class="org-ul">
<li><a href="https://scorhe.nih.gov/">SCORHE</a>: mice in a cage, 2 fisheye cameras</li>
<li><a href="https://data.bris.ac.uk/data/dataset/4vnrca7qw1642qlwxjadp87h7">Cows2021</a> (<a href="https://drive.google.com/file/d/14GcN4OBqsC1d93NLKEWnzaGN59VCrrRB/view">poster</a>): cows in a pen with top down camera</li>
<li><a href="https://github.com/AIFARMS/multi-camera-pig-tracking">Pigs</a> (<a href="https://drive.google.com/file/d/1ecdUNkKhlcNxA0ZbvaZBc8qJdrLHAmUV/view">poster</a>): pigs in a pen, multi-view</li>
<li><a href="https://vap.aau.dk/3d-zef/">3D-Zef</a> (<a href="https://vap.aau.dk/3d-zef/">poster</a>): zebrafish in a tank, multi-view</li>
</ul>
<p>
There was also one dataset of images with classes (no boxes or keypoints):
</p>
<ul class="org-ul">
<li><a href="https://www.crcv.ucf.edu/research/projects/florida-wildlife-camera-trap-dataset/">Florida Wildlife Camera Trap</a> (<a href="https://drive.google.com/file/d/1obALKqGCvhZFf-0nejcse1r0Yd0J2NnT/view">poster</a>): 104k images of 22 different animals from camera traps</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org310d4b5">
<h2 id="org310d4b5"><span class="section-number-2">2</span> Synthetic data</h2>
<div class="outline-text-2" id="text-2">
<div class="figure">
<p><img alt="synthetic-mouse.jpg" src="/images/cv4animals-2021/synthetic-mouse.jpg" style="max-height: 350px"/>
</p>
<p><span class="figure-number">Figure 2: </span>An impressive workflow for generating synthetic data, from the <a href="https://drive.google.com/file/d/1TtpimzqQ2ZKw31GbRBxlXLcClyCSBAtD/view">synthetic animated mouse</a> poster</p>
</div>
<p>
Along with more datasets, there was a lot of interest in using synthetic data to get a lot of valid ground truth by spending hours making an animation instead of spending hours manually annotating images. Although there is more set up, using synthetic data can make it viable to generate millions of annotated frames. I feel like this is a really promising technique and I was somewhat disappointed that none of the main talks mentioned synthetic data.
</p>
<p>
I was really impressed with the <a href="https://www.nature.com/articles/s41592-021-01103-9">synthetic animated mouse</a> work (image above). Their <a href="https://osf.io/5swfm/">mouse videos</a> look so good, both the original animation and the modified ones with the style altered to mimic the experiment!! If I were trying to track mice I would definitely look into their setup, perhaps along with a network pretrained on <a href="https://figshare.com/collections/Rat_7M/5295370/3">Rat 7M</a>.
</p>
<p>
There were more synthetic posters as well:
</p>
<ul class="org-ul">
<li><a href="https://drive.google.com/file/d/1kEWTiC_faNLjTPHqz2O8EzaFmnTLivx4/view?usp=sharing">SyDog</a>: synthetically generated dog dataset</li>
<li><a href="https://drive.google.com/file/d/1Txyqj7Mg3mHXXoHvE8BiV3byaBmDTbD2/view?usp=sharing">DynaDog+T</a>: a new 3D dog model</li>
<li><a href="https://drive.google.com/file/d/1S5q2JA8PDo0YV280pNdKV0TVpDB_Z7MX/view?usp=sharing">ZooBuilder</a>: using available 3D animations as data</li>
<li><a href="https://drive.google.com/file/d/1LrSRXug_DN8hGMvzbTj2WOPDXsFL7Gfx/view?usp=sharing">Style transfer for synthetic samples</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgfa0526b">
<h2 id="orgfa0526b"><span class="section-number-2">3</span> 3D models for animals</h2>
<div class="outline-text-2" id="text-3">
<p>
There was an odd focus in the main talks on estimating full 3D animal shape models, whereas this was much less emphasized in the poster sessions. Perhaps this reflects the interests of the organizers or their vision of where the field should be? Personally, I think these models are interesting and useful, but perhaps just as interesting as new datasets, synthetic data, and behavior. Since this was the focus of the talks, I’ll cover a bit more ground here.
</p>
<p>
In any case, I did enjoy hearing about all the different ways to skin a cat! It was interesting to contrast Silvia Zuffi’s models of quadrupeds with Ben Biggs’ quadruped models (working with Andrew Fitzgibbon) and with Marc Badger’s bird models. Each brought something different.
</p>
<div class="figure">
<p><img alt="3d-zebra.jpg" src="/images/cv4animals-2021/3d-zebra.jpg" style="max-height: 350px"/>
</p>
<p><span class="figure-number">Figure 3: </span>How to model a zebra according to Silvia Zuffi, from the <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/533/6034_after_pdfexpress.pdf">Three-D Safari</a> paper</p>
</div>
<p>
Silvia pioneered the <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Zuffi_3D_Menagerie_Modeling_CVPR_2017_paper.pdf">skinned multi-animal linear (SMAL)</a> model pipeline. She built a general 3D model from scans of animal toys, which she could fit to images with annotated keypoints and silhouettes. In follow up work, she showed how to <a href="https://files.is.tue.mpg.de/black/papers/zuffiCVPR2018.pdf">refine the shape of the 3D models</a> for specific animals, and then how to estimate the model directly from images of <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/533/6034_after_pdfexpress.pdf">animals in the wild</a>. She’s been pushing on adding a texture term in the model, so that you can have the 3D models actually look colorful which I find super cool.
</p>
<p>
Ben Biggs showed how to take the SMAL model and <a href="https://arxiv.org/abs/1811.05804">make it work well in videos</a> by throwing a full kitchen sink of optimization criteria. For good measure, he also showed a different way to refine the 3D model shape as well to estimate <a href="https://arxiv.org/pdf/2007.11110.pdf">dogs in the wild</a>. From what I understood, it sounded like the shape refinement may be more precise than Silvia’s, as it could handle the floppy dog ears.
</p>
<div class="figure">
<p><img alt="umap-aves.jpg" src="/images/cv4animals-2021/umap-aves.jpg" style="max-height: 350px"/>
</p>
<p><span class="figure-number">Figure 4: </span>Reconstructed shapes of birds match their evolutionary tree <a href="https://yufu-wang.github.io/aves/files/Wang_et_al_CVPR_2021_aves.pdf">Birds of a Feather</a> paper</p>
</div>
<p>
Personally, I liked Marc Badger’s talk, because I could tell he wanted to tackle the biological questions as much as how to build a robust vision pipeline. As I found, there’s a tradeoff between tackling both, but he seems to managing it quite nicely. He showed how to extend Silvia and Ben’s work (and <a href="https://akanazawa.github.io/cmr/">Angjoo Kanazawa’s</a>, among others) <a href="https://arxiv.org/abs/2008.06133">to cowbirds</a> in the lab and then also <a href="https://yufu-wang.github.io/aves/files/Wang_et_al_CVPR_2021_aves.pdf">birds in the wild</a>.
Connecting it to biology, he showed how the reconstructed shape of the birds matched their evolutionary lineage, which I found super cool.
</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgcbeff3f">
<h2 id="orgcbeff3f"><span class="section-number-2">4</span> Understanding behavior</h2>
<div class="outline-text-2" id="text-4">
<div class="figure">
<p><img alt="fly-behavior.jpg" src="/images/cv4animals-2021/fly-behavior.jpg" style="max-height: 350px"/>
</p>
<p><span class="figure-number">Figure 5: </span>Fly trajectories simulated using an artificial neural network, from <a href="https://arxiv.org/abs/1611.00094">this paper</a></p>
</div>
<p>
Once we have all the animals tracked, what do we do with all the tracking data? Certainly, there are applications to animation and augmented reality<sup><a class="footref" href="#fn.1" id="fnr.1">1</a></sup>. But the biologists are particularly interested in understanding how animals behave. There were some interesting perspectives on both unsupervised and supervised decompositions of animal behavior.
</p>
<p>
In the main talk series, Kristin Branson described her latest models to predict how flies move<sup><a class="footref" href="#fn.2" id="fnr.2">2</a></sup>, with the aim of deconstructing these models and connecting them to the emerging fly connectome. The questions were really interesting, especially on whether predicting a group of flies in a bowl is harder than a single fly in a bowl, due to the interactions amongst flies. I’m personally still curious how she plans to dive into the fitted models to get insights about behavior.
</p>
<p>
There were a few interesting posters that showed new ways to classify behavior from videos:
</p>
<ul class="org-ul">
<li><a href="https://drive.google.com/file/d/1jVeHXJbTQ8l_gAoSmX8fADVBWpsWWi1i/view?usp=sharing">Interpreting Expert Annotation Differences in Animal Behavior</a></li>
<li><a href="https://drive.google.com/file/d/1FmtGFy7FJFi4auf98X1VI7QJagmgVGZg/view?usp=sharing">Semi-supervised Sequence Modeling for Improved Behavioral Segmentation</a></li>
<li><a href="https://drive.google.com/file/d/13HghAqOusIr8Iu11DZW3db8TwA4MlbWe/view?usp=sharing">Spatio-Temporal Event Segmentation for Wildlife Extended Videos</a></li>
<li><a href="https://drive.google.com/file/d/1bCh4TzYOKJUfeGwxWHlFV5Z6Kt-mxMeO/view">Movement Tracks for the Automatic Detection of Fish Behavior in Videos</a></li>
<li><a href="https://drive.google.com/file/d/1YMB2Kx5MJZlU-TfNvZdZoh2iVXr9-blS/view?usp=sharing">Unsupervised Detection of Mouse Behavioural Anomalies using Two-stream Convolutional Autoencoders</a></li>
<li><a href="https://drive.google.com/file/d/1e-GxXDm4B9yLfuUaEDFKCDxs81XzNCKA/view?usp=sharing">Automatic Classification of Cichlid Behaviors Using 3D Convolutional Residual Networks</a></li>
</ul>
<p>
Overall, it feels like understanding behavior from automatically tracked kinematics is still relatively new. There aren’t the same level of datasets with animal video behavior annotations as there are for animal keypoints. The models of behavior are also not as clear as the 3D models of animals. I’m excited to see where this will all go.
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">
<div class="footdef"><sup><a class="footnum" href="#fnr.1" id="fn.1">1</a></sup> <div class="footpara"><p class="footpara">
Silvia Zuffi showed a slide of cute little fox in someone’s hand. This is what we need from augmented reality.
</p></div></div>
<div class="footdef"><sup><a class="footnum" href="#fnr.2" id="fn.2">2</a></sup> <div class="footpara"><p class="footpara">
I’m not sure that her work is published yet, but <a href="https://arxiv.org/abs/1611.00094">here</a> is the closest paper I found from her publications.
</p></div></div>
</div>
</div></div>
</body>
</html>
